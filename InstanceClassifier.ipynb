{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InstanceClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahgin/DeepLeaningProj/blob/master/InstanceClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvEbIcsEMonV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWLhQPqkJB7L",
        "colab_type": "text"
      },
      "source": [
        "## Copy data from google drive to colab env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wggjXXwF6Ri8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir /Drive\n",
        "!ls /Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8UMCBW2HrS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mkdir /Drive\n",
        "import shutil\n",
        "import glob\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "for src in glob.glob('/content/gdrive/My Drive/videos_2/yt_bb_detection_train/*.zip'):\n",
        "  dst = f'/Drive/{os.path.basename(src)}'\n",
        "  print(src, dst)    \n",
        "  if os.path.exists(src) and not os.path.exists(dst):\n",
        "    print(f'copy {src} to {dst}')\n",
        "    shutil.copy2(src, dst)\n",
        "  \n",
        " # Create a ZipFile Object and load sample.zip in it\n",
        "for z in glob.glob('/Drive/*zip'):\n",
        "  with ZipFile(z, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall('/Drive')\n",
        "    \n",
        "!ls /Drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XSA4C12JeqS",
        "colab_type": "text"
      },
      "source": [
        "## Data partition to train/test folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umgn7pe_cK8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "def list_dirs(directory):\n",
        "    \"\"\"Returns all directories in a given directory\n",
        "    \"\"\"\n",
        "    return [f for f in pathlib.Path(directory).iterdir() if f.is_dir()]\n",
        "\n",
        "def list_files(directory):\n",
        "    \"\"\"Returns all files in a given directory\n",
        "    \"\"\"\n",
        "    return [\n",
        "        f\n",
        "        for f in pathlib.Path(directory).iterdir()\n",
        "        if f.is_file() and not f.name.startswith(\".\")\n",
        "    ]\n",
        "\n",
        "def setup_files(class_dir, seed):\n",
        "    \"\"\"Returns shuffled files\n",
        "    \"\"\"\n",
        "    # make sure its reproducible\n",
        "    random.seed(seed)\n",
        "\n",
        "    files = list_files(class_dir)\n",
        "\n",
        "    files.sort()\n",
        "    random.shuffle(files)\n",
        "    return files\n",
        "\n",
        "def ratio(input, output=\"output\", seed=1337, ratio=(0.8, 0.1, 0.1)):\n",
        "    # make up for some impression\n",
        "    assert round(sum(ratio), 5) == 1\n",
        "    assert len(ratio) in (2, 3)\n",
        "\n",
        "    for class_dir in list_dirs(input):\n",
        "        for instance_dir in list_dirs(class_dir):\n",
        "            instancename = os.path.basename(instance_dir)\n",
        "            classname = os.path.basename(class_dir)\n",
        "            fulloutput = os.path.join(classname,instancename)\n",
        "            #output_dir = os.path.join(output, classname ,instancename)\n",
        "            split_class_dir_ratio(instance_dir, output, fulloutput, ratio, seed, None)\n",
        "\n",
        "\n",
        "def split_class_dir_fixed(class_dir, output, fixed, seed, prog_bar):\n",
        "    \"\"\"Splits one very class folder\n",
        "    \"\"\"\n",
        "    files = setup_files(class_dir, seed)\n",
        "\n",
        "    if not len(files) > sum(fixed):\n",
        "        raise ValueError(\n",
        "            f'The number of samples in class \"{class_dir.stem}\" are too few. There are only {len(files)} samples available but your fixed parameter {fixed} requires at least {sum(fixed)} files. You may want to split your classes by ratio.'\n",
        "        )\n",
        "\n",
        "    split_train = len(files) - sum(fixed)\n",
        "    split_val = split_train + fixed[0]\n",
        "\n",
        "    li = split_files(files, split_train, split_val, len(fixed) == 2)\n",
        "    copy_files(li, class_dir, output, prog_bar)\n",
        "    return len(files)\n",
        "\n",
        "def split_class_dir_ratio(class_dir, output, fulloutput, ratio, seed, prog_bar):\n",
        "    \"\"\"Splits one very class folder\n",
        "    \"\"\"\n",
        "    files = setup_files(class_dir, seed)\n",
        "\n",
        "    split_train = int(ratio[0] * len(files))\n",
        "    split_val = split_train + int(ratio[1] * len(files))\n",
        "\n",
        "    li = split_files(files, split_train, split_val, len(ratio) == 3)\n",
        "    copy_files(li, class_dir, output, fulloutput, prog_bar)\n",
        "\n",
        "\n",
        "def split_files(files, split_train, split_val, use_test):\n",
        "    \"\"\"Splits the files along the provided indices\n",
        "    \"\"\"\n",
        "    files_train = files[:split_train]\n",
        "    files_val = files[split_train:split_val] if use_test else files[split_train:]\n",
        "\n",
        "    li = [(files_train, \"train\"), (files_val, \"test\")]\n",
        "\n",
        "    # optional test folder\n",
        "    if use_test:\n",
        "        files_test = files[split_val:]\n",
        "        li.append((files_test, \"test\"))\n",
        "    return li\n",
        "\n",
        "\n",
        "def copy_files(files_type, class_dir, output, fulloutput, prog_bar):\n",
        "    \"\"\"Copies the files from the input folder to the output folder\n",
        "    \"\"\"\n",
        "    # get the last part within the file\n",
        "    for (files, folder_type) in files_type:\n",
        "        full_path = os.path.join(output, folder_type, fulloutput)\n",
        "\n",
        "        pathlib.Path(full_path).mkdir(parents=True, exist_ok=True)\n",
        "        for f in files:\n",
        "            if not prog_bar is None:\n",
        "                prog_bar.update()\n",
        "            shutil.copy2(f, full_path)\n",
        "            \n",
        "            \n",
        "ratio('/Drive', output='/Drive/splitdata/', seed=1337, ratio=(.8, .2))  #the partition to 80% train 20% test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwqURTd-KjOb",
        "colab_type": "text"
      },
      "source": [
        "## Augmentation Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFFxW22vKh-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubPolicy(object):\n",
        "    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n",
        "        ranges = {\n",
        "            \"shearX\": np.linspace(0, 0.3, 10),\n",
        "            \"shearY\": np.linspace(0, 0.3, 10),\n",
        "            \"translateX\": np.linspace(0, 150 / 331, 10),\n",
        "            \"translateY\": np.linspace(0, 150 / 331, 10),\n",
        "            \"rotate\": np.linspace(0, 30, 10),\n",
        "            \"color\": np.linspace(0.0, 0.9, 10),\n",
        "            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n",
        "            \"solarize\": np.linspace(256, 0, 10),\n",
        "            \"contrast\": np.linspace(0.0, 0.9, 10),\n",
        "            \"sharpness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"brightness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"autocontrast\": [0] * 10,\n",
        "            \"equalize\": [0] * 10,\n",
        "            \"invert\": [0] * 10\n",
        "        }\n",
        "\n",
        "        # from https://stackoverflow.com/questions/5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n",
        "        def rotate_with_fill(img, magnitude):\n",
        "            rot = img.convert(\"RGBA\").rotate(magnitude)\n",
        "            return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(img.mode)\n",
        "\n",
        "        func = {\n",
        "            \"shearX\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n",
        "                Image.BICUBIC, fillcolor=fillcolor),\n",
        "            \"shearY\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n",
        "                Image.BICUBIC, fillcolor=fillcolor),\n",
        "            \"translateX\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n",
        "                fillcolor=fillcolor),\n",
        "            \"translateY\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n",
        "                fillcolor=fillcolor),\n",
        "            \"rotate\": lambda img, magnitude: rotate_with_fill(img, magnitude),\n",
        "            # \"rotate\": lambda img, magnitude: img.rotate(magnitude * random.choice([-1, 1])),\n",
        "            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n",
        "            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n",
        "            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n",
        "            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n",
        "            \"equalize\": lambda img, magnitude: ImageOps.equalize(img),\n",
        "            \"invert\": lambda img, magnitude: ImageOps.invert(img)\n",
        "        }\n",
        "\n",
        "        # self.name = \"{}_{:.2f}_and_{}_{:.2f}\".format(\n",
        "        #     operation1, ranges[operation1][magnitude_idx1],\n",
        "        #     operation2, ranges[operation2][magnitude_idx2])\n",
        "        self.p1 = p1\n",
        "        self.operation1 = func[operation1]\n",
        "        self.magnitude1 = ranges[operation1][magnitude_idx1]\n",
        "        self.p2 = p2\n",
        "        self.operation2 = func[operation2]\n",
        "        self.magnitude2 = ranges[operation2][magnitude_idx2]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p1: img = self.operation1(img, self.magnitude1)\n",
        "        if random.random() < self.p2: img = self.operation2(img, self.magnitude2)\n",
        "        return img\n",
        "\n",
        "\n",
        "class ImageNetPolicy(object):\n",
        "    \"\"\" Randomly choose one of the best 24 Sub-policies on ImageNet.\n",
        "        Example:\n",
        "        >>> policy = ImageNetPolicy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     ImageNetPolicy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n",
        "            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n",
        "            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n",
        "            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n",
        "            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor)\n",
        "        ]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment ImageNet Policy\"\n",
        "      \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHgzS0BSK3r2",
        "colab_type": "text"
      },
      "source": [
        "## Create DataSet Class - Instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiQtzYuUM88R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataSet object\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import imageio\n",
        "import numpy as np  \n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class InstanceDataset(Dataset):\n",
        "    def __init__(self, basedir, transform, percentage):\n",
        "        super().__init__()\n",
        "        files = glob.glob(os.path.join(basedir ,'**/*.jpg'), recursive=True)\n",
        "        if(percentage < 100):\n",
        "          files_num = len(list(files))\n",
        "          choosefilesnumber = int(files_num * percentage / 100)\n",
        "          print(f'choosefilesnumber: {choosefilesnumber}  filesnum: {files_num}')\n",
        "          print((list(files)))\n",
        "          selectedfiles = np.random.choice(files, choosefilesnumber, replace=False)\n",
        "        elif(percentage == 100):\n",
        "          selectedfiles = files\n",
        "        \n",
        "        self.data = pd.DataFrame([self._split_file(f) for f in selectedfiles], \n",
        "                            columns=['instance_id', 'file_path'])\n",
        "        names  = np.unique(self.data['instance_id'])\n",
        "        \n",
        "        self.instanceDict = {str:index for index, str in enumerate(names)}          \n",
        "        self.data['instance_num'] = self.data['instance_id'].map(self.instanceDict)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def _split_file(self, f):\n",
        "        parts = f.split(os.sep)[-3:-1]\n",
        "        return parts[1], f   #label is originaly a str\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      dat = self.data.iloc[index]\n",
        "      #img = imageio.imread(dat['file_path'])\n",
        "      img = Image.open(dat['file_path'])\n",
        "      if self.transform:\n",
        "          img = self.transform(img)\n",
        "      img = np.resize(img, (3,128,128))\n",
        "      img = np.asarray(img)\n",
        "      return (img.astype(np.float32), dat['instance_num'])\n",
        "     \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3_nmptELe4_",
        "colab_type": "text"
      },
      "source": [
        "## Create train and test datasets including augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgdhhhjcwc1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n",
        "#basedir = '/content/gdrive/My Drive/video_data/'       \n",
        "trainbasedir = '/Drive/splitdata/train'   \n",
        "testbasedir = '/Drive/splitdata/test'\n",
        "\n",
        "data_transforms = transforms.Compose([transforms.Resize((256,256)),transforms.RandomResizedCrop(224), \n",
        "                         transforms.RandomHorizontalFlip(), #ImageNetPolicy(), \n",
        "                         transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "train_set = InstanceDataset(trainbasedir, data_transforms, 100) \n",
        "test_set = InstanceDataset(testbasedir, None, 100)                                                                            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1nkVWqUqjEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len((train_set.data['instance_num'])))\n",
        "print(len(np.unique(train_set.data['instance_num'])))\n",
        "print(max(train_set.data['instance_num']))\n",
        "print(len((test_set.data['instance_num'])))\n",
        "print(len(np.unique(test_set.data['instance_num'])))\n",
        "print(max(test_set.data['instance_num']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMwnJ5adL8RK",
        "colab_type": "text"
      },
      "source": [
        "## Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFZBZOwY1Ra4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True,\n",
        "                 num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=test_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=False,\n",
        "                 num_workers=4)\n",
        "\n",
        "print('Train size: {}'.format(len(train_loader)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGwIM6Y2MCXO",
        "colab_type": "text"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg0UH6MdAen0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perf_measure(y_actual, y_pred):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    cm = np.zeros((2,2))\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_actual[i]==y_pred[i]:\n",
        "           TP += 1\n",
        "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
        "           FP += 1\n",
        "        if y_actual[i]==y_pred[i]:\n",
        "           TN += 1\n",
        "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
        "           FN += 1\n",
        "\n",
        "        cm[0][0] = TP\n",
        "        cm[0][1] = FP\n",
        "        cm[1][0]  = FN\n",
        "        cm[1][1]  = TN\n",
        "\n",
        "    return (cm)\n",
        "  \n",
        "def create_confusion_matrix_fig(c_cm):\n",
        "    fig = plt.figure(figsize=(14, 12))\n",
        "    plt.imshow(c_cm, interpolation='nearest')\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.colorbar()\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Men7NTFOMMAn",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FWSSWG1T7Yz",
        "colab_type": "code",
        "outputId": "c9144684-1130-4ccd-8529-a7a10279f799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "\n",
        "import time \n",
        "import tqdm\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "#assert use_cuda\n",
        "\n",
        "model = models.resnet18(pretrained=False) #for third net to load our trained instance net\n",
        "\n",
        "# Writer will output to ./runs/ directory by default\n",
        "writer = SummaryWriter('with_cm')    \n",
        "    \n",
        "num_final_in = model.fc.in_features\n",
        "\n",
        "NUM_CLASSES_Instance = 6527 #6476  #len(np.unique(dataset.data['instance_num']))\n",
        "\n",
        "model.fc = nn.Linear(num_final_in, NUM_CLASSES_Instance)      \n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003) #, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.cuda()\n",
        "  model.to(torch.device(\"cuda\"))\n",
        "\n",
        "\n",
        "for epoch in range(50):\n",
        "    # trainning\n",
        "    sum_loss = 0 \n",
        "    total_cnt = 0\n",
        "    correct_cnt = 0\n",
        "    tf = time.time()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        \n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)       \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "                \n",
        "        pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label[1] == target.data).sum()\n",
        "        sum_loss += loss.data\n",
        "        \n",
        "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {}'.format(\n",
        "                epoch, batch_idx+1, sum_loss/batch_idx, float(correct_cnt)/total_cnt))    \n",
        "            \n",
        "    cm = perf_measure(target.data, pred_label[1])\n",
        "    cur_fig = create_confusion_matrix_fig(cm)\n",
        "    writer.add_figure('train_confusion_matrix', cur_fig, global_step=None, close=True, walltime=None)\n",
        "    writer.add_scalar('train accuracy', float(correct_cnt)/total_cnt, epoch)\n",
        "    writer.add_scalar('train loss', sum_loss/len(train_loader), epoch)\n",
        "    \n",
        "     # testing    \n",
        "    correct_cnt, sum_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        x = x.float()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)   \n",
        "        loss = criterion(out, target)\n",
        "        \n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth averages\n",
        "        sum_loss += loss.data\n",
        "        \n",
        "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
        "                epoch, batch_idx+1, sum_loss/batch_idx,float(correct_cnt)/total_cnt))\n",
        "    \n",
        "    try:\n",
        "      cm = perf_measure(target.data, pred_label)\n",
        "      cur_fig = create_confusion_matrix_fig(cm)\n",
        "      writer.add_figure('test_confusion_matrix', cur_fig, global_step=None, close=True, walltime=None)\n",
        "    except:\n",
        "      print(\"An exception in test_confusion_matrix\")\n",
        "      continue\n",
        "    writer.add_scalar('test accuracy', float(correct_cnt)/total_cnt, epoch)\n",
        "    writer.add_scalar('test loss', sum_loss/len(test_loader), epoch)\n",
        "\n",
        "writer.close()\n",
        "\n",
        "elapsed = time.time() - tf\n",
        "print(f'Elapsed time: {elapsed}')\n",
        "\n",
        "torch.save(model, '/content/gdrive/My Drive/Models/InstanceClassifierModel.pth')\n",
        "\n",
        "# model = models.resnet18(pretrained=True)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.eval()\n",
        "\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==>>> epoch: 0, batch index: 100, train loss: 9.116362, acc: 0.0\n",
            "==>>> epoch: 0, batch index: 200, train loss: 8.947982, acc: 0.0\n",
            "==>>> epoch: 0, batch index: 300, train loss: 8.892189, acc: 6.666666666666667e-05\n",
            "==>>> epoch: 0, batch index: 400, train loss: 8.862238, acc: 0.00015\n",
            "==>>> epoch: 0, batch index: 500, train loss: 8.835503, acc: 0.00024\n",
            "==>>> epoch: 0, batch index: 600, train loss: 8.812186, acc: 0.0003\n",
            "==>>> epoch: 0, batch index: 700, train loss: 8.792725, acc: 0.00037142857142857143\n",
            "==>>> epoch: 0, batch index: 800, train loss: 8.771975, acc: 0.000525\n",
            "==>>> epoch: 0, batch index: 900, train loss: 8.752710, acc: 0.0005777777777777778\n",
            "==>>> epoch: 0, batch index: 1000, train loss: 8.734489, acc: 0.0006\n",
            "==>>> epoch: 0, batch index: 1100, train loss: 8.716824, acc: 0.0006\n",
            "==>>> epoch: 0, batch index: 1200, train loss: 8.699799, acc: 0.0006166666666666666\n",
            "==>>> epoch: 0, batch index: 1300, train loss: 8.682438, acc: 0.0006153846153846154\n",
            "==>>> epoch: 0, batch index: 1400, train loss: 8.666043, acc: 0.0006285714285714285\n",
            "==>>> epoch: 0, batch index: 1500, train loss: 8.650254, acc: 0.0006533333333333333\n",
            "==>>> epoch: 0, batch index: 1600, train loss: 8.633517, acc: 0.0006375\n",
            "==>>> epoch: 0, batch index: 1627, train loss: 8.632730, acc: 0.0006272985572133184\n",
            "==>>> epoch: 0, batch index: 100, test loss: 9.502090, acc: 0.000\n",
            "==>>> epoch: 0, batch index: 200, test loss: 9.489802, acc: 0.000\n",
            "==>>> epoch: 0, batch index: 300, test loss: 9.471106, acc: 0.000\n",
            "==>>> epoch: 0, batch index: 370, test loss: 9.463735, acc: 0.000\n",
            "==>>> epoch: 1, batch index: 100, train loss: 8.337610, acc: 0.001\n",
            "==>>> epoch: 1, batch index: 200, train loss: 8.279035, acc: 0.0008\n",
            "==>>> epoch: 1, batch index: 300, train loss: 8.252142, acc: 0.0006\n",
            "==>>> epoch: 1, batch index: 400, train loss: 8.231608, acc: 0.00085\n",
            "==>>> epoch: 1, batch index: 500, train loss: 8.213972, acc: 0.00088\n",
            "==>>> epoch: 1, batch index: 600, train loss: 8.202885, acc: 0.0009333333333333333\n",
            "==>>> epoch: 1, batch index: 700, train loss: 8.191452, acc: 0.0009428571428571429\n",
            "==>>> epoch: 1, batch index: 800, train loss: 8.181761, acc: 0.00095\n",
            "==>>> epoch: 1, batch index: 900, train loss: 8.171974, acc: 0.0009555555555555555\n",
            "==>>> epoch: 1, batch index: 1000, train loss: 8.159846, acc: 0.00102\n",
            "==>>> epoch: 1, batch index: 1100, train loss: 8.151492, acc: 0.0010545454545454545\n",
            "==>>> epoch: 1, batch index: 1200, train loss: 8.145215, acc: 0.0010833333333333333\n",
            "==>>> epoch: 1, batch index: 1300, train loss: 8.137386, acc: 0.0010769230769230769\n",
            "==>>> epoch: 1, batch index: 1400, train loss: 8.130722, acc: 0.0011142857142857144\n",
            "==>>> epoch: 1, batch index: 1500, train loss: 8.123211, acc: 0.0010933333333333333\n",
            "==>>> epoch: 1, batch index: 1600, train loss: 8.115032, acc: 0.001125\n",
            "==>>> epoch: 1, batch index: 1627, train loss: 8.121850, acc: 0.0011315973973259862\n",
            "==>>> epoch: 1, batch index: 100, test loss: 10.570613, acc: 0.000\n",
            "==>>> epoch: 1, batch index: 200, test loss: 10.577684, acc: 0.000\n",
            "==>>> epoch: 1, batch index: 300, test loss: 10.568527, acc: 0.000\n",
            "==>>> epoch: 1, batch index: 370, test loss: 10.565430, acc: 0.000\n",
            "==>>> epoch: 2, batch index: 100, train loss: 8.042206, acc: 0.0022\n",
            "==>>> epoch: 2, batch index: 200, train loss: 7.979527, acc: 0.0027\n",
            "==>>> epoch: 2, batch index: 300, train loss: 7.951847, acc: 0.0021333333333333334\n",
            "==>>> epoch: 2, batch index: 400, train loss: 7.937477, acc: 0.00245\n",
            "==>>> epoch: 2, batch index: 500, train loss: 7.935522, acc: 0.00216\n",
            "==>>> epoch: 2, batch index: 600, train loss: 7.931131, acc: 0.0020666666666666667\n",
            "==>>> epoch: 2, batch index: 700, train loss: 7.924524, acc: 0.0019714285714285715\n",
            "==>>> epoch: 2, batch index: 800, train loss: 7.918625, acc: 0.001975\n",
            "==>>> epoch: 2, batch index: 900, train loss: 7.909246, acc: 0.0019555555555555554\n",
            "==>>> epoch: 2, batch index: 1000, train loss: 7.903941, acc: 0.00212\n",
            "==>>> epoch: 2, batch index: 1100, train loss: 7.900964, acc: 0.0020909090909090908\n",
            "==>>> epoch: 2, batch index: 1200, train loss: 7.896533, acc: 0.0021666666666666666\n",
            "==>>> epoch: 2, batch index: 1300, train loss: 7.888891, acc: 0.0023384615384615384\n",
            "==>>> epoch: 2, batch index: 1400, train loss: 7.884403, acc: 0.002357142857142857\n",
            "==>>> epoch: 2, batch index: 1500, train loss: 7.879080, acc: 0.00236\n",
            "==>>> epoch: 2, batch index: 1600, train loss: 7.874620, acc: 0.002325\n",
            "==>>> epoch: 2, batch index: 1627, train loss: 7.876482, acc: 0.0023369946249123625\n",
            "==>>> epoch: 2, batch index: 100, test loss: 11.403626, acc: 0.000\n",
            "==>>> epoch: 2, batch index: 200, test loss: 11.417219, acc: 0.000\n",
            "==>>> epoch: 2, batch index: 300, test loss: 11.414011, acc: 0.000\n",
            "==>>> epoch: 2, batch index: 370, test loss: 11.425579, acc: 0.000\n",
            "==>>> epoch: 3, batch index: 100, train loss: 7.850230, acc: 0.0024\n",
            "==>>> epoch: 3, batch index: 200, train loss: 7.791365, acc: 0.0035\n",
            "==>>> epoch: 3, batch index: 300, train loss: 7.762817, acc: 0.0036666666666666666\n",
            "==>>> epoch: 3, batch index: 400, train loss: 7.752969, acc: 0.00355\n",
            "==>>> epoch: 3, batch index: 500, train loss: 7.746988, acc: 0.00328\n",
            "==>>> epoch: 3, batch index: 600, train loss: 7.735824, acc: 0.0033666666666666667\n",
            "==>>> epoch: 3, batch index: 700, train loss: 7.728945, acc: 0.0032285714285714287\n",
            "==>>> epoch: 3, batch index: 800, train loss: 7.723918, acc: 0.00325\n",
            "==>>> epoch: 3, batch index: 900, train loss: 7.718408, acc: 0.0033777777777777777\n",
            "==>>> epoch: 3, batch index: 1000, train loss: 7.713487, acc: 0.00344\n",
            "==>>> epoch: 3, batch index: 1100, train loss: 7.707097, acc: 0.0034727272727272727\n",
            "==>>> epoch: 3, batch index: 1200, train loss: 7.701362, acc: 0.0035\n",
            "==>>> epoch: 3, batch index: 1300, train loss: 7.696953, acc: 0.0035076923076923076\n",
            "==>>> epoch: 3, batch index: 1400, train loss: 7.692064, acc: 0.0035285714285714287\n",
            "==>>> epoch: 3, batch index: 1500, train loss: 7.686936, acc: 0.003586666666666667\n",
            "==>>> epoch: 3, batch index: 1600, train loss: 7.683141, acc: 0.0036875\n",
            "==>>> epoch: 3, batch index: 1627, train loss: 7.686127, acc: 0.003677691541309455\n",
            "==>>> epoch: 3, batch index: 100, test loss: 11.994638, acc: 0.000\n",
            "==>>> epoch: 3, batch index: 200, test loss: 11.952483, acc: 0.000\n",
            "==>>> epoch: 3, batch index: 300, test loss: 11.953875, acc: 0.000\n",
            "==>>> epoch: 3, batch index: 370, test loss: 11.958204, acc: 0.000\n",
            "==>>> epoch: 4, batch index: 100, train loss: 7.619332, acc: 0.0042\n",
            "==>>> epoch: 4, batch index: 200, train loss: 7.569239, acc: 0.0041\n",
            "==>>> epoch: 4, batch index: 300, train loss: 7.535367, acc: 0.005066666666666666\n",
            "==>>> epoch: 4, batch index: 400, train loss: 7.525711, acc: 0.00615\n",
            "==>>> epoch: 4, batch index: 500, train loss: 7.518934, acc: 0.00648\n",
            "==>>> epoch: 4, batch index: 600, train loss: 7.514273, acc: 0.006733333333333333\n",
            "==>>> epoch: 4, batch index: 700, train loss: 7.510010, acc: 0.0066\n",
            "==>>> epoch: 4, batch index: 800, train loss: 7.505678, acc: 0.006675\n",
            "==>>> epoch: 4, batch index: 900, train loss: 7.498945, acc: 0.006711111111111111\n",
            "==>>> epoch: 4, batch index: 1000, train loss: 7.497205, acc: 0.00676\n",
            "==>>> epoch: 4, batch index: 1100, train loss: 7.493590, acc: 0.006909090909090909\n",
            "==>>> epoch: 4, batch index: 1200, train loss: 7.489241, acc: 0.0069\n",
            "==>>> epoch: 4, batch index: 1300, train loss: 7.484956, acc: 0.006846153846153846\n",
            "==>>> epoch: 4, batch index: 1400, train loss: 7.482689, acc: 0.006871428571428572\n",
            "==>>> epoch: 4, batch index: 1500, train loss: 7.481509, acc: 0.0068\n",
            "==>>> epoch: 4, batch index: 1600, train loss: 7.479129, acc: 0.0068375\n",
            "==>>> epoch: 4, batch index: 1627, train loss: 7.482608, acc: 0.006863384214216307\n",
            "==>>> epoch: 4, batch index: 100, test loss: 12.520314, acc: 0.000\n",
            "==>>> epoch: 4, batch index: 200, test loss: 12.437313, acc: 0.000\n",
            "==>>> epoch: 4, batch index: 300, test loss: 12.456915, acc: 0.000\n",
            "==>>> epoch: 4, batch index: 370, test loss: 12.455801, acc: 0.000\n",
            "==>>> epoch: 5, batch index: 100, train loss: 7.376306, acc: 0.0126\n",
            "==>>> epoch: 5, batch index: 200, train loss: 7.366986, acc: 0.011\n",
            "==>>> epoch: 5, batch index: 300, train loss: 7.355997, acc: 0.0094\n",
            "==>>> epoch: 5, batch index: 400, train loss: 7.350792, acc: 0.00915\n",
            "==>>> epoch: 5, batch index: 500, train loss: 7.344489, acc: 0.00952\n",
            "==>>> epoch: 5, batch index: 600, train loss: 7.345770, acc: 0.009566666666666666\n",
            "==>>> epoch: 5, batch index: 700, train loss: 7.343911, acc: 0.0096\n",
            "==>>> epoch: 5, batch index: 800, train loss: 7.338977, acc: 0.0099\n",
            "==>>> epoch: 5, batch index: 900, train loss: 7.335031, acc: 0.009977777777777778\n",
            "==>>> epoch: 5, batch index: 1000, train loss: 7.329647, acc: 0.01014\n",
            "==>>> epoch: 5, batch index: 1100, train loss: 7.324767, acc: 0.010036363636363637\n",
            "==>>> epoch: 5, batch index: 1200, train loss: 7.318847, acc: 0.010116666666666666\n",
            "==>>> epoch: 5, batch index: 1300, train loss: 7.315140, acc: 0.009876923076923077\n",
            "==>>> epoch: 5, batch index: 1400, train loss: 7.310232, acc: 0.009885714285714286\n",
            "==>>> epoch: 5, batch index: 1500, train loss: 7.306740, acc: 0.009866666666666666\n",
            "==>>> epoch: 5, batch index: 1600, train loss: 7.305845, acc: 0.009925\n",
            "==>>> epoch: 5, batch index: 1627, train loss: 7.305151, acc: 0.009852277339762119\n",
            "==>>> epoch: 5, batch index: 100, test loss: 12.978794, acc: 0.000\n",
            "==>>> epoch: 5, batch index: 200, test loss: 12.918499, acc: 0.000\n",
            "==>>> epoch: 5, batch index: 300, test loss: 12.910456, acc: 0.000\n",
            "==>>> epoch: 5, batch index: 370, test loss: 12.922218, acc: 0.000\n",
            "==>>> epoch: 6, batch index: 100, train loss: 7.192536, acc: 0.0154\n",
            "==>>> epoch: 6, batch index: 200, train loss: 7.179565, acc: 0.0138\n",
            "==>>> epoch: 6, batch index: 300, train loss: 7.158318, acc: 0.014333333333333333\n",
            "==>>> epoch: 6, batch index: 400, train loss: 7.152637, acc: 0.0142\n",
            "==>>> epoch: 6, batch index: 500, train loss: 7.151595, acc: 0.01412\n",
            "==>>> epoch: 6, batch index: 600, train loss: 7.150746, acc: 0.014133333333333333\n",
            "==>>> epoch: 6, batch index: 700, train loss: 7.147220, acc: 0.014485714285714286\n",
            "==>>> epoch: 6, batch index: 800, train loss: 7.144952, acc: 0.01435\n",
            "==>>> epoch: 6, batch index: 900, train loss: 7.143460, acc: 0.014444444444444444\n",
            "==>>> epoch: 6, batch index: 1000, train loss: 7.139674, acc: 0.0146\n",
            "==>>> epoch: 6, batch index: 1100, train loss: 7.136215, acc: 0.014618181818181818\n",
            "==>>> epoch: 6, batch index: 1200, train loss: 7.133050, acc: 0.014816666666666667\n",
            "==>>> epoch: 6, batch index: 1300, train loss: 7.128981, acc: 0.015184615384615385\n",
            "==>>> epoch: 6, batch index: 1400, train loss: 7.123596, acc: 0.015385714285714286\n",
            "==>>> epoch: 6, batch index: 1500, train loss: 7.120069, acc: 0.015706666666666667\n",
            "==>>> epoch: 6, batch index: 1600, train loss: 7.114843, acc: 0.01595\n",
            "==>>> epoch: 6, batch index: 1627, train loss: 7.117970, acc: 0.016039163109924846\n",
            "==>>> epoch: 6, batch index: 100, test loss: 13.465495, acc: 0.000\n",
            "==>>> epoch: 6, batch index: 200, test loss: 13.431781, acc: 0.000\n",
            "==>>> epoch: 6, batch index: 300, test loss: 13.459585, acc: 0.000\n",
            "==>>> epoch: 6, batch index: 370, test loss: 13.468102, acc: 0.000\n",
            "==>>> epoch: 7, batch index: 100, train loss: 7.160146, acc: 0.0174\n",
            "==>>> epoch: 7, batch index: 200, train loss: 7.073223, acc: 0.0179\n",
            "==>>> epoch: 7, batch index: 300, train loss: 7.018106, acc: 0.019666666666666666\n",
            "==>>> epoch: 7, batch index: 400, train loss: 7.001875, acc: 0.02065\n",
            "==>>> epoch: 7, batch index: 500, train loss: 6.984886, acc: 0.02096\n",
            "==>>> epoch: 7, batch index: 600, train loss: 6.979444, acc: 0.020933333333333335\n",
            "==>>> epoch: 7, batch index: 700, train loss: 6.972610, acc: 0.021628571428571428\n",
            "==>>> epoch: 7, batch index: 800, train loss: 6.964644, acc: 0.021175\n",
            "==>>> epoch: 7, batch index: 900, train loss: 6.953636, acc: 0.02157777777777778\n",
            "==>>> epoch: 7, batch index: 1000, train loss: 6.942614, acc: 0.02184\n",
            "==>>> epoch: 7, batch index: 1100, train loss: 6.935981, acc: 0.022127272727272728\n",
            "==>>> epoch: 7, batch index: 1200, train loss: 6.927530, acc: 0.022683333333333333\n",
            "==>>> epoch: 7, batch index: 1300, train loss: 6.921431, acc: 0.023\n",
            "==>>> epoch: 7, batch index: 1400, train loss: 6.919563, acc: 0.023114285714285716\n",
            "==>>> epoch: 7, batch index: 1500, train loss: 6.911417, acc: 0.023733333333333332\n",
            "==>>> epoch: 7, batch index: 1600, train loss: 6.905249, acc: 0.0239125\n",
            "==>>> epoch: 7, batch index: 1627, train loss: 6.912706, acc: 0.02398494483462688\n",
            "==>>> epoch: 7, batch index: 100, test loss: 14.042578, acc: 0.000\n",
            "==>>> epoch: 7, batch index: 200, test loss: 13.975453, acc: 0.000\n",
            "==>>> epoch: 7, batch index: 300, test loss: 13.986193, acc: 0.000\n",
            "==>>> epoch: 7, batch index: 370, test loss: 13.999071, acc: 0.000\n",
            "==>>> epoch: 8, batch index: 100, train loss: 6.787751, acc: 0.0358\n",
            "==>>> epoch: 8, batch index: 200, train loss: 6.726463, acc: 0.0358\n",
            "==>>> epoch: 8, batch index: 300, train loss: 6.698870, acc: 0.0368\n",
            "==>>> epoch: 8, batch index: 400, train loss: 6.693542, acc: 0.03625\n",
            "==>>> epoch: 8, batch index: 500, train loss: 6.695302, acc: 0.03616\n",
            "==>>> epoch: 8, batch index: 600, train loss: 6.690166, acc: 0.0359\n",
            "==>>> epoch: 8, batch index: 700, train loss: 6.682932, acc: 0.0364\n",
            "==>>> epoch: 8, batch index: 800, train loss: 6.680765, acc: 0.036475\n",
            "==>>> epoch: 8, batch index: 900, train loss: 6.675723, acc: 0.03631111111111111\n",
            "==>>> epoch: 8, batch index: 1000, train loss: 6.674984, acc: 0.0362\n",
            "==>>> epoch: 8, batch index: 1100, train loss: 6.672109, acc: 0.036763636363636366\n",
            "==>>> epoch: 8, batch index: 1200, train loss: 6.665754, acc: 0.037083333333333336\n",
            "==>>> epoch: 8, batch index: 1300, train loss: 6.660956, acc: 0.037292307692307694\n",
            "==>>> epoch: 8, batch index: 1400, train loss: 6.655022, acc: 0.0376\n",
            "==>>> epoch: 8, batch index: 1500, train loss: 6.646502, acc: 0.037906666666666665\n",
            "==>>> epoch: 8, batch index: 1600, train loss: 6.643633, acc: 0.0379875\n",
            "==>>> epoch: 8, batch index: 1627, train loss: 6.645962, acc: 0.03801921255581112\n",
            "==>>> epoch: 8, batch index: 100, test loss: 14.518337, acc: 0.000\n",
            "==>>> epoch: 8, batch index: 200, test loss: 14.462497, acc: 0.000\n",
            "==>>> epoch: 8, batch index: 300, test loss: 14.434048, acc: 0.000\n",
            "==>>> epoch: 8, batch index: 370, test loss: 14.429133, acc: 0.000\n",
            "==>>> epoch: 9, batch index: 100, train loss: 6.504577, acc: 0.0494\n",
            "==>>> epoch: 9, batch index: 200, train loss: 6.465722, acc: 0.0472\n",
            "==>>> epoch: 9, batch index: 300, train loss: 6.476504, acc: 0.047066666666666666\n",
            "==>>> epoch: 9, batch index: 400, train loss: 6.455867, acc: 0.04915\n",
            "==>>> epoch: 9, batch index: 500, train loss: 6.444432, acc: 0.04856\n",
            "==>>> epoch: 9, batch index: 600, train loss: 6.441114, acc: 0.0486\n",
            "==>>> epoch: 9, batch index: 700, train loss: 6.432651, acc: 0.04897142857142857\n",
            "==>>> epoch: 9, batch index: 800, train loss: 6.431784, acc: 0.04925\n",
            "==>>> epoch: 9, batch index: 900, train loss: 6.429980, acc: 0.05004444444444445\n",
            "==>>> epoch: 9, batch index: 1000, train loss: 6.427978, acc: 0.04996\n",
            "==>>> epoch: 9, batch index: 1100, train loss: 6.421622, acc: 0.05021818181818182\n",
            "==>>> epoch: 9, batch index: 1200, train loss: 6.415457, acc: 0.05076666666666667\n",
            "==>>> epoch: 9, batch index: 1300, train loss: 6.411301, acc: 0.051123076923076924\n",
            "==>>> epoch: 9, batch index: 1400, train loss: 6.405354, acc: 0.05167142857142857\n",
            "==>>> epoch: 9, batch index: 1500, train loss: 6.403135, acc: 0.05162666666666667\n",
            "==>>> epoch: 9, batch index: 1600, train loss: 6.398919, acc: 0.05195\n",
            "==>>> epoch: 9, batch index: 1627, train loss: 6.397598, acc: 0.05213958007896582\n",
            "==>>> epoch: 9, batch index: 100, test loss: 15.038486, acc: 0.001\n",
            "==>>> epoch: 9, batch index: 200, test loss: 14.977095, acc: 0.000\n",
            "==>>> epoch: 9, batch index: 300, test loss: 14.945301, acc: 0.000\n",
            "==>>> epoch: 9, batch index: 370, test loss: 14.950776, acc: 0.000\n",
            "==>>> epoch: 10, batch index: 100, train loss: 6.294014, acc: 0.0648\n",
            "==>>> epoch: 10, batch index: 200, train loss: 6.255729, acc: 0.0631\n",
            "==>>> epoch: 10, batch index: 300, train loss: 6.242640, acc: 0.06306666666666666\n",
            "==>>> epoch: 10, batch index: 400, train loss: 6.236371, acc: 0.06305\n",
            "==>>> epoch: 10, batch index: 500, train loss: 6.238979, acc: 0.06236\n",
            "==>>> epoch: 10, batch index: 600, train loss: 6.236936, acc: 0.06236666666666667\n",
            "==>>> epoch: 10, batch index: 700, train loss: 6.232141, acc: 0.062342857142857144\n",
            "==>>> epoch: 10, batch index: 800, train loss: 6.224219, acc: 0.063775\n",
            "==>>> epoch: 10, batch index: 900, train loss: 6.213488, acc: 0.06406666666666666\n",
            "==>>> epoch: 10, batch index: 1000, train loss: 6.201298, acc: 0.06508\n",
            "==>>> epoch: 10, batch index: 1100, train loss: 6.197830, acc: 0.06576363636363636\n",
            "==>>> epoch: 10, batch index: 1200, train loss: 6.192976, acc: 0.066\n",
            "==>>> epoch: 10, batch index: 1300, train loss: 6.189632, acc: 0.06626153846153846\n",
            "==>>> epoch: 10, batch index: 1400, train loss: 6.184063, acc: 0.06657142857142857\n",
            "==>>> epoch: 10, batch index: 1500, train loss: 6.181691, acc: 0.06681333333333334\n",
            "==>>> epoch: 10, batch index: 1600, train loss: 6.178440, acc: 0.0671875\n",
            "==>>> epoch: 10, batch index: 1627, train loss: 6.186056, acc: 0.06720704542379552\n",
            "==>>> epoch: 10, batch index: 100, test loss: 15.252139, acc: 0.000\n",
            "==>>> epoch: 10, batch index: 200, test loss: 15.245340, acc: 0.000\n",
            "==>>> epoch: 10, batch index: 300, test loss: 15.178128, acc: 0.000\n",
            "==>>> epoch: 10, batch index: 370, test loss: 15.175489, acc: 0.000\n",
            "==>>> epoch: 11, batch index: 100, train loss: 6.088456, acc: 0.0742\n",
            "==>>> epoch: 11, batch index: 200, train loss: 6.065062, acc: 0.077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-29b50e22299e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mcorrect_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;31m#for batch_idx, (x, target) in enumerate(tqdm.tqdm(train_loader)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}