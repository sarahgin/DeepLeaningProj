{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PretrainClass_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahgin/DeepLeaningProj/blob/master/PretrainClass_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOJogqNPwAfD",
        "colab_type": "text"
      },
      "source": [
        "**Mount data from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HcPTGrbS-C3",
        "colab_type": "code",
        "outputId": "df67a0ba-dff1-4714-81ef-d285384a55c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "# mount data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXS22Wk9TFb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir /Drive\n",
        "!ls /Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIL8rNKcwGAw",
        "colab_type": "text"
      },
      "source": [
        "**Extract data from zip files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R91JpZNRTJ5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import glob\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "for src in glob.glob('/content/gdrive/My Drive/yt_bb_detection_train/*.zip'):\n",
        "  dst = f'/Drive/{os.path.basename(src)}'\n",
        "  print(src, dst)    \n",
        "  if os.path.exists(src) and not os.path.exists(dst):\n",
        "    print(f'copy {src} to {dst}')\n",
        "    shutil.copy2(src, dst)\n",
        "  \n",
        " # Create a ZipFile Object and load sample.zip in it\n",
        "for z in glob.glob('/Drive/*zip'):\n",
        "  with ZipFile(z, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall('/Drive')\n",
        "    \n",
        "!ls /Drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhWUFAzwwK93",
        "colab_type": "text"
      },
      "source": [
        "**Data partition into train and test folders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iNJZOanTMtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "def list_dirs(directory):\n",
        "    \"\"\"Returns all directories in a given directory\n",
        "    \"\"\"\n",
        "    return [f for f in pathlib.Path(directory).iterdir() if f.is_dir()]\n",
        "\n",
        "def list_files(directory):\n",
        "    \"\"\"Returns all files in a given directory\n",
        "    \"\"\"\n",
        "    return [\n",
        "        f\n",
        "        for f in pathlib.Path(directory).iterdir()\n",
        "        if f.is_file() and not f.name.startswith(\".\")\n",
        "    ]\n",
        "\n",
        "def setup_files(class_dir, seed):\n",
        "    \"\"\"Returns shuffled files\n",
        "    \"\"\"\n",
        "    # make sure its reproducible\n",
        "    random.seed(seed)\n",
        "\n",
        "    files = list_files(class_dir)\n",
        "\n",
        "    files.sort()\n",
        "    random.shuffle(files)\n",
        "    return files\n",
        "\n",
        "def ratio(input, output=\"output\", seed=1337, ratio=(0.8, 0.1, 0.1)):\n",
        "    # make up for some impression\n",
        "    assert round(sum(ratio), 5) == 1\n",
        "    assert len(ratio) in (2, 3)\n",
        "\n",
        "    for class_dir in list_dirs(input):\n",
        "        for instance_dir in list_dirs(class_dir):\n",
        "            instancename = os.path.basename(instance_dir)\n",
        "            classname = os.path.basename(class_dir)\n",
        "            fulloutput = os.path.join(classname,instancename)\n",
        "            #output_dir = os.path.join(output, classname ,instancename)\n",
        "            split_class_dir_ratio(instance_dir, output, fulloutput, ratio, seed, None)\n",
        "\n",
        "\n",
        "def split_class_dir_fixed(class_dir, output, fixed, seed, prog_bar):\n",
        "    \"\"\"Splits one very class folder\n",
        "    \"\"\"\n",
        "    files = setup_files(class_dir, seed)\n",
        "\n",
        "    if not len(files) > sum(fixed):\n",
        "        raise ValueError(\n",
        "            f'The number of samples in class \"{class_dir.stem}\" are too few. There are only {len(files)} samples available but your fixed parameter {fixed} requires at least {sum(fixed)} files. You may want to split your classes by ratio.'\n",
        "        )\n",
        "\n",
        "    split_train = len(files) - sum(fixed)\n",
        "    split_val = split_train + fixed[0]\n",
        "\n",
        "    li = split_files(files, split_train, split_val, len(fixed) == 2)\n",
        "    copy_files(li, class_dir, output, prog_bar)\n",
        "    return len(files)\n",
        "\n",
        "def split_class_dir_ratio(class_dir, output, fulloutput, ratio, seed, prog_bar):\n",
        "    \"\"\"Splits one very class folder\n",
        "    \"\"\"\n",
        "    files = setup_files(class_dir, seed)\n",
        "\n",
        "    split_train = int(ratio[0] * len(files))\n",
        "    split_val = split_train + int(ratio[1] * len(files))\n",
        "\n",
        "    li = split_files(files, split_train, split_val, len(ratio) == 3)\n",
        "    copy_files(li, class_dir, output, fulloutput, prog_bar)\n",
        "\n",
        "\n",
        "def split_files(files, split_train, split_val, use_test):\n",
        "    \"\"\"Splits the files along the provided indices\n",
        "    \"\"\"\n",
        "    files_train = files[:split_train]\n",
        "    files_val = files[split_train:split_val] if use_test else files[split_train:]\n",
        "\n",
        "    li = [(files_train, \"train\"), (files_val, \"test\")]\n",
        "\n",
        "    # optional test folder\n",
        "    if use_test:\n",
        "        files_test = files[split_val:]\n",
        "        li.append((files_test, \"test\"))\n",
        "    return li\n",
        "\n",
        "\n",
        "def copy_files(files_type, class_dir, output, fulloutput, prog_bar):\n",
        "    \"\"\"Copies the files from the input folder to the output folder\n",
        "    \"\"\"\n",
        "    # get the last part within the file\n",
        "    for (files, folder_type) in files_type:\n",
        "        full_path = os.path.join(output, folder_type, fulloutput)\n",
        "\n",
        "        pathlib.Path(full_path).mkdir(parents=True, exist_ok=True)\n",
        "        for f in files:\n",
        "            if not prog_bar is None:\n",
        "                prog_bar.update()\n",
        "            shutil.copy2(f, full_path)\n",
        "            \n",
        "            \n",
        "ratio('/Drive', output='/Drive/data/', seed=1337, ratio=(.8, .2))  #the partition to 80% train 20% test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBPIZ5gbwP__",
        "colab_type": "text"
      },
      "source": [
        "**Class Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuQBZRFMUY_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataSet object\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import imageio\n",
        "import numpy as np  \n",
        "from PIL import Image\n",
        "\n",
        "class ClassDataset(Dataset):\n",
        "      \n",
        "  def __init__(self, basedir, transform, percentage):\n",
        "        super().__init__()\n",
        "        files = glob.glob(os.path.join(basedir ,'**/*.jpg'), recursive=True)\n",
        "        if(percentage < 100):\n",
        "          files_num = len(list(files))\n",
        "          choosefilesnumber = int(files_num * percentage / 100)\n",
        "          print(f'choosefilesnumber: {choosefilesnumber}  filesnum: {files_num}')\n",
        "          print((list(files)))\n",
        "          selectedfiles = np.random.choice(files, choosefilesnumber, replace=False)\n",
        "        elif(percentage == 100):\n",
        "          selectedfiles = files\n",
        "        \n",
        "        self.data = pd.DataFrame([self._split_file(f) for f in selectedfiles], \n",
        "                            columns=['class_id', 'file_path'])\n",
        "        names  = np.unique(self.data['class_id'])\n",
        "        \n",
        "        self.classDict = {str:index for index, str in enumerate(names)}          \n",
        "        self.data['class_num'] = self.data['class_id'].map(self.classDict)\n",
        "        self.transform = transform\n",
        "        \n",
        "  def _split_file(self, f):\n",
        "        parts = f.split(os.sep)[-3:-1]\n",
        "        return parts[0], f   #label is originaly a str\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      dat = self.data.iloc[index]\n",
        "      img = Image.open(dat['file_path'])\n",
        "      if self.transform:\n",
        "          img = self.transform(img)\n",
        "      img = np.resize(img, (3,128,128))\n",
        "      img = np.asarray(img)\n",
        "      return (img.astype(np.float32), dat['class_num'])\n",
        "     \n",
        "  def __len__(self):\n",
        "      return len(self.data)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psqrSYRXwZcF",
        "colab_type": "text"
      },
      "source": [
        "**Create train and test datasets including augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj-39nGzUeRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n",
        "#basedir = '/content/gdrive/My Drive/video_data/'       \n",
        "trainbasedir = '/Drive/data/train'   \n",
        "testbasedir = '/Drive/data/test'\n",
        "\n",
        "data_transforms = transforms.Compose([transforms.Resize((256,256)),transforms.RandomResizedCrop(224), \n",
        "                         transforms.RandomHorizontalFlip(), #ImageNetPolicy(), \n",
        "                         transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_set = ClassDataset(trainbasedir, data_transforms, 60)\n",
        "test_set = ClassDataset(testbasedir, None, 100)             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlSvGQU-weOi",
        "colab_type": "text"
      },
      "source": [
        "**Create data loaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3eNvtWkUpYB",
        "colab_type": "code",
        "outputId": "0de1819c-06ad-42d7-c180-147c57e6c430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True,\n",
        "                 num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=test_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=False,\n",
        "                 num_workers=4)\n",
        "\n",
        "print('Train size: {}'.format(len(train_loader)))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOl4lcmOwhNu",
        "colab_type": "text"
      },
      "source": [
        "**Confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH7hfQ8JUvaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perf_measure(y_actual, y_pred):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    cm = np.zeros((2,2))\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_actual[i]==y_pred[i]:\n",
        "           TP += 1\n",
        "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
        "           FP += 1\n",
        "        if y_actual[i]==y_pred[i]:\n",
        "           TN += 1\n",
        "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
        "           FN += 1\n",
        "\n",
        "        cm[0][0] = TP\n",
        "        cm[0][1] = FP\n",
        "        cm[1][0]  = FN\n",
        "        cm[1][1]  = TN\n",
        "\n",
        "    return (cm)\n",
        "  \n",
        "def create_confusion_matrix_fig(c_cm):\n",
        "    fig = plt.figure(figsize=(14, 12))\n",
        "    plt.imshow(c_cm, interpolation='nearest')\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.colorbar()\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9KXBaFrwk0F",
        "colab_type": "text"
      },
      "source": [
        "**Training model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZftbY7iEU1sG",
        "colab_type": "code",
        "outputId": "bf508672-0170-48d6-b272-e8a51969ada8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "\n",
        "import time \n",
        "import tqdm\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "assert use_cuda\n",
        "\n",
        "model = models.resnet18(pretrained=True) #for third net to load our trained instance net\n",
        "\n",
        "# Writer will output to ./runs/ directory by default\n",
        "writer = SummaryWriter('with_cm')    \n",
        "    \n",
        "num_final_in = model.fc.in_features\n",
        "\n",
        "NUM_CLASSES_Class = 23\n",
        "\n",
        "model.fc = nn.Linear(num_final_in, NUM_CLASSES_Class)      \n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003) #, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.cuda()\n",
        "  model.to(torch.device(\"cuda\"))\n",
        "\n",
        "#model = torch.load('/content/gdrive/My Drive/myModels/InstanceModel.pth')\n",
        "#num_final_in = model.fc.in_features\n",
        "#print(f'last fc number: {num_final_in})\n",
        "#model.fc = nn.Linear(num_final_in, NUM_CLASSES_Instance) \n",
        "\n",
        "for epoch in range(50):\n",
        "    # trainning\n",
        "    sum_loss = 0 \n",
        "    total_cnt = 0\n",
        "    correct_cnt = 0\n",
        "    tf = time.time()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "    #for batch_idx, (x, target) in enumerate(tqdm.tqdm(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        \n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)       \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "                \n",
        "        pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label[1] == target.data).sum()\n",
        "        sum_loss += loss.data\n",
        "        \n",
        "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {}'.format(\n",
        "                epoch, batch_idx+1, sum_loss/batch_idx, float(correct_cnt)/total_cnt))    \n",
        "            \n",
        "    cm = perf_measure(target.data, pred_label[1])\n",
        "    cur_fig = create_confusion_matrix_fig(cm)\n",
        "    writer.add_figure('train_confusion_matrix', cur_fig, global_step=None, close=True, walltime=None)\n",
        "    writer.add_scalar('train accuracy', float(correct_cnt)/total_cnt, epoch)\n",
        "    writer.add_scalar('train loss', sum_loss/len(train_loader), epoch)\n",
        "    \n",
        "     # testing    \n",
        "    correct_cnt, sum_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        x = x.float()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)   \n",
        "        loss = criterion(out, target)\n",
        "        \n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth averages\n",
        "        sum_loss += loss.data\n",
        "        \n",
        "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
        "                epoch, batch_idx+1, sum_loss/batch_idx,float(correct_cnt)/total_cnt))\n",
        "    \n",
        "    try:\n",
        "      cm = perf_measure(target.data, pred_label)\n",
        "      cur_fig = create_confusion_matrix_fig(cm)\n",
        "      writer.add_figure('test_confusion_matrix', cur_fig, global_step=None, close=True, walltime=None)\n",
        "    except:\n",
        "      print(\"An exception in test_confusion_matrix\")\n",
        "      continue\n",
        "    writer.add_scalar('test accuracy', float(correct_cnt)/total_cnt, epoch)\n",
        "    writer.add_scalar('test loss', sum_loss/len(test_loader), epoch)\n",
        "\n",
        "#torch.save(model.state_dict(), 'yt_bb_detection_train/mymodel3')\n",
        "writer.close()\n",
        "\n",
        "elapsed = time.time() - tf\n",
        "print(f'Elapsed time: {elapsed}')\n",
        "\n",
        "#orch.save(model, '/content/gdrive/My Drive/myModels/newClassNoPretrainModel.pth')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 44.7M/44.7M [00:03<00:00, 11.9MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==>>> epoch: 0, batch index: 100, train loss: 3.119800, acc: 0.2306\n",
            "==>>> epoch: 0, batch index: 200, train loss: 3.020582, acc: 0.2326\n",
            "==>>> epoch: 0, batch index: 300, train loss: 2.958454, acc: 0.23206666666666667\n",
            "==>>> epoch: 0, batch index: 400, train loss: 2.922987, acc: 0.2312\n",
            "==>>> epoch: 0, batch index: 500, train loss: 2.899788, acc: 0.22952\n",
            "==>>> epoch: 0, batch index: 600, train loss: 2.870196, acc: 0.23246666666666665\n",
            "==>>> epoch: 0, batch index: 700, train loss: 2.848921, acc: 0.23311428571428572\n",
            "==>>> epoch: 0, batch index: 800, train loss: 2.830535, acc: 0.235125\n",
            "==>>> epoch: 0, batch index: 900, train loss: 2.819038, acc: 0.23633333333333334\n",
            "==>>> epoch: 0, batch index: 976, train loss: 2.809015, acc: 0.23667486674866747\n",
            "==>>> epoch: 0, batch index: 100, test loss: 3.567758, acc: 0.008\n",
            "==>>> epoch: 0, batch index: 200, test loss: 2.679602, acc: 0.364\n",
            "==>>> epoch: 0, batch index: 300, test loss: 2.933280, acc: 0.247\n",
            "==>>> epoch: 0, batch index: 370, test loss: 3.116197, acc: 0.202\n",
            "==>>> epoch: 1, batch index: 100, train loss: 2.678675, acc: 0.2496\n",
            "==>>> epoch: 1, batch index: 200, train loss: 2.665035, acc: 0.2499\n",
            "==>>> epoch: 1, batch index: 300, train loss: 2.669535, acc: 0.2454\n",
            "==>>> epoch: 1, batch index: 400, train loss: 2.661403, acc: 0.24775\n",
            "==>>> epoch: 1, batch index: 500, train loss: 2.657346, acc: 0.24924\n",
            "==>>> epoch: 1, batch index: 600, train loss: 2.650442, acc: 0.25143333333333334\n",
            "==>>> epoch: 1, batch index: 700, train loss: 2.644578, acc: 0.2512285714285714\n",
            "==>>> epoch: 1, batch index: 800, train loss: 2.638005, acc: 0.252575\n",
            "==>>> epoch: 1, batch index: 900, train loss: 2.632480, acc: 0.2540222222222222\n",
            "==>>> epoch: 1, batch index: 976, train loss: 2.628258, acc: 0.2555350553505535\n",
            "==>>> epoch: 1, batch index: 100, test loss: 3.897777, acc: 0.003\n",
            "==>>> epoch: 1, batch index: 200, test loss: 2.911738, acc: 0.287\n",
            "==>>> epoch: 1, batch index: 300, test loss: 3.139801, acc: 0.201\n",
            "==>>> epoch: 1, batch index: 370, test loss: 3.271278, acc: 0.167\n",
            "==>>> epoch: 2, batch index: 100, train loss: 2.597365, acc: 0.2684\n",
            "==>>> epoch: 2, batch index: 200, train loss: 2.569770, acc: 0.265\n",
            "==>>> epoch: 2, batch index: 300, train loss: 2.562446, acc: 0.26693333333333336\n",
            "==>>> epoch: 2, batch index: 400, train loss: 2.556319, acc: 0.26575\n",
            "==>>> epoch: 2, batch index: 500, train loss: 2.546969, acc: 0.2688\n",
            "==>>> epoch: 2, batch index: 600, train loss: 2.544367, acc: 0.26793333333333336\n",
            "==>>> epoch: 2, batch index: 700, train loss: 2.538913, acc: 0.26877142857142855\n",
            "==>>> epoch: 2, batch index: 800, train loss: 2.531901, acc: 0.2704\n",
            "==>>> epoch: 2, batch index: 900, train loss: 2.526468, acc: 0.27197777777777776\n",
            "==>>> epoch: 2, batch index: 976, train loss: 2.524858, acc: 0.27187371873718735\n",
            "==>>> epoch: 2, batch index: 100, test loss: 3.697110, acc: 0.020\n",
            "==>>> epoch: 2, batch index: 200, test loss: 2.924968, acc: 0.244\n",
            "==>>> epoch: 2, batch index: 300, test loss: 3.149290, acc: 0.177\n",
            "==>>> epoch: 2, batch index: 370, test loss: 3.277270, acc: 0.148\n",
            "==>>> epoch: 3, batch index: 100, train loss: 2.499375, acc: 0.282\n",
            "==>>> epoch: 3, batch index: 200, train loss: 2.515166, acc: 0.2765\n",
            "==>>> epoch: 3, batch index: 300, train loss: 2.498420, acc: 0.27986666666666665\n",
            "==>>> epoch: 3, batch index: 400, train loss: 2.486620, acc: 0.28195\n",
            "==>>> epoch: 3, batch index: 500, train loss: 2.483633, acc: 0.28044\n",
            "==>>> epoch: 3, batch index: 600, train loss: 2.480375, acc: 0.2803\n",
            "==>>> epoch: 3, batch index: 700, train loss: 2.473794, acc: 0.2823428571428571\n",
            "==>>> epoch: 3, batch index: 800, train loss: 2.471955, acc: 0.282875\n",
            "==>>> epoch: 3, batch index: 900, train loss: 2.469111, acc: 0.2840888888888889\n",
            "==>>> epoch: 3, batch index: 976, train loss: 2.465790, acc: 0.2852808528085281\n",
            "==>>> epoch: 3, batch index: 100, test loss: 3.752854, acc: 0.014\n",
            "==>>> epoch: 3, batch index: 200, test loss: 2.842753, acc: 0.297\n",
            "==>>> epoch: 3, batch index: 300, test loss: 3.158509, acc: 0.205\n",
            "==>>> epoch: 3, batch index: 370, test loss: 3.293894, acc: 0.172\n",
            "==>>> epoch: 4, batch index: 100, train loss: 2.409805, acc: 0.3094\n",
            "==>>> epoch: 4, batch index: 200, train loss: 2.418933, acc: 0.3012\n",
            "==>>> epoch: 4, batch index: 300, train loss: 2.415456, acc: 0.2998\n",
            "==>>> epoch: 4, batch index: 400, train loss: 2.413006, acc: 0.3001\n",
            "==>>> epoch: 4, batch index: 500, train loss: 2.410731, acc: 0.3004\n",
            "==>>> epoch: 4, batch index: 600, train loss: 2.410964, acc: 0.30043333333333333\n",
            "==>>> epoch: 4, batch index: 700, train loss: 2.407559, acc: 0.3011142857142857\n",
            "==>>> epoch: 4, batch index: 800, train loss: 2.404322, acc: 0.3019\n",
            "==>>> epoch: 4, batch index: 900, train loss: 2.408967, acc: 0.30102222222222225\n",
            "==>>> epoch: 4, batch index: 976, train loss: 2.409391, acc: 0.3007790077900779\n",
            "==>>> epoch: 4, batch index: 100, test loss: 3.847626, acc: 0.021\n",
            "==>>> epoch: 4, batch index: 200, test loss: 2.948098, acc: 0.272\n",
            "==>>> epoch: 4, batch index: 300, test loss: 3.229965, acc: 0.190\n",
            "==>>> epoch: 4, batch index: 370, test loss: 3.351802, acc: 0.159\n",
            "==>>> epoch: 5, batch index: 100, train loss: 2.404242, acc: 0.3014\n",
            "==>>> epoch: 5, batch index: 200, train loss: 2.400362, acc: 0.3043\n",
            "==>>> epoch: 5, batch index: 300, train loss: 2.397687, acc: 0.30366666666666664\n",
            "==>>> epoch: 5, batch index: 400, train loss: 2.393923, acc: 0.3042\n",
            "==>>> epoch: 5, batch index: 500, train loss: 2.389273, acc: 0.3036\n",
            "==>>> epoch: 5, batch index: 600, train loss: 2.386137, acc: 0.30543333333333333\n",
            "==>>> epoch: 5, batch index: 700, train loss: 2.384473, acc: 0.3056\n",
            "==>>> epoch: 5, batch index: 800, train loss: 2.381454, acc: 0.305975\n",
            "==>>> epoch: 5, batch index: 900, train loss: 2.378162, acc: 0.307\n",
            "==>>> epoch: 5, batch index: 976, train loss: 2.379667, acc: 0.3062320623206232\n",
            "==>>> epoch: 5, batch index: 100, test loss: 3.940209, acc: 0.017\n",
            "==>>> epoch: 5, batch index: 200, test loss: 2.948641, acc: 0.270\n",
            "==>>> epoch: 5, batch index: 300, test loss: 3.228722, acc: 0.189\n",
            "==>>> epoch: 5, batch index: 370, test loss: 3.380982, acc: 0.159\n",
            "==>>> epoch: 6, batch index: 100, train loss: 2.385176, acc: 0.3088\n",
            "==>>> epoch: 6, batch index: 200, train loss: 2.362154, acc: 0.3117\n",
            "==>>> epoch: 6, batch index: 300, train loss: 2.349921, acc: 0.3142\n",
            "==>>> epoch: 6, batch index: 400, train loss: 2.342854, acc: 0.3152\n",
            "==>>> epoch: 6, batch index: 500, train loss: 2.350036, acc: 0.31384\n",
            "==>>> epoch: 6, batch index: 600, train loss: 2.353487, acc: 0.3131333333333333\n",
            "==>>> epoch: 6, batch index: 700, train loss: 2.351752, acc: 0.3134\n",
            "==>>> epoch: 6, batch index: 800, train loss: 2.347073, acc: 0.315275\n",
            "==>>> epoch: 6, batch index: 900, train loss: 2.344750, acc: 0.31593333333333334\n",
            "==>>> epoch: 6, batch index: 976, train loss: 2.342594, acc: 0.3159491594915949\n",
            "==>>> epoch: 6, batch index: 100, test loss: 3.932812, acc: 0.017\n",
            "==>>> epoch: 6, batch index: 200, test loss: 3.014251, acc: 0.245\n",
            "==>>> epoch: 6, batch index: 300, test loss: 3.303427, acc: 0.175\n",
            "==>>> epoch: 6, batch index: 370, test loss: 3.453630, acc: 0.149\n",
            "==>>> epoch: 7, batch index: 100, train loss: 2.341942, acc: 0.3138\n",
            "==>>> epoch: 7, batch index: 200, train loss: 2.329538, acc: 0.315\n",
            "==>>> epoch: 7, batch index: 300, train loss: 2.316175, acc: 0.3191333333333333\n",
            "==>>> epoch: 7, batch index: 400, train loss: 2.313801, acc: 0.32025\n",
            "==>>> epoch: 7, batch index: 500, train loss: 2.312064, acc: 0.32232\n",
            "==>>> epoch: 7, batch index: 600, train loss: 2.315394, acc: 0.3212333333333333\n",
            "==>>> epoch: 7, batch index: 700, train loss: 2.316231, acc: 0.3216\n",
            "==>>> epoch: 7, batch index: 800, train loss: 2.316208, acc: 0.3223\n",
            "==>>> epoch: 7, batch index: 900, train loss: 2.315229, acc: 0.3225777777777778\n",
            "==>>> epoch: 7, batch index: 976, train loss: 2.312646, acc: 0.3229807298072981\n",
            "==>>> epoch: 7, batch index: 100, test loss: 4.004579, acc: 0.016\n",
            "==>>> epoch: 7, batch index: 200, test loss: 3.063920, acc: 0.233\n",
            "==>>> epoch: 7, batch index: 300, test loss: 3.357577, acc: 0.166\n",
            "==>>> epoch: 7, batch index: 370, test loss: 3.517975, acc: 0.141\n",
            "==>>> epoch: 8, batch index: 100, train loss: 2.324872, acc: 0.3354\n",
            "==>>> epoch: 8, batch index: 200, train loss: 2.296760, acc: 0.3354\n",
            "==>>> epoch: 8, batch index: 300, train loss: 2.293482, acc: 0.3330666666666667\n",
            "==>>> epoch: 8, batch index: 400, train loss: 2.288950, acc: 0.33315\n",
            "==>>> epoch: 8, batch index: 500, train loss: 2.285919, acc: 0.33296\n",
            "==>>> epoch: 8, batch index: 600, train loss: 2.289296, acc: 0.33113333333333334\n",
            "==>>> epoch: 8, batch index: 700, train loss: 2.287479, acc: 0.3318\n",
            "==>>> epoch: 8, batch index: 800, train loss: 2.279896, acc: 0.3332\n",
            "==>>> epoch: 8, batch index: 900, train loss: 2.280642, acc: 0.33353333333333335\n",
            "==>>> epoch: 8, batch index: 976, train loss: 2.278238, acc: 0.33396883968839686\n",
            "==>>> epoch: 8, batch index: 100, test loss: 3.997876, acc: 0.017\n",
            "==>>> epoch: 8, batch index: 200, test loss: 3.032295, acc: 0.251\n",
            "==>>> epoch: 8, batch index: 300, test loss: 3.357137, acc: 0.175\n",
            "==>>> epoch: 8, batch index: 370, test loss: 3.514519, acc: 0.149\n",
            "==>>> epoch: 9, batch index: 100, train loss: 2.273573, acc: 0.347\n",
            "==>>> epoch: 9, batch index: 200, train loss: 2.257119, acc: 0.346\n",
            "==>>> epoch: 9, batch index: 300, train loss: 2.262538, acc: 0.34313333333333335\n",
            "==>>> epoch: 9, batch index: 400, train loss: 2.273996, acc: 0.3393\n",
            "==>>> epoch: 9, batch index: 500, train loss: 2.277594, acc: 0.33748\n",
            "==>>> epoch: 9, batch index: 600, train loss: 2.276939, acc: 0.3378\n",
            "==>>> epoch: 9, batch index: 700, train loss: 2.275137, acc: 0.33762857142857144\n",
            "==>>> epoch: 9, batch index: 800, train loss: 2.266975, acc: 0.33965\n",
            "==>>> epoch: 9, batch index: 900, train loss: 2.264224, acc: 0.33982222222222225\n",
            "==>>> epoch: 9, batch index: 976, train loss: 2.263307, acc: 0.339749897498975\n",
            "==>>> epoch: 9, batch index: 100, test loss: 3.897843, acc: 0.022\n",
            "==>>> epoch: 9, batch index: 200, test loss: 2.926763, acc: 0.249\n",
            "==>>> epoch: 9, batch index: 300, test loss: 3.315304, acc: 0.173\n",
            "==>>> epoch: 9, batch index: 370, test loss: 3.472743, acc: 0.147\n",
            "==>>> epoch: 10, batch index: 100, train loss: 2.268268, acc: 0.3358\n",
            "==>>> epoch: 10, batch index: 200, train loss: 2.249375, acc: 0.3415\n",
            "==>>> epoch: 10, batch index: 300, train loss: 2.235407, acc: 0.34386666666666665\n",
            "==>>> epoch: 10, batch index: 400, train loss: 2.230614, acc: 0.3455\n",
            "==>>> epoch: 10, batch index: 500, train loss: 2.261936, acc: 0.33756\n",
            "==>>> epoch: 10, batch index: 600, train loss: 2.286012, acc: 0.3305666666666667\n",
            "==>>> epoch: 10, batch index: 700, train loss: 2.291524, acc: 0.32954285714285714\n",
            "==>>> epoch: 10, batch index: 800, train loss: 2.293103, acc: 0.329775\n",
            "==>>> epoch: 10, batch index: 900, train loss: 2.294843, acc: 0.32911111111111113\n",
            "==>>> epoch: 10, batch index: 976, train loss: 2.297139, acc: 0.3293972939729397\n",
            "==>>> epoch: 10, batch index: 100, test loss: 3.915038, acc: 0.020\n",
            "==>>> epoch: 10, batch index: 200, test loss: 2.884102, acc: 0.267\n",
            "==>>> epoch: 10, batch index: 300, test loss: 3.194690, acc: 0.188\n",
            "==>>> epoch: 10, batch index: 370, test loss: 3.362283, acc: 0.158\n",
            "==>>> epoch: 11, batch index: 100, train loss: 2.277532, acc: 0.347\n",
            "==>>> epoch: 11, batch index: 200, train loss: 2.259866, acc: 0.344\n",
            "==>>> epoch: 11, batch index: 300, train loss: 2.249290, acc: 0.3426\n",
            "==>>> epoch: 11, batch index: 400, train loss: 2.241965, acc: 0.3435\n",
            "==>>> epoch: 11, batch index: 500, train loss: 2.240062, acc: 0.34408\n",
            "==>>> epoch: 11, batch index: 600, train loss: 2.237777, acc: 0.34396666666666664\n",
            "==>>> epoch: 11, batch index: 700, train loss: 2.236778, acc: 0.34374285714285713\n",
            "==>>> epoch: 11, batch index: 800, train loss: 2.234354, acc: 0.344225\n",
            "==>>> epoch: 11, batch index: 900, train loss: 2.228221, acc: 0.34586666666666666\n",
            "==>>> epoch: 11, batch index: 976, train loss: 2.226187, acc: 0.34706847068470686\n",
            "==>>> epoch: 11, batch index: 100, test loss: 4.034213, acc: 0.018\n",
            "==>>> epoch: 11, batch index: 200, test loss: 3.048794, acc: 0.235\n",
            "==>>> epoch: 11, batch index: 300, test loss: 3.350303, acc: 0.166\n",
            "==>>> epoch: 11, batch index: 370, test loss: 3.493351, acc: 0.143\n",
            "==>>> epoch: 12, batch index: 100, train loss: 2.204772, acc: 0.364\n",
            "==>>> epoch: 12, batch index: 200, train loss: 2.207419, acc: 0.3564\n",
            "==>>> epoch: 12, batch index: 300, train loss: 2.189399, acc: 0.3592\n",
            "==>>> epoch: 12, batch index: 400, train loss: 2.188101, acc: 0.35975\n",
            "==>>> epoch: 12, batch index: 500, train loss: 2.185816, acc: 0.36132\n",
            "==>>> epoch: 12, batch index: 600, train loss: 2.187600, acc: 0.3612666666666667\n",
            "==>>> epoch: 12, batch index: 700, train loss: 2.183796, acc: 0.3615714285714286\n",
            "==>>> epoch: 12, batch index: 800, train loss: 2.181658, acc: 0.361125\n",
            "==>>> epoch: 12, batch index: 900, train loss: 2.187008, acc: 0.3582888888888889\n",
            "==>>> epoch: 12, batch index: 976, train loss: 2.185700, acc: 0.3587125871258713\n",
            "==>>> epoch: 12, batch index: 100, test loss: 3.979050, acc: 0.027\n",
            "==>>> epoch: 12, batch index: 200, test loss: 3.091077, acc: 0.213\n",
            "==>>> epoch: 12, batch index: 300, test loss: 3.389388, acc: 0.154\n",
            "==>>> epoch: 12, batch index: 370, test loss: 3.550235, acc: 0.131\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-efaff7870c04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}