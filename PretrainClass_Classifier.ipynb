{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PretrainClass_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahgin/DeepLeaningProj/blob/master/PretrainClass_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HcPTGrbS-C3",
        "colab_type": "code",
        "outputId": "e69a6f59-324d-49d1-fd8f-a39975edfc91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# mount data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXS22Wk9TFb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir /Drive\n",
        "!ls /Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R91JpZNRTJ5f",
        "colab_type": "code",
        "outputId": "3f01c8a0-2801-47ca-9c7b-b0e10e8e4cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "import shutil\n",
        "import glob\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "for src in glob.glob('/content/gdrive/My Drive/videos_2/yt_bb_detection_train/*.zip'):\n",
        "  dst = f'/Drive/{os.path.basename(src)}'\n",
        "  print(src, dst)    \n",
        "  if os.path.exists(src) and not os.path.exists(dst):\n",
        "    print(f'copy {src} to {dst}')\n",
        "    shutil.copy2(src, dst)\n",
        "  \n",
        " # Create a ZipFile Object and load sample.zip in it\n",
        "for z in glob.glob('/Drive/*zip'):\n",
        "  with ZipFile(z, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall('/Drive')\n",
        "    \n",
        "!ls /Drive\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/0.zip /Drive/0.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/0.zip to /Drive/0.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/1.zip /Drive/1.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/1.zip to /Drive/1.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/2.zip /Drive/2.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/2.zip to /Drive/2.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/3.zip /Drive/3.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/3.zip to /Drive/3.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/4.zip /Drive/4.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/4.zip to /Drive/4.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/5.zip /Drive/5.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/5.zip to /Drive/5.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/6.zip /Drive/6.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/6.zip to /Drive/6.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/7.zip /Drive/7.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/7.zip to /Drive/7.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/8.zip /Drive/8.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/8.zip to /Drive/8.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/9.zip /Drive/9.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/9.zip to /Drive/9.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/10.zip /Drive/10.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/10.zip to /Drive/10.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/11.zip /Drive/11.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/11.zip to /Drive/11.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/12.zip /Drive/12.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/12.zip to /Drive/12.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/13.zip /Drive/13.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/13.zip to /Drive/13.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/14.zip /Drive/14.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/14.zip to /Drive/14.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/15.zip /Drive/15.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/15.zip to /Drive/15.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/16.zip /Drive/16.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/16.zip to /Drive/16.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/17.zip /Drive/17.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/17.zip to /Drive/17.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/18.zip /Drive/18.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/18.zip to /Drive/18.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/19.zip /Drive/19.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/19.zip to /Drive/19.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/20.zip /Drive/20.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/20.zip to /Drive/20.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/21.zip /Drive/21.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/21.zip to /Drive/21.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/23.zip /Drive/23.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/23.zip to /Drive/23.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/yt_bb_detection_train.zip /Drive/yt_bb_detection_train.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/yt_bb_detection_train.zip to /Drive/yt_bb_detection_train.zip\n",
            "0\t12\t15.zip\t19\t21.zip\t4.zip  8\n",
            "0.zip\t12.zip\t16\t19.zip\t23\t5      8.zip\n",
            "1\t13\t16.zip\t1.zip\t23.zip\t5.zip  9\n",
            "10\t13.zip\t17\t2\t2.zip\t6      9.zip\n",
            "10.zip\t14\t17.zip\t20\t3\t6.zip  yt_bb_detection_train.zip\n",
            "11\t14.zip\t18\t20.zip\t3.zip\t7\n",
            "11.zip\t15\t18.zip\t21\t4\t7.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iNJZOanTMtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "def list_dirs(directory):\n",
        "    \"\"\"Returns all directories in a given directory\n",
        "    \"\"\"\n",
        "    return [f for f in pathlib.Path(directory).iterdir() if f.is_dir()]\n",
        "\n",
        "def list_files(directory):\n",
        "    \"\"\"Returns all files in a given directory\n",
        "    \"\"\"\n",
        "    return [\n",
        "        f\n",
        "        for f in pathlib.Path(directory).iterdir()\n",
        "        if f.is_file() and not f.name.startswith(\".\")\n",
        "    ]\n",
        "\n",
        "def setup_files(class_dir, seed):\n",
        "    \"\"\"Returns shuffled files\n",
        "    \"\"\"\n",
        "    # make sure its reproducible\n",
        "    random.seed(seed)\n",
        "\n",
        "    files = list_files(class_dir)\n",
        "\n",
        "    files.sort()\n",
        "    random.shuffle(files)\n",
        "    return files\n",
        "\n",
        "def ratio(input, output=\"output\", seed=1337, ratio=(0.8, 0.1, 0.1)):\n",
        "    # make up for some impression\n",
        "    assert round(sum(ratio), 5) == 1\n",
        "    assert len(ratio) in (2, 3)\n",
        "\n",
        "    for class_dir in list_dirs(input):\n",
        "        for instance_dir in list_dirs(class_dir):\n",
        "            instancename = os.path.basename(instance_dir)\n",
        "            classname = os.path.basename(class_dir)\n",
        "            fulloutput = os.path.join(classname,instancename)\n",
        "            #output_dir = os.path.join(output, classname ,instancename)\n",
        "            split_class_dir_ratio(instance_dir, output, fulloutput, ratio, seed, None)\n",
        "\n",
        "\n",
        "def split_class_dir_fixed(class_dir, output, fixed, seed, prog_bar):\n",
        "    \"\"\"Splits one very class folder\n",
        "    \"\"\"\n",
        "    files = setup_files(class_dir, seed)\n",
        "\n",
        "    if not len(files) > sum(fixed):\n",
        "        raise ValueError(\n",
        "            f'The number of samples in class \"{class_dir.stem}\" are too few. There are only {len(files)} samples available but your fixed parameter {fixed} requires at least {sum(fixed)} files. You may want to split your classes by ratio.'\n",
        "        )\n",
        "\n",
        "    split_train = len(files) - sum(fixed)\n",
        "    split_val = split_train + fixed[0]\n",
        "\n",
        "    li = split_files(files, split_train, split_val, len(fixed) == 2)\n",
        "    copy_files(li, class_dir, output, prog_bar)\n",
        "    return len(files)\n",
        "\n",
        "def split_class_dir_ratio(class_dir, output, fulloutput, ratio, seed, prog_bar):\n",
        "    \"\"\"Splits one very class folder\n",
        "    \"\"\"\n",
        "    files = setup_files(class_dir, seed)\n",
        "\n",
        "    split_train = int(ratio[0] * len(files))\n",
        "    split_val = split_train + int(ratio[1] * len(files))\n",
        "\n",
        "    li = split_files(files, split_train, split_val, len(ratio) == 3)\n",
        "    copy_files(li, class_dir, output, fulloutput, prog_bar)\n",
        "\n",
        "\n",
        "def split_files(files, split_train, split_val, use_test):\n",
        "    \"\"\"Splits the files along the provided indices\n",
        "    \"\"\"\n",
        "    files_train = files[:split_train]\n",
        "    files_val = files[split_train:split_val] if use_test else files[split_train:]\n",
        "\n",
        "    li = [(files_train, \"train\"), (files_val, \"test\")]\n",
        "\n",
        "    # optional test folder\n",
        "    if use_test:\n",
        "        files_test = files[split_val:]\n",
        "        li.append((files_test, \"test\"))\n",
        "    return li\n",
        "\n",
        "\n",
        "def copy_files(files_type, class_dir, output, fulloutput, prog_bar):\n",
        "    \"\"\"Copies the files from the input folder to the output folder\n",
        "    \"\"\"\n",
        "    # get the last part within the file\n",
        "    for (files, folder_type) in files_type:\n",
        "        full_path = os.path.join(output, folder_type, fulloutput)\n",
        "\n",
        "        pathlib.Path(full_path).mkdir(parents=True, exist_ok=True)\n",
        "        for f in files:\n",
        "            if not prog_bar is None:\n",
        "                prog_bar.update()\n",
        "            shutil.copy2(f, full_path)\n",
        "            \n",
        "            \n",
        "ratio('/Drive', output='/Drive/data/', seed=1337, ratio=(.8, .2))  #the partition to 80% train 20% test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuQBZRFMUY_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataSet object\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import imageio\n",
        "import numpy as np  \n",
        "from PIL import Image\n",
        "\n",
        "class ClassDataset(Dataset):\n",
        "      \n",
        "  def __init__(self, basedir, transform, percentage):\n",
        "        super().__init__()\n",
        "        files = glob.glob(os.path.join(basedir ,'**/*.jpg'), recursive=True)\n",
        "        if(percentage < 100):\n",
        "          files_num = len(list(files))\n",
        "          choosefilesnumber = int(files_num * percentage / 100)\n",
        "          print(f'choosefilesnumber: {choosefilesnumber}  filesnum: {files_num}')\n",
        "          print((list(files)))\n",
        "          selectedfiles = np.random.choice(files, choosefilesnumber, replace=False)\n",
        "        elif(percentage == 100):\n",
        "          selectedfiles = files\n",
        "        \n",
        "        self.data = pd.DataFrame([self._split_file(f) for f in selectedfiles], \n",
        "                            columns=['class_id', 'file_path'])\n",
        "        names  = np.unique(self.data['class_id'])\n",
        "        \n",
        "        self.classDict = {str:index for index, str in enumerate(names)}          \n",
        "        self.data['class_num'] = self.data['class_id'].map(self.classDict)\n",
        "        self.transform = transform\n",
        "        \n",
        "  def _split_file(self, f):\n",
        "        parts = f.split(os.sep)[-3:-1]\n",
        "        return parts[0], f   #label is originaly a str\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      dat = self.data.iloc[index]\n",
        "      img = Image.open(dat['file_path'])\n",
        "      if self.transform:\n",
        "          img = self.transform(img)\n",
        "      img = np.resize(img, (3,128,128))\n",
        "      img = np.asarray(img)\n",
        "      return (img.astype(np.float32), dat['class_num'])\n",
        "     \n",
        "  def __len__(self):\n",
        "      return len(self.data)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj-39nGzUeRl",
        "colab_type": "code",
        "outputId": "fe997c09-505a-4c7d-96f4-ab5fd1f971fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n",
        "#basedir = '/content/gdrive/My Drive/video_data/'       \n",
        "trainbasedir = '/Drive/data/train'   \n",
        "testbasedir = '/Drive/data/test'\n",
        "\n",
        "data_transforms = transforms.Compose([transforms.Resize((256,256)),transforms.RandomResizedCrop(224), \n",
        "                         transforms.RandomHorizontalFlip(), #ImageNetPolicy(), \n",
        "                         transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_set = ClassDataset(trainbasedir, data_transforms, 60)\n",
        "test_set = ClassDataset(testbasedir, None, 100)\n",
        "#train_set = InstanceDataset(trainbasedir, data_transforms, 100) \n",
        "#test_set = InstanceDataset(testbasedir, None, 100)              "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3eNvtWkUpYB",
        "colab_type": "code",
        "outputId": "e8d89fa0-e1e2-4340-f448-d23590548fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True,\n",
        "                 num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=test_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=False,\n",
        "                 num_workers=4)\n",
        "\n",
        "print('Train size: {}'.format(len(train_loader)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH7hfQ8JUvaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perf_measure(y_actual, y_pred):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    cm = np.zeros((2,2))\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_actual[i]==y_pred[i]:\n",
        "           TP += 1\n",
        "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
        "           FP += 1\n",
        "        if y_actual[i]==y_pred[i]:\n",
        "           TN += 1\n",
        "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
        "           FN += 1\n",
        "\n",
        "        cm[0][0] = TP\n",
        "        cm[0][1] = FP\n",
        "        cm[1][0]  = FN\n",
        "        cm[1][1]  = TN\n",
        "\n",
        "    return (cm)\n",
        "  \n",
        "def create_confusion_matrix_fig(c_cm):\n",
        "    fig = plt.figure(figsize=(14, 12))\n",
        "    plt.imshow(c_cm, interpolation='nearest')\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.colorbar()\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZftbY7iEU1sG",
        "colab_type": "code",
        "outputId": "dc9cbe05-6365-4cba-b9f6-d049beabf5d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "\n",
        "import time \n",
        "import tqdm\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "assert use_cuda\n",
        "\n",
        "model = models.resnet18(pretrained=True) #for third net to load our trained instance net\n",
        "\n",
        "# Writer will output to ./runs/ directory by default\n",
        "writer = SummaryWriter('with_cm')    \n",
        "    \n",
        "num_final_in = model.fc.in_features\n",
        "\n",
        "NUM_CLASSES_Instance = 6476  #len(np.unique(dataset.data['instance_num']))\n",
        "NUM_CLASSES_Class = 23\n",
        "\n",
        "model.fc = nn.Linear(num_final_in, NUM_CLASSES_Class)      \n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003) #, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.cuda()\n",
        "  model.to(torch.device(\"cuda\"))\n",
        "\n",
        "#model = torch.load('/content/gdrive/My Drive/myModels/InstanceModel.pth')\n",
        "#num_final_in = model.fc.in_features\n",
        "#print(f'last fc number: {num_final_in})\n",
        "#model.fc = nn.Linear(num_final_in, NUM_CLASSES_Instance) \n",
        "\n",
        "for epoch in range(50):\n",
        "    # trainning\n",
        "    sum_loss = 0 \n",
        "    total_cnt = 0\n",
        "    correct_cnt = 0\n",
        "    tf = time.time()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "    #for batch_idx, (x, target) in enumerate(tqdm.tqdm(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        \n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)       \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "                \n",
        "        pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label[1] == target.data).sum()\n",
        "        sum_loss += loss.data\n",
        "        \n",
        "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {}'.format(\n",
        "                epoch, batch_idx+1, sum_loss/batch_idx, float(correct_cnt)/total_cnt))    \n",
        "            \n",
        "    cm = perf_measure(target.data, pred_label[1])\n",
        "    cur_fig = create_confusion_matrix_fig(cm)\n",
        "    writer.add_figure('train_confusion_matrix', cur_fig, global_step=None, close=True, walltime=None)\n",
        "    writer.add_scalar('train accuracy', float(correct_cnt)/total_cnt, epoch)\n",
        "    writer.add_scalar('train loss', sum_loss/len(train_loader), epoch)\n",
        "    \n",
        "     # testing    \n",
        "    correct_cnt, sum_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        x = x.float()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)   \n",
        "        loss = criterion(out, target)\n",
        "        \n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth averages\n",
        "        sum_loss += loss.data\n",
        "        \n",
        "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
        "                epoch, batch_idx+1, sum_loss/batch_idx,float(correct_cnt)/total_cnt))\n",
        "    \n",
        "    try:\n",
        "      cm = perf_measure(target.data, pred_label)\n",
        "      cur_fig = create_confusion_matrix_fig(cm)\n",
        "      writer.add_figure('test_confusion_matrix', cur_fig, global_step=None, close=True, walltime=None)\n",
        "    except:\n",
        "      print(\"An exception in test_confusion_matrix\")\n",
        "      continue\n",
        "    writer.add_scalar('test accuracy', float(correct_cnt)/total_cnt, epoch)\n",
        "    writer.add_scalar('test loss', sum_loss/len(test_loader), epoch)\n",
        "\n",
        "#torch.save(model.state_dict(), 'yt_bb_detection_train/mymodel3')\n",
        "writer.close()\n",
        "\n",
        "elapsed = time.time() - tf\n",
        "print(f'Elapsed time: {elapsed}')\n",
        "\n",
        "#orch.save(model, '/content/gdrive/My Drive/myModels/newClassNoPretrainModel.pth')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 46827520/46827520 [00:00<00:00, 106252927.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==>>> epoch: 0, batch index: 100, train loss: 3.102846, acc: 0.2288\n",
            "==>>> epoch: 0, batch index: 200, train loss: 2.993840, acc: 0.2274\n",
            "==>>> epoch: 0, batch index: 300, train loss: 2.946150, acc: 0.22913333333333333\n",
            "==>>> epoch: 0, batch index: 400, train loss: 2.919277, acc: 0.22945\n",
            "==>>> epoch: 0, batch index: 500, train loss: 2.892313, acc: 0.23124\n",
            "==>>> epoch: 0, batch index: 600, train loss: 2.864496, acc: 0.23383333333333334\n",
            "==>>> epoch: 0, batch index: 700, train loss: 2.842185, acc: 0.237\n",
            "==>>> epoch: 0, batch index: 800, train loss: 2.826134, acc: 0.2383\n",
            "==>>> epoch: 0, batch index: 900, train loss: 2.814292, acc: 0.23877777777777778\n",
            "==>>> epoch: 0, batch index: 968, train loss: 2.804430, acc: 0.23947765357356862\n",
            "==>>> epoch: 0, batch index: 100, test loss: 3.697992, acc: 0.012\n",
            "==>>> epoch: 0, batch index: 200, test loss: 2.842515, acc: 0.328\n",
            "==>>> epoch: 0, batch index: 300, test loss: 3.053473, acc: 0.226\n",
            "==>>> epoch: 0, batch index: 367, test loss: 3.098923, acc: 0.186\n",
            "==>>> epoch: 1, batch index: 100, train loss: 2.711994, acc: 0.248\n",
            "==>>> epoch: 1, batch index: 200, train loss: 2.689211, acc: 0.2491\n",
            "==>>> epoch: 1, batch index: 300, train loss: 2.666994, acc: 0.25133333333333335\n",
            "==>>> epoch: 1, batch index: 400, train loss: 2.656085, acc: 0.25295\n",
            "==>>> epoch: 1, batch index: 500, train loss: 2.654130, acc: 0.253\n",
            "==>>> epoch: 1, batch index: 600, train loss: 2.651525, acc: 0.2537333333333333\n",
            "==>>> epoch: 1, batch index: 700, train loss: 2.646512, acc: 0.2543714285714286\n",
            "==>>> epoch: 1, batch index: 800, train loss: 2.643836, acc: 0.2546\n",
            "==>>> epoch: 1, batch index: 900, train loss: 2.642458, acc: 0.25384444444444443\n",
            "==>>> epoch: 1, batch index: 968, train loss: 2.644559, acc: 0.25344546149554725\n",
            "==>>> epoch: 1, batch index: 100, test loss: 3.492508, acc: 0.026\n",
            "==>>> epoch: 1, batch index: 200, test loss: 2.738449, acc: 0.333\n",
            "==>>> epoch: 1, batch index: 300, test loss: 2.978050, acc: 0.223\n",
            "==>>> epoch: 1, batch index: 367, test loss: 3.084383, acc: 0.183\n",
            "==>>> epoch: 2, batch index: 100, train loss: 2.614206, acc: 0.2602\n",
            "==>>> epoch: 2, batch index: 200, train loss: 2.594656, acc: 0.2682\n",
            "==>>> epoch: 2, batch index: 300, train loss: 2.588667, acc: 0.2654666666666667\n",
            "==>>> epoch: 2, batch index: 400, train loss: 2.587023, acc: 0.2652\n",
            "==>>> epoch: 2, batch index: 500, train loss: 2.577482, acc: 0.26676\n",
            "==>>> epoch: 2, batch index: 600, train loss: 2.578684, acc: 0.26566666666666666\n",
            "==>>> epoch: 2, batch index: 700, train loss: 2.577872, acc: 0.26471428571428574\n",
            "==>>> epoch: 2, batch index: 800, train loss: 2.572423, acc: 0.265175\n",
            "==>>> epoch: 2, batch index: 900, train loss: 2.564551, acc: 0.26664444444444446\n",
            "==>>> epoch: 2, batch index: 968, train loss: 2.560036, acc: 0.2675372440440523\n",
            "==>>> epoch: 2, batch index: 100, test loss: 3.702916, acc: 0.030\n",
            "==>>> epoch: 2, batch index: 200, test loss: 2.838353, acc: 0.318\n",
            "==>>> epoch: 2, batch index: 300, test loss: 3.083260, acc: 0.216\n",
            "==>>> epoch: 2, batch index: 367, test loss: 3.202890, acc: 0.177\n",
            "==>>> epoch: 3, batch index: 100, train loss: 2.502562, acc: 0.28\n",
            "==>>> epoch: 3, batch index: 200, train loss: 2.509462, acc: 0.2759\n",
            "==>>> epoch: 3, batch index: 300, train loss: 2.513059, acc: 0.27626666666666666\n",
            "==>>> epoch: 3, batch index: 400, train loss: 2.514915, acc: 0.27445\n",
            "==>>> epoch: 3, batch index: 500, train loss: 2.510257, acc: 0.276\n",
            "==>>> epoch: 3, batch index: 600, train loss: 2.507433, acc: 0.2758\n",
            "==>>> epoch: 3, batch index: 700, train loss: 2.501291, acc: 0.2775142857142857\n",
            "==>>> epoch: 3, batch index: 800, train loss: 2.497291, acc: 0.279825\n",
            "==>>> epoch: 3, batch index: 900, train loss: 2.493847, acc: 0.28097777777777777\n",
            "==>>> epoch: 3, batch index: 968, train loss: 2.494342, acc: 0.2806992168936091\n",
            "==>>> epoch: 3, batch index: 100, test loss: 3.655499, acc: 0.025\n",
            "==>>> epoch: 3, batch index: 200, test loss: 2.827229, acc: 0.309\n",
            "==>>> epoch: 3, batch index: 300, test loss: 3.076727, acc: 0.209\n",
            "==>>> epoch: 3, batch index: 367, test loss: 3.191026, acc: 0.174\n",
            "==>>> epoch: 4, batch index: 100, train loss: 2.490646, acc: 0.2892\n",
            "==>>> epoch: 4, batch index: 200, train loss: 2.454485, acc: 0.2922\n",
            "==>>> epoch: 4, batch index: 300, train loss: 2.449954, acc: 0.2909333333333333\n",
            "==>>> epoch: 4, batch index: 400, train loss: 2.449881, acc: 0.29225\n",
            "==>>> epoch: 4, batch index: 500, train loss: 2.447978, acc: 0.29304\n",
            "==>>> epoch: 4, batch index: 600, train loss: 2.448572, acc: 0.2931666666666667\n",
            "==>>> epoch: 4, batch index: 700, train loss: 2.448993, acc: 0.2924285714285714\n",
            "==>>> epoch: 4, batch index: 800, train loss: 2.452745, acc: 0.2922\n",
            "==>>> epoch: 4, batch index: 900, train loss: 2.453661, acc: 0.2916888888888889\n",
            "==>>> epoch: 4, batch index: 968, train loss: 2.451127, acc: 0.29274541810442795\n",
            "==>>> epoch: 4, batch index: 100, test loss: 3.851470, acc: 0.022\n",
            "==>>> epoch: 4, batch index: 200, test loss: 2.849777, acc: 0.315\n",
            "==>>> epoch: 4, batch index: 300, test loss: 3.118043, acc: 0.214\n",
            "==>>> epoch: 4, batch index: 367, test loss: 3.233961, acc: 0.177\n",
            "==>>> epoch: 5, batch index: 100, train loss: 2.462367, acc: 0.2926\n",
            "==>>> epoch: 5, batch index: 200, train loss: 2.425335, acc: 0.3069\n",
            "==>>> epoch: 5, batch index: 300, train loss: 2.423915, acc: 0.30146666666666666\n",
            "==>>> epoch: 5, batch index: 400, train loss: 2.425084, acc: 0.29995\n",
            "==>>> epoch: 5, batch index: 500, train loss: 2.422370, acc: 0.29924\n",
            "==>>> epoch: 5, batch index: 600, train loss: 2.418769, acc: 0.29983333333333334\n",
            "==>>> epoch: 5, batch index: 700, train loss: 2.418663, acc: 0.2992\n",
            "==>>> epoch: 5, batch index: 800, train loss: 2.418152, acc: 0.2993\n",
            "==>>> epoch: 5, batch index: 900, train loss: 2.417268, acc: 0.29886666666666667\n",
            "==>>> epoch: 5, batch index: 968, train loss: 2.413125, acc: 0.3004318449490671\n",
            "==>>> epoch: 5, batch index: 100, test loss: 3.948313, acc: 0.024\n",
            "==>>> epoch: 5, batch index: 200, test loss: 2.987752, acc: 0.269\n",
            "==>>> epoch: 5, batch index: 300, test loss: 3.230168, acc: 0.186\n",
            "==>>> epoch: 5, batch index: 367, test loss: 3.341126, acc: 0.156\n",
            "==>>> epoch: 6, batch index: 100, train loss: 2.425367, acc: 0.2976\n",
            "==>>> epoch: 6, batch index: 200, train loss: 2.403653, acc: 0.3019\n",
            "==>>> epoch: 6, batch index: 300, train loss: 2.395151, acc: 0.3042\n",
            "==>>> epoch: 6, batch index: 400, train loss: 2.387908, acc: 0.30655\n",
            "==>>> epoch: 6, batch index: 500, train loss: 2.392441, acc: 0.30536\n",
            "==>>> epoch: 6, batch index: 600, train loss: 2.387998, acc: 0.3061\n",
            "==>>> epoch: 6, batch index: 700, train loss: 2.386851, acc: 0.3067142857142857\n",
            "==>>> epoch: 6, batch index: 800, train loss: 2.381748, acc: 0.30875\n",
            "==>>> epoch: 6, batch index: 900, train loss: 2.380211, acc: 0.30904444444444445\n",
            "==>>> epoch: 6, batch index: 968, train loss: 2.380693, acc: 0.3083868834845135\n",
            "==>>> epoch: 6, batch index: 100, test loss: 3.814392, acc: 0.029\n",
            "==>>> epoch: 6, batch index: 200, test loss: 2.850165, acc: 0.312\n",
            "==>>> epoch: 6, batch index: 300, test loss: 3.160975, acc: 0.211\n",
            "==>>> epoch: 6, batch index: 367, test loss: 3.274136, acc: 0.175\n",
            "==>>> epoch: 7, batch index: 100, train loss: 2.381007, acc: 0.3142\n",
            "==>>> epoch: 7, batch index: 200, train loss: 2.384490, acc: 0.3102\n",
            "==>>> epoch: 7, batch index: 300, train loss: 2.421206, acc: 0.2992666666666667\n",
            "==>>> epoch: 7, batch index: 400, train loss: 2.416250, acc: 0.30255\n",
            "==>>> epoch: 7, batch index: 500, train loss: 2.411946, acc: 0.30348\n",
            "==>>> epoch: 7, batch index: 600, train loss: 2.404485, acc: 0.3048666666666667\n",
            "==>>> epoch: 7, batch index: 700, train loss: 2.397899, acc: 0.3055428571428571\n",
            "==>>> epoch: 7, batch index: 800, train loss: 2.389263, acc: 0.307775\n",
            "==>>> epoch: 7, batch index: 900, train loss: 2.379975, acc: 0.30973333333333336\n",
            "==>>> epoch: 7, batch index: 968, train loss: 2.377386, acc: 0.310804388701779\n",
            "==>>> epoch: 7, batch index: 100, test loss: 4.030488, acc: 0.029\n",
            "==>>> epoch: 7, batch index: 200, test loss: 3.012522, acc: 0.292\n",
            "==>>> epoch: 7, batch index: 300, test loss: 3.288710, acc: 0.202\n",
            "==>>> epoch: 7, batch index: 367, test loss: 3.424147, acc: 0.167\n",
            "==>>> epoch: 8, batch index: 100, train loss: 2.349247, acc: 0.324\n",
            "==>>> epoch: 8, batch index: 200, train loss: 2.344576, acc: 0.3252\n",
            "==>>> epoch: 8, batch index: 300, train loss: 2.365953, acc: 0.31793333333333335\n",
            "==>>> epoch: 8, batch index: 400, train loss: 2.360210, acc: 0.3188\n",
            "==>>> epoch: 8, batch index: 500, train loss: 2.353964, acc: 0.32052\n",
            "==>>> epoch: 8, batch index: 600, train loss: 2.354097, acc: 0.3197\n",
            "==>>> epoch: 8, batch index: 700, train loss: 2.352231, acc: 0.3201428571428571\n",
            "==>>> epoch: 8, batch index: 800, train loss: 2.346261, acc: 0.3209\n",
            "==>>> epoch: 8, batch index: 900, train loss: 2.342264, acc: 0.32197777777777775\n",
            "==>>> epoch: 8, batch index: 968, train loss: 2.341340, acc: 0.32235469140649214\n",
            "==>>> epoch: 8, batch index: 100, test loss: 3.860238, acc: 0.038\n",
            "==>>> epoch: 8, batch index: 200, test loss: 2.953670, acc: 0.270\n",
            "==>>> epoch: 8, batch index: 300, test loss: 3.258160, acc: 0.184\n",
            "==>>> epoch: 8, batch index: 367, test loss: 3.371400, acc: 0.153\n",
            "==>>> epoch: 9, batch index: 100, train loss: 2.346973, acc: 0.3214\n",
            "==>>> epoch: 9, batch index: 200, train loss: 2.324768, acc: 0.3244\n",
            "==>>> epoch: 9, batch index: 300, train loss: 2.308215, acc: 0.3293333333333333\n",
            "==>>> epoch: 9, batch index: 400, train loss: 2.314681, acc: 0.3282\n",
            "==>>> epoch: 9, batch index: 500, train loss: 2.308020, acc: 0.32904\n",
            "==>>> epoch: 9, batch index: 600, train loss: 2.304702, acc: 0.3287\n",
            "==>>> epoch: 9, batch index: 700, train loss: 2.306576, acc: 0.328\n",
            "==>>> epoch: 9, batch index: 800, train loss: 2.306768, acc: 0.3279\n",
            "==>>> epoch: 9, batch index: 900, train loss: 2.307667, acc: 0.3277333333333333\n",
            "==>>> epoch: 9, batch index: 968, train loss: 2.304999, acc: 0.3283261359175155\n",
            "==>>> epoch: 9, batch index: 100, test loss: 3.918899, acc: 0.045\n",
            "==>>> epoch: 9, batch index: 200, test loss: 3.047722, acc: 0.248\n",
            "==>>> epoch: 9, batch index: 300, test loss: 3.370056, acc: 0.172\n",
            "==>>> epoch: 9, batch index: 367, test loss: 3.466294, acc: 0.144\n",
            "==>>> epoch: 10, batch index: 100, train loss: 2.291895, acc: 0.3356\n",
            "==>>> epoch: 10, batch index: 200, train loss: 2.280219, acc: 0.3354\n",
            "==>>> epoch: 10, batch index: 300, train loss: 2.287445, acc: 0.3316\n",
            "==>>> epoch: 10, batch index: 400, train loss: 2.280411, acc: 0.3355\n",
            "==>>> epoch: 10, batch index: 500, train loss: 2.279887, acc: 0.33688\n",
            "==>>> epoch: 10, batch index: 600, train loss: 2.278113, acc: 0.3358\n",
            "==>>> epoch: 10, batch index: 700, train loss: 2.280264, acc: 0.33494285714285715\n",
            "==>>> epoch: 10, batch index: 800, train loss: 2.279192, acc: 0.334425\n",
            "==>>> epoch: 10, batch index: 900, train loss: 2.279236, acc: 0.33484444444444444\n",
            "==>>> epoch: 10, batch index: 968, train loss: 2.279931, acc: 0.33454552968159185\n",
            "==>>> epoch: 10, batch index: 100, test loss: 4.024635, acc: 0.034\n",
            "==>>> epoch: 10, batch index: 200, test loss: 3.057889, acc: 0.258\n",
            "==>>> epoch: 10, batch index: 300, test loss: 3.389230, acc: 0.179\n",
            "==>>> epoch: 10, batch index: 367, test loss: 3.494931, acc: 0.150\n",
            "==>>> epoch: 11, batch index: 100, train loss: 2.321912, acc: 0.3316\n",
            "==>>> epoch: 11, batch index: 200, train loss: 2.303412, acc: 0.3334\n",
            "==>>> epoch: 11, batch index: 300, train loss: 2.286906, acc: 0.33766666666666667\n",
            "==>>> epoch: 11, batch index: 400, train loss: 2.290820, acc: 0.33355\n",
            "==>>> epoch: 11, batch index: 500, train loss: 2.287514, acc: 0.335\n",
            "==>>> epoch: 11, batch index: 600, train loss: 2.286411, acc: 0.33386666666666664\n",
            "==>>> epoch: 11, batch index: 700, train loss: 2.278276, acc: 0.33685714285714285\n",
            "==>>> epoch: 11, batch index: 800, train loss: 2.272345, acc: 0.33775\n",
            "==>>> epoch: 11, batch index: 900, train loss: 2.272452, acc: 0.3374666666666667\n",
            "==>>> epoch: 11, batch index: 968, train loss: 2.268225, acc: 0.3385540426059466\n",
            "==>>> epoch: 11, batch index: 100, test loss: 4.056386, acc: 0.034\n",
            "==>>> epoch: 11, batch index: 200, test loss: 3.035899, acc: 0.265\n",
            "==>>> epoch: 11, batch index: 300, test loss: 3.329535, acc: 0.184\n",
            "==>>> epoch: 11, batch index: 367, test loss: 3.458304, acc: 0.154\n",
            "==>>> epoch: 12, batch index: 100, train loss: 2.288575, acc: 0.3378\n",
            "==>>> epoch: 12, batch index: 200, train loss: 2.276269, acc: 0.3385\n",
            "==>>> epoch: 12, batch index: 300, train loss: 2.266877, acc: 0.33686666666666665\n",
            "==>>> epoch: 12, batch index: 400, train loss: 2.268937, acc: 0.33665\n",
            "==>>> epoch: 12, batch index: 500, train loss: 2.265152, acc: 0.33872\n",
            "==>>> epoch: 12, batch index: 600, train loss: 2.258042, acc: 0.3406666666666667\n",
            "==>>> epoch: 12, batch index: 700, train loss: 2.255108, acc: 0.3412857142857143\n",
            "==>>> epoch: 12, batch index: 800, train loss: 2.253036, acc: 0.3421\n",
            "==>>> epoch: 12, batch index: 900, train loss: 2.254378, acc: 0.3416222222222222\n",
            "==>>> epoch: 12, batch index: 968, train loss: 2.251218, acc: 0.34221129408847656\n",
            "==>>> epoch: 12, batch index: 100, test loss: 4.045413, acc: 0.035\n",
            "==>>> epoch: 12, batch index: 200, test loss: 3.055433, acc: 0.254\n",
            "==>>> epoch: 12, batch index: 300, test loss: 3.394547, acc: 0.177\n",
            "==>>> epoch: 12, batch index: 367, test loss: 3.530378, acc: 0.147\n",
            "==>>> epoch: 13, batch index: 100, train loss: 2.276836, acc: 0.3412\n",
            "==>>> epoch: 13, batch index: 200, train loss: 2.262147, acc: 0.3459\n",
            "==>>> epoch: 13, batch index: 300, train loss: 2.249858, acc: 0.3500666666666667\n",
            "==>>> epoch: 13, batch index: 400, train loss: 2.244028, acc: 0.3496\n",
            "==>>> epoch: 13, batch index: 500, train loss: 2.234616, acc: 0.35208\n",
            "==>>> epoch: 13, batch index: 600, train loss: 2.234107, acc: 0.3524\n",
            "==>>> epoch: 13, batch index: 700, train loss: 2.231427, acc: 0.35254285714285716\n",
            "==>>> epoch: 13, batch index: 800, train loss: 2.227165, acc: 0.353425\n",
            "==>>> epoch: 13, batch index: 900, train loss: 2.227810, acc: 0.35257777777777777\n",
            "==>>> epoch: 13, batch index: 968, train loss: 2.227907, acc: 0.3509101803830816\n",
            "==>>> epoch: 13, batch index: 100, test loss: 4.065282, acc: 0.036\n",
            "==>>> epoch: 13, batch index: 200, test loss: 3.106547, acc: 0.232\n",
            "==>>> epoch: 13, batch index: 300, test loss: 3.394939, acc: 0.165\n",
            "==>>> epoch: 13, batch index: 367, test loss: 3.516491, acc: 0.138\n",
            "==>>> epoch: 14, batch index: 100, train loss: 2.226421, acc: 0.3516\n",
            "==>>> epoch: 14, batch index: 200, train loss: 2.216997, acc: 0.349\n",
            "==>>> epoch: 14, batch index: 300, train loss: 2.212488, acc: 0.3492\n",
            "==>>> epoch: 14, batch index: 400, train loss: 2.209938, acc: 0.34905\n",
            "==>>> epoch: 14, batch index: 500, train loss: 2.221100, acc: 0.34624\n",
            "==>>> epoch: 14, batch index: 600, train loss: 2.221465, acc: 0.34563333333333335\n",
            "==>>> epoch: 14, batch index: 700, train loss: 2.221024, acc: 0.3472857142857143\n",
            "==>>> epoch: 14, batch index: 800, train loss: 2.217263, acc: 0.34845\n",
            "==>>> epoch: 14, batch index: 900, train loss: 2.217236, acc: 0.3486888888888889\n",
            "==>>> epoch: 14, batch index: 968, train loss: 2.217811, acc: 0.3483480381015352\n",
            "==>>> epoch: 14, batch index: 100, test loss: 4.099991, acc: 0.035\n",
            "==>>> epoch: 14, batch index: 200, test loss: 3.053518, acc: 0.271\n",
            "==>>> epoch: 14, batch index: 300, test loss: 3.410259, acc: 0.187\n",
            "==>>> epoch: 14, batch index: 367, test loss: 3.546646, acc: 0.155\n",
            "==>>> epoch: 15, batch index: 100, train loss: 2.194832, acc: 0.361\n",
            "==>>> epoch: 15, batch index: 200, train loss: 2.205806, acc: 0.3541\n",
            "==>>> epoch: 15, batch index: 300, train loss: 2.257781, acc: 0.34226666666666666\n",
            "==>>> epoch: 15, batch index: 400, train loss: 2.275012, acc: 0.3359\n",
            "==>>> epoch: 15, batch index: 500, train loss: 2.279051, acc: 0.33476\n",
            "==>>> epoch: 15, batch index: 600, train loss: 2.273408, acc: 0.33616666666666667\n",
            "==>>> epoch: 15, batch index: 700, train loss: 2.268565, acc: 0.33805714285714283\n",
            "==>>> epoch: 15, batch index: 800, train loss: 2.261502, acc: 0.339675\n",
            "==>>> epoch: 15, batch index: 900, train loss: 2.258666, acc: 0.3397777777777778\n",
            "==>>> epoch: 15, batch index: 968, train loss: 2.256502, acc: 0.33977312643345664\n",
            "==>>> epoch: 15, batch index: 100, test loss: 3.920665, acc: 0.032\n",
            "==>>> epoch: 15, batch index: 200, test loss: 3.028693, acc: 0.235\n",
            "==>>> epoch: 15, batch index: 300, test loss: 3.311045, acc: 0.165\n",
            "==>>> epoch: 15, batch index: 367, test loss: 3.427296, acc: 0.138\n",
            "==>>> epoch: 16, batch index: 100, train loss: 2.197878, acc: 0.3682\n",
            "==>>> epoch: 16, batch index: 200, train loss: 2.186407, acc: 0.3635\n",
            "==>>> epoch: 16, batch index: 300, train loss: 2.193300, acc: 0.36206666666666665\n",
            "==>>> epoch: 16, batch index: 400, train loss: 2.191222, acc: 0.36245\n",
            "==>>> epoch: 16, batch index: 500, train loss: 2.196271, acc: 0.35836\n",
            "==>>> epoch: 16, batch index: 600, train loss: 2.195434, acc: 0.3589333333333333\n",
            "==>>> epoch: 16, batch index: 700, train loss: 2.191735, acc: 0.3590285714285714\n",
            "==>>> epoch: 16, batch index: 800, train loss: 2.193305, acc: 0.3588\n",
            "==>>> epoch: 16, batch index: 900, train loss: 2.192517, acc: 0.3581111111111111\n",
            "==>>> epoch: 16, batch index: 968, train loss: 2.193826, acc: 0.3568196375808418\n",
            "==>>> epoch: 16, batch index: 100, test loss: 4.077568, acc: 0.029\n",
            "==>>> epoch: 16, batch index: 200, test loss: 3.037770, acc: 0.253\n",
            "==>>> epoch: 16, batch index: 300, test loss: 3.341787, acc: 0.180\n",
            "==>>> epoch: 16, batch index: 367, test loss: 3.470378, acc: 0.149\n",
            "==>>> epoch: 17, batch index: 100, train loss: 2.178941, acc: 0.3736\n",
            "==>>> epoch: 17, batch index: 200, train loss: 2.172376, acc: 0.3689\n",
            "==>>> epoch: 17, batch index: 300, train loss: 2.180054, acc: 0.3621333333333333\n",
            "==>>> epoch: 17, batch index: 400, train loss: 2.177257, acc: 0.3627\n",
            "==>>> epoch: 17, batch index: 500, train loss: 2.174461, acc: 0.36028\n",
            "==>>> epoch: 17, batch index: 600, train loss: 2.176004, acc: 0.359\n",
            "==>>> epoch: 17, batch index: 700, train loss: 2.175223, acc: 0.36005714285714285\n",
            "==>>> epoch: 17, batch index: 800, train loss: 2.170084, acc: 0.36215\n",
            "==>>> epoch: 17, batch index: 900, train loss: 2.169692, acc: 0.36191111111111113\n",
            "==>>> epoch: 17, batch index: 968, train loss: 2.166957, acc: 0.3628530694051284\n",
            "==>>> epoch: 17, batch index: 100, test loss: 4.111083, acc: 0.029\n",
            "==>>> epoch: 17, batch index: 200, test loss: 3.105033, acc: 0.240\n",
            "==>>> epoch: 17, batch index: 300, test loss: 3.407775, acc: 0.171\n",
            "==>>> epoch: 17, batch index: 367, test loss: 3.522131, acc: 0.143\n",
            "==>>> epoch: 18, batch index: 100, train loss: 2.202334, acc: 0.3632\n",
            "==>>> epoch: 18, batch index: 200, train loss: 2.176660, acc: 0.3662\n",
            "==>>> epoch: 18, batch index: 300, train loss: 2.163880, acc: 0.36806666666666665\n",
            "==>>> epoch: 18, batch index: 400, train loss: 2.154407, acc: 0.37125\n",
            "==>>> epoch: 18, batch index: 500, train loss: 2.155777, acc: 0.36996\n",
            "==>>> epoch: 18, batch index: 600, train loss: 2.154608, acc: 0.3703\n",
            "==>>> epoch: 18, batch index: 700, train loss: 2.152246, acc: 0.3692\n",
            "==>>> epoch: 18, batch index: 800, train loss: 2.147273, acc: 0.370825\n",
            "==>>> epoch: 18, batch index: 900, train loss: 2.148386, acc: 0.3704\n",
            "==>>> epoch: 18, batch index: 968, train loss: 2.148715, acc: 0.3697956484906089\n",
            "==>>> epoch: 18, batch index: 100, test loss: 4.123980, acc: 0.032\n",
            "==>>> epoch: 18, batch index: 200, test loss: 3.102716, acc: 0.243\n",
            "==>>> epoch: 18, batch index: 300, test loss: 3.456233, acc: 0.171\n",
            "==>>> epoch: 18, batch index: 367, test loss: 3.581616, acc: 0.142\n",
            "==>>> epoch: 19, batch index: 100, train loss: 2.168150, acc: 0.3734\n",
            "==>>> epoch: 19, batch index: 200, train loss: 2.164871, acc: 0.3714\n",
            "==>>> epoch: 19, batch index: 300, train loss: 2.146817, acc: 0.3762666666666667\n",
            "==>>> epoch: 19, batch index: 400, train loss: 2.140335, acc: 0.37625\n",
            "==>>> epoch: 19, batch index: 500, train loss: 2.135526, acc: 0.37552\n",
            "==>>> epoch: 19, batch index: 600, train loss: 2.137299, acc: 0.37496666666666667\n",
            "==>>> epoch: 19, batch index: 700, train loss: 2.135913, acc: 0.37402857142857143\n",
            "==>>> epoch: 19, batch index: 800, train loss: 2.138786, acc: 0.374925\n",
            "==>>> epoch: 19, batch index: 900, train loss: 2.138679, acc: 0.3738666666666667\n",
            "==>>> epoch: 19, batch index: 968, train loss: 2.137339, acc: 0.37430005992106946\n",
            "==>>> epoch: 19, batch index: 100, test loss: 4.034510, acc: 0.040\n",
            "==>>> epoch: 19, batch index: 200, test loss: 3.125430, acc: 0.230\n",
            "==>>> epoch: 19, batch index: 300, test loss: 3.421166, acc: 0.164\n",
            "==>>> epoch: 19, batch index: 367, test loss: 3.540312, acc: 0.137\n",
            "==>>> epoch: 20, batch index: 100, train loss: 2.150693, acc: 0.3726\n",
            "==>>> epoch: 20, batch index: 200, train loss: 2.150832, acc: 0.3696\n",
            "==>>> epoch: 20, batch index: 300, train loss: 2.139367, acc: 0.3704\n",
            "==>>> epoch: 20, batch index: 400, train loss: 2.128148, acc: 0.3724\n",
            "==>>> epoch: 20, batch index: 500, train loss: 2.125489, acc: 0.37452\n",
            "==>>> epoch: 20, batch index: 600, train loss: 2.118368, acc: 0.37816666666666665\n",
            "==>>> epoch: 20, batch index: 700, train loss: 2.115666, acc: 0.37811428571428574\n",
            "==>>> epoch: 20, batch index: 800, train loss: 2.116605, acc: 0.37735\n",
            "==>>> epoch: 20, batch index: 900, train loss: 2.116913, acc: 0.3771555555555556\n",
            "==>>> epoch: 20, batch index: 968, train loss: 2.119120, acc: 0.3769035270781247\n",
            "==>>> epoch: 20, batch index: 100, test loss: 4.108202, acc: 0.028\n",
            "==>>> epoch: 20, batch index: 200, test loss: 3.105018, acc: 0.225\n",
            "==>>> epoch: 20, batch index: 300, test loss: 3.394363, acc: 0.161\n",
            "==>>> epoch: 20, batch index: 367, test loss: 3.515607, acc: 0.135\n",
            "==>>> epoch: 21, batch index: 100, train loss: 2.125060, acc: 0.381\n",
            "==>>> epoch: 21, batch index: 200, train loss: 2.136988, acc: 0.3754\n",
            "==>>> epoch: 21, batch index: 300, train loss: 2.123739, acc: 0.3781333333333333\n",
            "==>>> epoch: 21, batch index: 400, train loss: 2.116215, acc: 0.3801\n",
            "==>>> epoch: 21, batch index: 500, train loss: 2.108480, acc: 0.38268\n",
            "==>>> epoch: 21, batch index: 600, train loss: 2.110549, acc: 0.3815\n",
            "==>>> epoch: 21, batch index: 700, train loss: 2.107829, acc: 0.38174285714285716\n",
            "==>>> epoch: 21, batch index: 800, train loss: 2.107722, acc: 0.38085\n",
            "==>>> epoch: 21, batch index: 900, train loss: 2.107274, acc: 0.38151111111111113\n",
            "==>>> epoch: 21, batch index: 968, train loss: 2.109449, acc: 0.38151125069735725\n",
            "==>>> epoch: 21, batch index: 100, test loss: 4.113286, acc: 0.025\n",
            "==>>> epoch: 21, batch index: 200, test loss: 3.128705, acc: 0.224\n",
            "==>>> epoch: 21, batch index: 300, test loss: 3.397832, acc: 0.163\n",
            "==>>> epoch: 21, batch index: 367, test loss: 3.536799, acc: 0.137\n",
            "==>>> epoch: 22, batch index: 100, train loss: 2.100923, acc: 0.386\n",
            "==>>> epoch: 22, batch index: 200, train loss: 2.096446, acc: 0.3836\n",
            "==>>> epoch: 22, batch index: 300, train loss: 2.094575, acc: 0.382\n",
            "==>>> epoch: 22, batch index: 400, train loss: 2.119372, acc: 0.3762\n",
            "==>>> epoch: 22, batch index: 500, train loss: 2.126047, acc: 0.37368\n",
            "==>>> epoch: 22, batch index: 600, train loss: 2.123656, acc: 0.37533333333333335\n",
            "==>>> epoch: 22, batch index: 700, train loss: 2.121577, acc: 0.3752285714285714\n",
            "==>>> epoch: 22, batch index: 800, train loss: 2.118655, acc: 0.3757\n",
            "==>>> epoch: 22, batch index: 900, train loss: 2.118274, acc: 0.3754\n",
            "==>>> epoch: 22, batch index: 968, train loss: 2.117575, acc: 0.3752711944955266\n",
            "==>>> epoch: 22, batch index: 100, test loss: 4.214458, acc: 0.025\n",
            "==>>> epoch: 22, batch index: 200, test loss: 3.160383, acc: 0.231\n",
            "==>>> epoch: 22, batch index: 300, test loss: 3.476324, acc: 0.166\n",
            "==>>> epoch: 22, batch index: 367, test loss: 3.596292, acc: 0.139\n",
            "==>>> epoch: 23, batch index: 100, train loss: 2.108757, acc: 0.3888\n",
            "==>>> epoch: 23, batch index: 200, train loss: 2.098901, acc: 0.3876\n",
            "==>>> epoch: 23, batch index: 300, train loss: 2.095654, acc: 0.38706666666666667\n",
            "==>>> epoch: 23, batch index: 400, train loss: 2.094565, acc: 0.3856\n",
            "==>>> epoch: 23, batch index: 500, train loss: 2.103328, acc: 0.38204\n",
            "==>>> epoch: 23, batch index: 600, train loss: 2.102898, acc: 0.38253333333333334\n",
            "==>>> epoch: 23, batch index: 700, train loss: 2.100156, acc: 0.3828\n",
            "==>>> epoch: 23, batch index: 800, train loss: 2.098499, acc: 0.383625\n",
            "==>>> epoch: 23, batch index: 900, train loss: 2.091527, acc: 0.38508888888888887\n",
            "==>>> epoch: 23, batch index: 968, train loss: 2.089809, acc: 0.3857263879992561\n",
            "==>>> epoch: 23, batch index: 100, test loss: 4.205578, acc: 0.031\n",
            "==>>> epoch: 23, batch index: 200, test loss: 3.221420, acc: 0.197\n",
            "==>>> epoch: 23, batch index: 300, test loss: 3.537685, acc: 0.144\n",
            "==>>> epoch: 23, batch index: 367, test loss: 3.636859, acc: 0.123\n",
            "==>>> epoch: 24, batch index: 100, train loss: 2.089078, acc: 0.3888\n",
            "==>>> epoch: 24, batch index: 200, train loss: 2.080436, acc: 0.392\n",
            "==>>> epoch: 24, batch index: 300, train loss: 2.075793, acc: 0.39366666666666666\n",
            "==>>> epoch: 24, batch index: 400, train loss: 2.076958, acc: 0.3915\n",
            "==>>> epoch: 24, batch index: 500, train loss: 2.071967, acc: 0.39236\n",
            "==>>> epoch: 24, batch index: 600, train loss: 2.071350, acc: 0.3930666666666667\n",
            "==>>> epoch: 24, batch index: 700, train loss: 2.071629, acc: 0.3918285714285714\n",
            "==>>> epoch: 24, batch index: 800, train loss: 2.070585, acc: 0.39175\n",
            "==>>> epoch: 24, batch index: 900, train loss: 2.070359, acc: 0.3916222222222222\n",
            "==>>> epoch: 24, batch index: 968, train loss: 2.068193, acc: 0.3917598198235428\n",
            "==>>> epoch: 24, batch index: 100, test loss: 4.186754, acc: 0.033\n",
            "==>>> epoch: 24, batch index: 200, test loss: 3.175077, acc: 0.222\n",
            "==>>> epoch: 24, batch index: 300, test loss: 3.484387, acc: 0.159\n",
            "==>>> epoch: 24, batch index: 367, test loss: 3.608312, acc: 0.134\n",
            "==>>> epoch: 25, batch index: 100, train loss: 2.068334, acc: 0.392\n",
            "==>>> epoch: 25, batch index: 200, train loss: 2.055720, acc: 0.3979\n",
            "==>>> epoch: 25, batch index: 300, train loss: 2.066751, acc: 0.39286666666666664\n",
            "==>>> epoch: 25, batch index: 400, train loss: 2.067964, acc: 0.39245\n",
            "==>>> epoch: 25, batch index: 500, train loss: 2.060864, acc: 0.39488\n",
            "==>>> epoch: 25, batch index: 600, train loss: 2.057097, acc: 0.3953333333333333\n",
            "==>>> epoch: 25, batch index: 700, train loss: 2.056986, acc: 0.39571428571428574\n",
            "==>>> epoch: 25, batch index: 800, train loss: 2.056414, acc: 0.394875\n",
            "==>>> epoch: 25, batch index: 900, train loss: 2.055882, acc: 0.39486666666666664\n",
            "==>>> epoch: 25, batch index: 968, train loss: 2.054566, acc: 0.3950451474264934\n",
            "==>>> epoch: 25, batch index: 100, test loss: 4.257199, acc: 0.039\n",
            "==>>> epoch: 25, batch index: 200, test loss: 3.367578, acc: 0.187\n",
            "==>>> epoch: 25, batch index: 300, test loss: 3.646428, acc: 0.139\n",
            "==>>> epoch: 25, batch index: 367, test loss: 3.759283, acc: 0.118\n",
            "==>>> epoch: 26, batch index: 100, train loss: 2.072288, acc: 0.3964\n",
            "==>>> epoch: 26, batch index: 200, train loss: 2.046758, acc: 0.4\n",
            "==>>> epoch: 26, batch index: 300, train loss: 2.047792, acc: 0.3988\n",
            "==>>> epoch: 26, batch index: 400, train loss: 2.053674, acc: 0.39715\n",
            "==>>> epoch: 26, batch index: 500, train loss: 2.050878, acc: 0.39664\n",
            "==>>> epoch: 26, batch index: 600, train loss: 2.046579, acc: 0.3980666666666667\n",
            "==>>> epoch: 26, batch index: 700, train loss: 2.044217, acc: 0.39925714285714287\n",
            "==>>> epoch: 26, batch index: 800, train loss: 2.045616, acc: 0.399425\n",
            "==>>> epoch: 26, batch index: 900, train loss: 2.044161, acc: 0.39897777777777776\n",
            "==>>> epoch: 26, batch index: 968, train loss: 2.043403, acc: 0.39917763497737463\n",
            "==>>> epoch: 26, batch index: 100, test loss: 4.297569, acc: 0.033\n",
            "==>>> epoch: 26, batch index: 200, test loss: 3.255681, acc: 0.212\n",
            "==>>> epoch: 26, batch index: 300, test loss: 3.554508, acc: 0.157\n",
            "==>>> epoch: 26, batch index: 367, test loss: 3.667012, acc: 0.132\n",
            "==>>> epoch: 27, batch index: 100, train loss: 2.081784, acc: 0.391\n",
            "==>>> epoch: 27, batch index: 200, train loss: 2.083739, acc: 0.3885\n",
            "==>>> epoch: 27, batch index: 300, train loss: 2.068057, acc: 0.3937333333333333\n",
            "==>>> epoch: 27, batch index: 400, train loss: 2.059436, acc: 0.39445\n",
            "==>>> epoch: 27, batch index: 500, train loss: 2.059339, acc: 0.3946\n",
            "==>>> epoch: 27, batch index: 600, train loss: 2.057871, acc: 0.39363333333333334\n",
            "==>>> epoch: 27, batch index: 700, train loss: 2.056820, acc: 0.3944857142857143\n",
            "==>>> epoch: 27, batch index: 800, train loss: 2.051685, acc: 0.395075\n",
            "==>>> epoch: 27, batch index: 900, train loss: 2.051406, acc: 0.39513333333333334\n",
            "==>>> epoch: 27, batch index: 968, train loss: 2.048141, acc: 0.3960576068764593\n",
            "==>>> epoch: 27, batch index: 100, test loss: 4.257041, acc: 0.035\n",
            "==>>> epoch: 27, batch index: 200, test loss: 3.199674, acc: 0.227\n",
            "==>>> epoch: 27, batch index: 300, test loss: 3.542459, acc: 0.162\n",
            "==>>> epoch: 27, batch index: 367, test loss: 3.672076, acc: 0.136\n",
            "==>>> epoch: 28, batch index: 100, train loss: 2.053709, acc: 0.394\n",
            "==>>> epoch: 28, batch index: 200, train loss: 2.040020, acc: 0.3966\n",
            "==>>> epoch: 28, batch index: 300, train loss: 2.032120, acc: 0.399\n",
            "==>>> epoch: 28, batch index: 400, train loss: 2.031815, acc: 0.39865\n",
            "==>>> epoch: 28, batch index: 500, train loss: 2.029240, acc: 0.3994\n",
            "==>>> epoch: 28, batch index: 600, train loss: 2.028880, acc: 0.3995666666666667\n",
            "==>>> epoch: 28, batch index: 700, train loss: 2.026040, acc: 0.4004285714285714\n",
            "==>>> epoch: 28, batch index: 800, train loss: 2.026235, acc: 0.4006\n",
            "==>>> epoch: 28, batch index: 900, train loss: 2.023445, acc: 0.40246666666666664\n",
            "==>>> epoch: 28, batch index: 968, train loss: 2.019970, acc: 0.4035167469058\n",
            "==>>> epoch: 28, batch index: 100, test loss: 4.288482, acc: 0.043\n",
            "==>>> epoch: 28, batch index: 200, test loss: 3.257714, acc: 0.214\n",
            "==>>> epoch: 28, batch index: 300, test loss: 3.575581, acc: 0.155\n",
            "==>>> epoch: 28, batch index: 367, test loss: 3.688560, acc: 0.130\n",
            "==>>> epoch: 29, batch index: 100, train loss: 2.001616, acc: 0.4178\n",
            "==>>> epoch: 29, batch index: 200, train loss: 2.003616, acc: 0.4131\n",
            "==>>> epoch: 29, batch index: 300, train loss: 2.009131, acc: 0.41173333333333334\n",
            "==>>> epoch: 29, batch index: 400, train loss: 2.004362, acc: 0.4121\n",
            "==>>> epoch: 29, batch index: 500, train loss: 2.003980, acc: 0.40984\n",
            "==>>> epoch: 29, batch index: 600, train loss: 2.004262, acc: 0.40976666666666667\n",
            "==>>> epoch: 29, batch index: 700, train loss: 2.002447, acc: 0.4105142857142857\n",
            "==>>> epoch: 29, batch index: 800, train loss: 2.001223, acc: 0.41105\n",
            "==>>> epoch: 29, batch index: 900, train loss: 2.002032, acc: 0.4114\n",
            "==>>> epoch: 29, batch index: 968, train loss: 2.001608, acc: 0.4116370849432816\n",
            "==>>> epoch: 29, batch index: 100, test loss: 4.369850, acc: 0.038\n",
            "==>>> epoch: 29, batch index: 200, test loss: 3.299054, acc: 0.212\n",
            "==>>> epoch: 29, batch index: 300, test loss: 3.617227, acc: 0.154\n",
            "==>>> epoch: 29, batch index: 367, test loss: 3.738395, acc: 0.129\n",
            "==>>> epoch: 30, batch index: 100, train loss: 2.035704, acc: 0.4104\n",
            "==>>> epoch: 30, batch index: 200, train loss: 2.011528, acc: 0.4123\n",
            "==>>> epoch: 30, batch index: 300, train loss: 2.015057, acc: 0.4080666666666667\n",
            "==>>> epoch: 30, batch index: 400, train loss: 2.009209, acc: 0.41115\n",
            "==>>> epoch: 30, batch index: 500, train loss: 2.024275, acc: 0.40576\n",
            "==>>> epoch: 30, batch index: 600, train loss: 2.024259, acc: 0.40553333333333336\n",
            "==>>> epoch: 30, batch index: 700, train loss: 2.021493, acc: 0.4064857142857143\n",
            "==>>> epoch: 30, batch index: 800, train loss: 2.019448, acc: 0.406375\n",
            "==>>> epoch: 30, batch index: 900, train loss: 2.016603, acc: 0.4068888888888889\n",
            "==>>> epoch: 30, batch index: 968, train loss: 2.014710, acc: 0.40715333595057546\n",
            "==>>> epoch: 30, batch index: 100, test loss: 4.301946, acc: 0.036\n",
            "==>>> epoch: 30, batch index: 200, test loss: 3.281454, acc: 0.207\n",
            "==>>> epoch: 30, batch index: 300, test loss: 3.581278, acc: 0.150\n",
            "==>>> epoch: 30, batch index: 367, test loss: 3.687483, acc: 0.126\n",
            "==>>> epoch: 31, batch index: 100, train loss: 1.986231, acc: 0.4162\n",
            "==>>> epoch: 31, batch index: 200, train loss: 1.984747, acc: 0.4148\n",
            "==>>> epoch: 31, batch index: 300, train loss: 1.985721, acc: 0.4158\n",
            "==>>> epoch: 31, batch index: 400, train loss: 1.992807, acc: 0.4128\n",
            "==>>> epoch: 31, batch index: 500, train loss: 1.989973, acc: 0.41188\n",
            "==>>> epoch: 31, batch index: 600, train loss: 1.980950, acc: 0.41413333333333335\n",
            "==>>> epoch: 31, batch index: 700, train loss: 1.982356, acc: 0.4133428571428571\n",
            "==>>> epoch: 31, batch index: 800, train loss: 1.984686, acc: 0.413425\n",
            "==>>> epoch: 31, batch index: 900, train loss: 1.980584, acc: 0.41535555555555553\n",
            "==>>> epoch: 31, batch index: 968, train loss: 1.980702, acc: 0.4150050622972498\n",
            "==>>> epoch: 31, batch index: 100, test loss: 4.340767, acc: 0.040\n",
            "==>>> epoch: 31, batch index: 200, test loss: 3.358379, acc: 0.197\n",
            "==>>> epoch: 31, batch index: 300, test loss: 3.687229, acc: 0.143\n",
            "==>>> epoch: 31, batch index: 367, test loss: 3.794086, acc: 0.121\n",
            "==>>> epoch: 32, batch index: 100, train loss: 1.973846, acc: 0.4192\n",
            "==>>> epoch: 32, batch index: 200, train loss: 1.955218, acc: 0.4248\n",
            "==>>> epoch: 32, batch index: 300, train loss: 1.956986, acc: 0.4218\n",
            "==>>> epoch: 32, batch index: 400, train loss: 1.956181, acc: 0.4213\n",
            "==>>> epoch: 32, batch index: 500, train loss: 1.954931, acc: 0.4232\n",
            "==>>> epoch: 32, batch index: 600, train loss: 1.960406, acc: 0.42256666666666665\n",
            "==>>> epoch: 32, batch index: 700, train loss: 1.962755, acc: 0.4221714285714286\n",
            "==>>> epoch: 32, batch index: 800, train loss: 1.961206, acc: 0.422125\n",
            "==>>> epoch: 32, batch index: 900, train loss: 1.962423, acc: 0.4220888888888889\n",
            "==>>> epoch: 32, batch index: 968, train loss: 1.961958, acc: 0.42296010083269625\n",
            "==>>> epoch: 32, batch index: 100, test loss: 4.303053, acc: 0.036\n",
            "==>>> epoch: 32, batch index: 200, test loss: 3.293344, acc: 0.202\n",
            "==>>> epoch: 32, batch index: 300, test loss: 3.595907, acc: 0.148\n",
            "==>>> epoch: 32, batch index: 367, test loss: 3.699920, acc: 0.126\n",
            "==>>> epoch: 33, batch index: 100, train loss: 1.960072, acc: 0.4216\n",
            "==>>> epoch: 33, batch index: 200, train loss: 1.972149, acc: 0.422\n",
            "==>>> epoch: 33, batch index: 300, train loss: 1.969942, acc: 0.42233333333333334\n",
            "==>>> epoch: 33, batch index: 400, train loss: 1.962759, acc: 0.42305\n",
            "==>>> epoch: 33, batch index: 500, train loss: 1.956327, acc: 0.4234\n",
            "==>>> epoch: 33, batch index: 600, train loss: 1.951440, acc: 0.4250333333333333\n",
            "==>>> epoch: 33, batch index: 700, train loss: 1.959345, acc: 0.42228571428571426\n",
            "==>>> epoch: 33, batch index: 800, train loss: 1.961668, acc: 0.422975\n",
            "==>>> epoch: 33, batch index: 900, train loss: 1.963991, acc: 0.4212\n",
            "==>>> epoch: 33, batch index: 968, train loss: 1.961809, acc: 0.42194764138273033\n",
            "==>>> epoch: 33, batch index: 100, test loss: 4.321659, acc: 0.039\n",
            "==>>> epoch: 33, batch index: 200, test loss: 3.304214, acc: 0.208\n",
            "==>>> epoch: 33, batch index: 300, test loss: 3.636504, acc: 0.150\n",
            "==>>> epoch: 33, batch index: 367, test loss: 3.747159, acc: 0.126\n",
            "==>>> epoch: 34, batch index: 100, train loss: 1.965116, acc: 0.4298\n",
            "==>>> epoch: 34, batch index: 200, train loss: 1.945325, acc: 0.4301\n",
            "==>>> epoch: 34, batch index: 300, train loss: 1.946456, acc: 0.4272666666666667\n",
            "==>>> epoch: 34, batch index: 400, train loss: 1.949025, acc: 0.4261\n",
            "==>>> epoch: 34, batch index: 500, train loss: 1.951998, acc: 0.42652\n",
            "==>>> epoch: 34, batch index: 600, train loss: 1.951452, acc: 0.42696666666666666\n",
            "==>>> epoch: 34, batch index: 700, train loss: 1.952232, acc: 0.42814285714285716\n",
            "==>>> epoch: 34, batch index: 800, train loss: 1.947401, acc: 0.42925\n",
            "==>>> epoch: 34, batch index: 900, train loss: 1.944355, acc: 0.43075555555555556\n",
            "==>>> epoch: 34, batch index: 968, train loss: 1.942936, acc: 0.43029526623551045\n",
            "==>>> epoch: 34, batch index: 100, test loss: 4.401647, acc: 0.039\n",
            "==>>> epoch: 34, batch index: 200, test loss: 3.338318, acc: 0.200\n",
            "==>>> epoch: 34, batch index: 300, test loss: 3.667212, acc: 0.143\n",
            "==>>> epoch: 34, batch index: 367, test loss: 3.780905, acc: 0.121\n",
            "==>>> epoch: 35, batch index: 100, train loss: 1.969099, acc: 0.4216\n",
            "==>>> epoch: 35, batch index: 200, train loss: 1.948763, acc: 0.4261\n",
            "==>>> epoch: 35, batch index: 300, train loss: 1.943109, acc: 0.427\n",
            "==>>> epoch: 35, batch index: 400, train loss: 1.935073, acc: 0.4276\n",
            "==>>> epoch: 35, batch index: 500, train loss: 1.933707, acc: 0.42752\n",
            "==>>> epoch: 35, batch index: 600, train loss: 1.935238, acc: 0.42673333333333335\n",
            "==>>> epoch: 35, batch index: 700, train loss: 1.932871, acc: 0.42751428571428574\n",
            "==>>> epoch: 35, batch index: 800, train loss: 1.933788, acc: 0.42795\n",
            "==>>> epoch: 35, batch index: 900, train loss: 1.933692, acc: 0.4282222222222222\n",
            "==>>> epoch: 35, batch index: 968, train loss: 1.933848, acc: 0.42833233464884185\n",
            "==>>> epoch: 35, batch index: 100, test loss: 4.322570, acc: 0.041\n",
            "==>>> epoch: 35, batch index: 200, test loss: 3.311043, acc: 0.201\n",
            "==>>> epoch: 35, batch index: 300, test loss: 3.664901, acc: 0.145\n",
            "==>>> epoch: 35, batch index: 367, test loss: 3.770168, acc: 0.124\n",
            "==>>> epoch: 36, batch index: 100, train loss: 1.948043, acc: 0.43\n",
            "==>>> epoch: 36, batch index: 200, train loss: 1.945530, acc: 0.4351\n",
            "==>>> epoch: 36, batch index: 300, train loss: 1.934339, acc: 0.4356\n",
            "==>>> epoch: 36, batch index: 400, train loss: 1.931784, acc: 0.43455\n",
            "==>>> epoch: 36, batch index: 500, train loss: 1.930105, acc: 0.43352\n",
            "==>>> epoch: 36, batch index: 600, train loss: 1.931337, acc: 0.4330333333333333\n",
            "==>>> epoch: 36, batch index: 700, train loss: 1.936803, acc: 0.43205714285714286\n",
            "==>>> epoch: 36, batch index: 800, train loss: 1.936108, acc: 0.432225\n",
            "==>>> epoch: 36, batch index: 900, train loss: 1.931412, acc: 0.4336\n",
            "==>>> epoch: 36, batch index: 968, train loss: 1.929917, acc: 0.43420046697109327\n",
            "==>>> epoch: 36, batch index: 100, test loss: 4.377533, acc: 0.043\n",
            "==>>> epoch: 36, batch index: 200, test loss: 3.435046, acc: 0.171\n",
            "==>>> epoch: 36, batch index: 300, test loss: 3.713907, acc: 0.128\n",
            "==>>> epoch: 36, batch index: 367, test loss: 3.788465, acc: 0.111\n",
            "==>>> epoch: 37, batch index: 100, train loss: 1.900104, acc: 0.4436\n",
            "==>>> epoch: 37, batch index: 200, train loss: 1.897930, acc: 0.4419\n",
            "==>>> epoch: 37, batch index: 300, train loss: 1.908102, acc: 0.43853333333333333\n",
            "==>>> epoch: 37, batch index: 400, train loss: 1.902427, acc: 0.44115\n",
            "==>>> epoch: 37, batch index: 500, train loss: 1.902327, acc: 0.44172\n",
            "==>>> epoch: 37, batch index: 600, train loss: 1.899865, acc: 0.4422\n",
            "==>>> epoch: 37, batch index: 700, train loss: 1.903282, acc: 0.4408285714285714\n",
            "==>>> epoch: 37, batch index: 800, train loss: 1.902945, acc: 0.440275\n",
            "==>>> epoch: 37, batch index: 900, train loss: 1.906194, acc: 0.4391777777777778\n",
            "==>>> epoch: 37, batch index: 968, train loss: 1.906938, acc: 0.43922143934541397\n",
            "==>>> epoch: 37, batch index: 100, test loss: 4.391477, acc: 0.039\n",
            "==>>> epoch: 37, batch index: 200, test loss: 3.366399, acc: 0.205\n",
            "==>>> epoch: 37, batch index: 300, test loss: 3.703789, acc: 0.148\n",
            "==>>> epoch: 37, batch index: 367, test loss: 3.806165, acc: 0.126\n",
            "==>>> epoch: 38, batch index: 100, train loss: 1.878874, acc: 0.4534\n",
            "==>>> epoch: 38, batch index: 200, train loss: 1.899470, acc: 0.4434\n",
            "==>>> epoch: 38, batch index: 300, train loss: 1.910092, acc: 0.4388666666666667\n",
            "==>>> epoch: 38, batch index: 400, train loss: 1.895560, acc: 0.44205\n",
            "==>>> epoch: 38, batch index: 500, train loss: 1.896170, acc: 0.44128\n",
            "==>>> epoch: 38, batch index: 600, train loss: 1.896835, acc: 0.4407333333333333\n",
            "==>>> epoch: 38, batch index: 700, train loss: 1.899143, acc: 0.43994285714285714\n",
            "==>>> epoch: 38, batch index: 800, train loss: 1.901155, acc: 0.440125\n",
            "==>>> epoch: 38, batch index: 900, train loss: 1.898417, acc: 0.4401555555555556\n",
            "==>>> epoch: 38, batch index: 968, train loss: 1.901922, acc: 0.4389734900923611\n",
            "==>>> epoch: 38, batch index: 100, test loss: 4.363076, acc: 0.035\n",
            "==>>> epoch: 38, batch index: 200, test loss: 3.323249, acc: 0.205\n",
            "==>>> epoch: 38, batch index: 300, test loss: 3.690478, acc: 0.149\n",
            "==>>> epoch: 38, batch index: 367, test loss: 3.793009, acc: 0.127\n",
            "==>>> epoch: 39, batch index: 100, train loss: 1.886688, acc: 0.4454\n",
            "==>>> epoch: 39, batch index: 200, train loss: 1.885218, acc: 0.4452\n",
            "==>>> epoch: 39, batch index: 300, train loss: 1.888640, acc: 0.4437333333333333\n",
            "==>>> epoch: 39, batch index: 400, train loss: 1.889105, acc: 0.44505\n",
            "==>>> epoch: 39, batch index: 500, train loss: 1.888219, acc: 0.44504\n",
            "==>>> epoch: 39, batch index: 600, train loss: 1.885966, acc: 0.4448666666666667\n",
            "==>>> epoch: 39, batch index: 700, train loss: 1.885831, acc: 0.4448\n",
            "==>>> epoch: 39, batch index: 800, train loss: 1.886102, acc: 0.4452\n",
            "==>>> epoch: 39, batch index: 900, train loss: 1.887782, acc: 0.4450222222222222\n",
            "==>>> epoch: 39, batch index: 968, train loss: 1.889674, acc: 0.44409777465545386\n",
            "==>>> epoch: 39, batch index: 100, test loss: 4.314909, acc: 0.034\n",
            "==>>> epoch: 39, batch index: 200, test loss: 3.299697, acc: 0.202\n",
            "==>>> epoch: 39, batch index: 300, test loss: 3.687250, acc: 0.145\n",
            "==>>> epoch: 39, batch index: 367, test loss: 3.779248, acc: 0.124\n",
            "==>>> epoch: 40, batch index: 100, train loss: 1.890623, acc: 0.4508\n",
            "==>>> epoch: 40, batch index: 200, train loss: 1.886211, acc: 0.4479\n",
            "==>>> epoch: 40, batch index: 300, train loss: 1.880809, acc: 0.4496\n",
            "==>>> epoch: 40, batch index: 400, train loss: 1.884231, acc: 0.44845\n",
            "==>>> epoch: 40, batch index: 500, train loss: 1.883386, acc: 0.44752\n",
            "==>>> epoch: 40, batch index: 600, train loss: 1.879761, acc: 0.4483333333333333\n",
            "==>>> epoch: 40, batch index: 700, train loss: 1.883160, acc: 0.4458285714285714\n",
            "==>>> epoch: 40, batch index: 800, train loss: 1.883890, acc: 0.444925\n",
            "==>>> epoch: 40, batch index: 900, train loss: 1.876324, acc: 0.4468888888888889\n",
            "==>>> epoch: 40, batch index: 968, train loss: 1.878777, acc: 0.44634998037068413\n",
            "==>>> epoch: 40, batch index: 100, test loss: 4.452318, acc: 0.030\n",
            "==>>> epoch: 40, batch index: 200, test loss: 3.414271, acc: 0.180\n",
            "==>>> epoch: 40, batch index: 300, test loss: 3.722827, acc: 0.134\n",
            "==>>> epoch: 40, batch index: 367, test loss: 3.811324, acc: 0.115\n",
            "==>>> epoch: 41, batch index: 100, train loss: 1.843279, acc: 0.4646\n",
            "==>>> epoch: 41, batch index: 200, train loss: 1.851993, acc: 0.4604\n",
            "==>>> epoch: 41, batch index: 300, train loss: 1.857790, acc: 0.45566666666666666\n",
            "==>>> epoch: 41, batch index: 400, train loss: 1.860829, acc: 0.4544\n",
            "==>>> epoch: 41, batch index: 500, train loss: 1.863467, acc: 0.45212\n",
            "==>>> epoch: 41, batch index: 600, train loss: 1.869416, acc: 0.45016666666666666\n",
            "==>>> epoch: 41, batch index: 700, train loss: 1.865841, acc: 0.451\n",
            "==>>> epoch: 41, batch index: 800, train loss: 1.868576, acc: 0.450625\n",
            "==>>> epoch: 41, batch index: 900, train loss: 1.871756, acc: 0.4492888888888889\n",
            "==>>> epoch: 41, batch index: 968, train loss: 1.871488, acc: 0.4487054982746864\n",
            "==>>> epoch: 41, batch index: 100, test loss: 4.394846, acc: 0.037\n",
            "==>>> epoch: 41, batch index: 200, test loss: 3.369160, acc: 0.197\n",
            "==>>> epoch: 41, batch index: 300, test loss: 3.677449, acc: 0.144\n",
            "==>>> epoch: 41, batch index: 367, test loss: 3.773410, acc: 0.122\n",
            "==>>> epoch: 42, batch index: 100, train loss: 1.855769, acc: 0.4548\n",
            "==>>> epoch: 42, batch index: 200, train loss: 1.844251, acc: 0.4579\n",
            "==>>> epoch: 42, batch index: 300, train loss: 1.863531, acc: 0.45206666666666667\n",
            "==>>> epoch: 42, batch index: 400, train loss: 1.859781, acc: 0.4528\n",
            "==>>> epoch: 42, batch index: 500, train loss: 1.867275, acc: 0.45192\n",
            "==>>> epoch: 42, batch index: 600, train loss: 1.865410, acc: 0.45143333333333335\n",
            "==>>> epoch: 42, batch index: 700, train loss: 1.866034, acc: 0.45071428571428573\n",
            "==>>> epoch: 42, batch index: 800, train loss: 1.863003, acc: 0.45115\n",
            "==>>> epoch: 42, batch index: 900, train loss: 1.863295, acc: 0.45106666666666667\n",
            "==>>> epoch: 42, batch index: 968, train loss: 1.864382, acc: 0.4501931937930037\n",
            "==>>> epoch: 42, batch index: 100, test loss: 4.472708, acc: 0.034\n",
            "==>>> epoch: 42, batch index: 200, test loss: 3.330541, acc: 0.211\n",
            "==>>> epoch: 42, batch index: 300, test loss: 3.706370, acc: 0.152\n",
            "==>>> epoch: 42, batch index: 367, test loss: 3.831287, acc: 0.128\n",
            "==>>> epoch: 43, batch index: 100, train loss: 1.886133, acc: 0.4368\n",
            "==>>> epoch: 43, batch index: 200, train loss: 1.864066, acc: 0.4515\n",
            "==>>> epoch: 43, batch index: 300, train loss: 1.862073, acc: 0.4534666666666667\n",
            "==>>> epoch: 43, batch index: 400, train loss: 1.860677, acc: 0.4521\n",
            "==>>> epoch: 43, batch index: 500, train loss: 1.855815, acc: 0.4544\n",
            "==>>> epoch: 43, batch index: 600, train loss: 1.853219, acc: 0.4546\n",
            "==>>> epoch: 43, batch index: 700, train loss: 1.848987, acc: 0.45454285714285714\n",
            "==>>> epoch: 43, batch index: 800, train loss: 1.851317, acc: 0.453525\n",
            "==>>> epoch: 43, batch index: 900, train loss: 1.852413, acc: 0.4535111111111111\n",
            "==>>> epoch: 43, batch index: 968, train loss: 1.855225, acc: 0.452652023885778\n",
            "==>>> epoch: 43, batch index: 100, test loss: 4.451378, acc: 0.036\n",
            "==>>> epoch: 43, batch index: 200, test loss: 3.414777, acc: 0.193\n",
            "==>>> epoch: 43, batch index: 300, test loss: 3.745187, acc: 0.141\n",
            "==>>> epoch: 43, batch index: 367, test loss: 3.821866, acc: 0.121\n",
            "==>>> epoch: 44, batch index: 100, train loss: 1.839491, acc: 0.4618\n",
            "==>>> epoch: 44, batch index: 200, train loss: 1.864693, acc: 0.4552\n",
            "==>>> epoch: 44, batch index: 300, train loss: 1.858827, acc: 0.4540666666666667\n",
            "==>>> epoch: 44, batch index: 400, train loss: 1.854308, acc: 0.45445\n",
            "==>>> epoch: 44, batch index: 500, train loss: 1.854124, acc: 0.4564\n",
            "==>>> epoch: 44, batch index: 600, train loss: 1.854516, acc: 0.4542\n",
            "==>>> epoch: 44, batch index: 700, train loss: 1.849671, acc: 0.4554\n",
            "==>>> epoch: 44, batch index: 800, train loss: 1.851125, acc: 0.4543\n",
            "==>>> epoch: 44, batch index: 900, train loss: 1.850814, acc: 0.4544888888888889\n",
            "==>>> epoch: 44, batch index: 968, train loss: 1.849709, acc: 0.4549662169142715\n",
            "==>>> epoch: 44, batch index: 100, test loss: 4.503869, acc: 0.035\n",
            "==>>> epoch: 44, batch index: 200, test loss: 3.487460, acc: 0.181\n",
            "==>>> epoch: 44, batch index: 300, test loss: 3.804087, acc: 0.135\n",
            "==>>> epoch: 44, batch index: 367, test loss: 3.891857, acc: 0.116\n",
            "==>>> epoch: 45, batch index: 100, train loss: 1.810596, acc: 0.478\n",
            "==>>> epoch: 45, batch index: 200, train loss: 1.832073, acc: 0.4626\n",
            "==>>> epoch: 45, batch index: 300, train loss: 1.832285, acc: 0.46253333333333335\n",
            "==>>> epoch: 45, batch index: 400, train loss: 1.836100, acc: 0.4613\n",
            "==>>> epoch: 45, batch index: 500, train loss: 1.835378, acc: 0.46084\n",
            "==>>> epoch: 45, batch index: 600, train loss: 1.833771, acc: 0.46186666666666665\n",
            "==>>> epoch: 45, batch index: 700, train loss: 1.835726, acc: 0.4606571428571429\n",
            "==>>> epoch: 45, batch index: 800, train loss: 1.834920, acc: 0.461375\n",
            "==>>> epoch: 45, batch index: 900, train loss: 1.833106, acc: 0.46113333333333334\n",
            "==>>> epoch: 45, batch index: 968, train loss: 1.834537, acc: 0.4597599024732938\n",
            "==>>> epoch: 45, batch index: 100, test loss: 4.533039, acc: 0.034\n",
            "==>>> epoch: 45, batch index: 200, test loss: 3.401602, acc: 0.204\n",
            "==>>> epoch: 45, batch index: 300, test loss: 3.709641, acc: 0.148\n",
            "==>>> epoch: 45, batch index: 367, test loss: 3.794350, acc: 0.127\n",
            "==>>> epoch: 46, batch index: 100, train loss: 1.819830, acc: 0.4682\n",
            "==>>> epoch: 46, batch index: 200, train loss: 1.836143, acc: 0.4617\n",
            "==>>> epoch: 46, batch index: 300, train loss: 1.827680, acc: 0.46073333333333333\n",
            "==>>> epoch: 46, batch index: 400, train loss: 1.821805, acc: 0.4619\n",
            "==>>> epoch: 46, batch index: 500, train loss: 1.827796, acc: 0.45944\n",
            "==>>> epoch: 46, batch index: 600, train loss: 1.823551, acc: 0.4604333333333333\n",
            "==>>> epoch: 46, batch index: 700, train loss: 1.826715, acc: 0.45977142857142855\n",
            "==>>> epoch: 46, batch index: 800, train loss: 1.827941, acc: 0.4596\n",
            "==>>> epoch: 46, batch index: 900, train loss: 1.829232, acc: 0.4603333333333333\n",
            "==>>> epoch: 46, batch index: 968, train loss: 1.827605, acc: 0.4606690497344877\n",
            "==>>> epoch: 46, batch index: 100, test loss: 4.488961, acc: 0.036\n",
            "==>>> epoch: 46, batch index: 200, test loss: 3.425081, acc: 0.205\n",
            "==>>> epoch: 46, batch index: 300, test loss: 3.800252, acc: 0.148\n",
            "==>>> epoch: 46, batch index: 367, test loss: 3.909933, acc: 0.125\n",
            "==>>> epoch: 47, batch index: 100, train loss: 1.844275, acc: 0.4602\n",
            "==>>> epoch: 47, batch index: 200, train loss: 1.822461, acc: 0.4659\n",
            "==>>> epoch: 47, batch index: 300, train loss: 1.817185, acc: 0.46453333333333335\n",
            "==>>> epoch: 47, batch index: 400, train loss: 1.807678, acc: 0.46485\n",
            "==>>> epoch: 47, batch index: 500, train loss: 1.818104, acc: 0.46304\n",
            "==>>> epoch: 47, batch index: 600, train loss: 1.814758, acc: 0.4642\n",
            "==>>> epoch: 47, batch index: 700, train loss: 1.815898, acc: 0.4624\n",
            "==>>> epoch: 47, batch index: 800, train loss: 1.813011, acc: 0.4633\n",
            "==>>> epoch: 47, batch index: 900, train loss: 1.808159, acc: 0.46353333333333335\n",
            "==>>> epoch: 47, batch index: 968, train loss: 1.809780, acc: 0.4636651032088766\n",
            "==>>> epoch: 47, batch index: 100, test loss: 4.631196, acc: 0.029\n",
            "==>>> epoch: 47, batch index: 200, test loss: 3.420312, acc: 0.212\n",
            "==>>> epoch: 47, batch index: 300, test loss: 3.715730, acc: 0.154\n",
            "==>>> epoch: 47, batch index: 367, test loss: 3.822879, acc: 0.131\n",
            "==>>> epoch: 48, batch index: 100, train loss: 1.819324, acc: 0.4718\n",
            "==>>> epoch: 48, batch index: 200, train loss: 1.802649, acc: 0.4706\n",
            "==>>> epoch: 48, batch index: 300, train loss: 1.806476, acc: 0.469\n",
            "==>>> epoch: 48, batch index: 400, train loss: 1.841897, acc: 0.45805\n",
            "==>>> epoch: 48, batch index: 500, train loss: 1.856198, acc: 0.45616\n",
            "==>>> epoch: 48, batch index: 600, train loss: 1.855936, acc: 0.45576666666666665\n",
            "==>>> epoch: 48, batch index: 700, train loss: 1.849874, acc: 0.45708571428571426\n",
            "==>>> epoch: 48, batch index: 800, train loss: 1.850738, acc: 0.455175\n",
            "==>>> epoch: 48, batch index: 900, train loss: 1.851449, acc: 0.4548888888888889\n",
            "==>>> epoch: 48, batch index: 968, train loss: 1.851457, acc: 0.4551315164163068\n",
            "==>>> epoch: 48, batch index: 100, test loss: 4.440522, acc: 0.033\n",
            "==>>> epoch: 48, batch index: 200, test loss: 3.404136, acc: 0.191\n",
            "==>>> epoch: 48, batch index: 300, test loss: 3.736282, acc: 0.139\n",
            "==>>> epoch: 48, batch index: 367, test loss: 3.844033, acc: 0.119\n",
            "==>>> epoch: 49, batch index: 100, train loss: 1.812124, acc: 0.4624\n",
            "==>>> epoch: 49, batch index: 200, train loss: 1.809027, acc: 0.464\n",
            "==>>> epoch: 49, batch index: 300, train loss: 1.812969, acc: 0.4626\n",
            "==>>> epoch: 49, batch index: 400, train loss: 1.808635, acc: 0.4643\n",
            "==>>> epoch: 49, batch index: 500, train loss: 1.811058, acc: 0.46236\n",
            "==>>> epoch: 49, batch index: 600, train loss: 1.810096, acc: 0.4635\n",
            "==>>> epoch: 49, batch index: 700, train loss: 1.813576, acc: 0.4624\n",
            "==>>> epoch: 49, batch index: 800, train loss: 1.811663, acc: 0.464675\n",
            "==>>> epoch: 49, batch index: 900, train loss: 1.809909, acc: 0.46575555555555553\n",
            "==>>> epoch: 49, batch index: 968, train loss: 1.809715, acc: 0.46558670992003637\n",
            "==>>> epoch: 49, batch index: 100, test loss: 4.575317, acc: 0.038\n",
            "==>>> epoch: 49, batch index: 200, test loss: 3.569455, acc: 0.171\n",
            "==>>> epoch: 49, batch index: 300, test loss: 3.897049, acc: 0.126\n",
            "==>>> epoch: 49, batch index: 367, test loss: 3.949053, acc: 0.110\n",
            "Elapsed time: 276.10789823532104\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}