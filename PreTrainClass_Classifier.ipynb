{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreTrainClass_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahgin/DeepLeaningProj/blob/master/PreTrainClass_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HcPTGrbS-C3",
        "colab_type": "code",
        "outputId": "a301365d-f489-4d91-8558-5a6b485f1b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# mount data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXS22Wk9TFb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir /Drive\n",
        "!ls /Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R91JpZNRTJ5f",
        "colab_type": "code",
        "outputId": "bdbf6b29-7809-4a0b-e4c6-8a36799a2192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "import shutil\n",
        "import glob\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "for src in glob.glob('/content/gdrive/My Drive/videos_2/yt_bb_detection_train/*.zip'):\n",
        "  dst = f'/Drive/{os.path.basename(src)}'\n",
        "  print(src, dst)    \n",
        "  if os.path.exists(src) and not os.path.exists(dst):\n",
        "    print(f'copy {src} to {dst}')\n",
        "    shutil.copy2(src, dst)\n",
        "  \n",
        " # Create a ZipFile Object and load sample.zip in it\n",
        "for z in glob.glob('/Drive/*zip'):\n",
        "  with ZipFile(z, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall('/Drive')\n",
        "    \n",
        "!ls /Drive\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/0.zip /Drive/0.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/0.zip to /Drive/0.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/1.zip /Drive/1.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/1.zip to /Drive/1.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/2.zip /Drive/2.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/2.zip to /Drive/2.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/3.zip /Drive/3.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/3.zip to /Drive/3.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/4.zip /Drive/4.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/4.zip to /Drive/4.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/5.zip /Drive/5.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/5.zip to /Drive/5.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/6.zip /Drive/6.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/6.zip to /Drive/6.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/7.zip /Drive/7.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/7.zip to /Drive/7.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/8.zip /Drive/8.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/8.zip to /Drive/8.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/9.zip /Drive/9.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/9.zip to /Drive/9.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/10.zip /Drive/10.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/10.zip to /Drive/10.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/11.zip /Drive/11.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/11.zip to /Drive/11.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/12.zip /Drive/12.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/12.zip to /Drive/12.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/13.zip /Drive/13.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/13.zip to /Drive/13.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/14.zip /Drive/14.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/14.zip to /Drive/14.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/15.zip /Drive/15.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/15.zip to /Drive/15.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/16.zip /Drive/16.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/16.zip to /Drive/16.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/17.zip /Drive/17.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/17.zip to /Drive/17.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/18.zip /Drive/18.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/18.zip to /Drive/18.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/19.zip /Drive/19.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/19.zip to /Drive/19.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/20.zip /Drive/20.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/20.zip to /Drive/20.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/21.zip /Drive/21.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/21.zip to /Drive/21.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/23.zip /Drive/23.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/23.zip to /Drive/23.zip\n",
            "/content/gdrive/My Drive/videos_2/yt_bb_detection_train/yt_bb_detection_train.zip /Drive/yt_bb_detection_train.zip\n",
            "copy /content/gdrive/My Drive/videos_2/yt_bb_detection_train/yt_bb_detection_train.zip to /Drive/yt_bb_detection_train.zip\n",
            "0\t12\t15.zip\t19\t21.zip\t4.zip  8\n",
            "0.zip\t12.zip\t16\t19.zip\t23\t5      8.zip\n",
            "1\t13\t16.zip\t1.zip\t23.zip\t5.zip  9\n",
            "10\t13.zip\t17\t2\t2.zip\t6      9.zip\n",
            "10.zip\t14\t17.zip\t20\t3\t6.zip  yt_bb_detection_train.zip\n",
            "11\t14.zip\t18\t20.zip\t3.zip\t7\n",
            "11.zip\t15\t18.zip\t21\t4\t7.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iNJZOanTMtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dont run\n",
        "import shutil\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "def list_dirs(directory):\n",
        "    \"\"\"Returns all directories in a given directory\n",
        "    \"\"\"\n",
        "    return [f for f in pathlib.Path(directory).iterdir() if f.is_dir()]\n",
        "\n",
        "def list_files(directory):\n",
        "    \"\"\"Returns all files in a given directory\n",
        "    \"\"\"\n",
        "    return [\n",
        "        f\n",
        "        for f in pathlib.Path(directory).iterdir()\n",
        "        if f.is_file() and not f.name.startswith(\".\")\n",
        "    ]\n",
        "\n",
        "def setup_files(class_dir, seed):\n",
        "    \"\"\"Returns shuffled files\n",
        "    \"\"\"\n",
        "    # make sure its reproducible\n",
        "    random.seed(seed)\n",
        "\n",
        "    files = list_files(class_dir)\n",
        "\n",
        "    files.sort()\n",
        "    random.shuffle(files)\n",
        "    return files\n",
        "\n",
        "def ratio(input, output=\"output\", seed=1337, ratio=(0.8, 0.1, 0.1)):\n",
        "    # make up for some impression\n",
        "    assert round(sum(ratio), 5) == 1\n",
        "    assert len(ratio) in (2, 3)\n",
        "\n",
        "    for class_dir in list_dirs(input):\n",
        "        for instance_dir in list_dirs(class_dir):\n",
        "            instancename = os.path.basename(instance_dir)\n",
        "            classname = os.path.basename(class_dir)\n",
        "            fulloutput = os.path.join(classname,instancename)\n",
        "            #output_dir = os.path.join(output, classname ,instancename)\n",
        "            split_class_dir_ratio(instance_dir, output, fulloutput, ratio, seed, None)\n",
        "\n",
        "\n",
        "def split_class_dir_fixed(class_dir, output, fixed, seed, prog_bar):\n",
        "    \"\"\"Splits one very class folder\n",
        "    \"\"\"\n",
        "    files = setup_files(class_dir, seed)\n",
        "\n",
        "    if not len(files) > sum(fixed):\n",
        "        raise ValueError(\n",
        "            f'The number of samples in class \"{class_dir.stem}\" are too few. There are only {len(files)} samples available but your fixed parameter {fixed} requires at least {sum(fixed)} files. You may want to split your classes by ratio.'\n",
        "        )\n",
        "\n",
        "    split_train = len(files) - sum(fixed)\n",
        "    split_val = split_train + fixed[0]\n",
        "\n",
        "    li = split_files(files, split_train, split_val, len(fixed) == 2)\n",
        "    copy_files(li, class_dir, output, prog_bar)\n",
        "    return len(files)\n",
        "\n",
        "def split_class_dir_ratio(class_dir, output, fulloutput, ratio, seed, prog_bar):\n",
        "    \"\"\"Splits one very class folder\n",
        "    \"\"\"\n",
        "    files = setup_files(class_dir, seed)\n",
        "\n",
        "    split_train = int(ratio[0] * len(files))\n",
        "    split_val = split_train + int(ratio[1] * len(files))\n",
        "\n",
        "    li = split_files(files, split_train, split_val, len(ratio) == 3)\n",
        "    copy_files(li, class_dir, output, fulloutput, prog_bar)\n",
        "\n",
        "\n",
        "def split_files(files, split_train, split_val, use_test):\n",
        "    \"\"\"Splits the files along the provided indices\n",
        "    \"\"\"\n",
        "    files_train = files[:split_train]\n",
        "    files_val = files[split_train:split_val] if use_test else files[split_train:]\n",
        "\n",
        "    li = [(files_train, \"train\"), (files_val, \"test\")]\n",
        "\n",
        "    # optional test folder\n",
        "    if use_test:\n",
        "        files_test = files[split_val:]\n",
        "        li.append((files_test, \"test\"))\n",
        "    return li\n",
        "\n",
        "\n",
        "def copy_files(files_type, class_dir, output, fulloutput, prog_bar):\n",
        "    \"\"\"Copies the files from the input folder to the output folder\n",
        "    \"\"\"\n",
        "    # get the last part within the file\n",
        "    for (files, folder_type) in files_type:\n",
        "        full_path = os.path.join(output, folder_type, fulloutput)\n",
        "\n",
        "        pathlib.Path(full_path).mkdir(parents=True, exist_ok=True)\n",
        "        for f in files:\n",
        "            if not prog_bar is None:\n",
        "                prog_bar.update()\n",
        "            shutil.copy2(f, full_path)\n",
        "            \n",
        "            \n",
        "ratio('/Drive', output='/Drive/data/', seed=1337, ratio=(.8, .2))  #the partition to 80% train 20% test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuQBZRFMUY_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataSet object\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import imageio\n",
        "import numpy as np  \n",
        "from PIL import Image\n",
        "\n",
        "class ClassDataset(Dataset):\n",
        "      \n",
        "  def __init__(self, basedir, transform = None, percentage = 100):\n",
        "        super().__init__()\n",
        "        files = glob.glob(os.path.join(basedir ,'**/*.jpg'), recursive=True)\n",
        "        if(percentage < 100):\n",
        "          files_num = len(list(files))\n",
        "          choosefilesnumber = int(files_num * percentage / 100)\n",
        "          print(f'choosefilesnumber: {choosefilesnumber}  filesnum: {files_num}')\n",
        "          print((list(files)))\n",
        "          selectedfiles = np.random.choice(files, choosefilesnumber, replace=False)\n",
        "        elif(percentage == 100):\n",
        "          selectedfiles = files\n",
        "        \n",
        "        self.data = pd.DataFrame([self._split_file(f) for f in selectedfiles], \n",
        "                            columns=['class_id', 'file_path'])\n",
        "        names  = np.unique(self.data['class_id'])\n",
        "        \n",
        "        self.classDict = {str:index for index, str in enumerate(names)}          \n",
        "        self.data['class_num'] = self.data['class_id'].map(self.classDict)\n",
        "        self.transform = transform\n",
        "        \n",
        "  def _split_file(self, f):\n",
        "        parts = f.split(os.sep)[-3:-1]\n",
        "        return parts[0], f   #label is originaly a str\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      dat = self.data.iloc[index]\n",
        "      img = Image.open(dat['file_path'])\n",
        "      if self.transform:\n",
        "          img = self.transform(img)\n",
        "      img = np.resize(img, (3,128,128))\n",
        "      img = np.asarray(img)\n",
        "      return (img.astype(np.float32), dat['class_num'])\n",
        "     \n",
        "  def __len__(self):\n",
        "      return len(self.data)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhdNmAYbVTYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "import torch\n",
        "\n",
        "basedir = '/Drive'\n",
        "\n",
        "dataset = ClassDataset(basedir)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# run next\n",
        "'''\n",
        "test_dataset.dataset.transform = transforms.Compose([\n",
        "    transforms.Resize(img_resolution),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset.dataset.transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(img_resolution[0]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj-39nGzUeRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dont run\n",
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n",
        "#basedir = '/content/gdrive/My Drive/video_data/'       \n",
        "trainbasedir = '/Drive/data/train'   \n",
        "testbasedir = '/Drive/data/test'\n",
        "\n",
        "data_transforms = transforms.Compose([transforms.Resize((256,256)),transforms.RandomResizedCrop(224), \n",
        "                         transforms.RandomHorizontalFlip(), #ImageNetPolicy(), \n",
        "                         transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_set = ClassDataset(trainbasedir, data_transforms, 60)\n",
        "test_set = ClassDataset(testbasedir, None, 100)\n",
        "#train_set = InstanceDataset(trainbasedir, data_transforms, 100) \n",
        "#test_set = InstanceDataset(testbasedir, None, 100)              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3eNvtWkUpYB",
        "colab_type": "code",
        "outputId": "8b8145dc-2a69-4710-f08a-c6bff3a1793e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True,\n",
        "                 num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=test_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=False,\n",
        "                 num_workers=4)\n",
        "\n",
        "print('Train size: {}'.format(len(train_loader)))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 1597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEaVM64RZ66S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH7hfQ8JUvaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perf_measure(y_actual, y_pred):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    cm = np.zeros((2,2))\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_actual[i]==y_pred[i]:\n",
        "           TP += 1\n",
        "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
        "           FP += 1\n",
        "        if y_actual[i]==y_pred[i]:\n",
        "           TN += 1\n",
        "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
        "           FN += 1\n",
        "\n",
        "        cm[0][0] = TP\n",
        "        cm[0][1] = FP\n",
        "        cm[1][0]  = FN\n",
        "        cm[1][1]  = TN\n",
        "\n",
        "    return (cm)\n",
        "  \n",
        "def create_confusion_matrix_fig(c_cm):\n",
        "    fig = plt.figure(figsize=(14, 12))\n",
        "    plt.imshow(c_cm, interpolation='nearest')\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.colorbar()\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZftbY7iEU1sG",
        "colab_type": "code",
        "outputId": "40ccf96a-b512-4def-fe65-cd2b3cacb234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "\n",
        "import time \n",
        "import tqdm\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "assert use_cuda\n",
        "\n",
        "model = models.resnet18(pretrained=True) #for third net to load our trained instance net\n",
        "\n",
        "# Writer will output to ./runs/ directory by default\n",
        "writer = SummaryWriter('with_cm')    \n",
        "    \n",
        "num_final_in = model.fc.in_features\n",
        "\n",
        "NUM_CLASSES_Instance = 6476  #len(np.unique(dataset.data['instance_num']))\n",
        "NUM_CLASSES_Class = 23\n",
        "\n",
        "model.fc = nn.Linear(num_final_in, NUM_CLASSES_Class)      \n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003) #, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.cuda()\n",
        "  model.to(torch.device(\"cuda\"))\n",
        "\n",
        "#model = torch.load('/content/gdrive/My Drive/myModels/InstanceModel.pth')\n",
        "#num_final_in = model.fc.in_features\n",
        "#print(f'last fc number: {num_final_in})\n",
        "#model.fc = nn.Linear(num_final_in, NUM_CLASSES_Instance) \n",
        "\n",
        "for epoch in range(50):\n",
        "    # trainning\n",
        "    sum_loss = 0 \n",
        "    total_cnt = 0\n",
        "    correct_cnt = 0\n",
        "    tf = time.time()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "    #for batch_idx, (x, target) in enumerate(tqdm.tqdm(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        \n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)       \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "                \n",
        "        pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label[1] == target.data).sum()\n",
        "        sum_loss += loss.data\n",
        "        \n",
        "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {}'.format(\n",
        "                epoch, batch_idx+1, sum_loss/batch_idx, float(correct_cnt)/total_cnt))    \n",
        "            \n",
        "    cm = perf_measure(target.data, pred_label[1])\n",
        "    cur_fig = create_confusion_matrix_fig(cm)\n",
        "    writer.add_figure('train_confusion_matrix', cur_fig, global_step=None, close=True, walltime=None)\n",
        "    writer.add_scalar('train accuracy', float(correct_cnt)/total_cnt, epoch)\n",
        "    writer.add_scalar('train loss', sum_loss/len(train_loader), epoch)\n",
        "    \n",
        "     # testing    \n",
        "    correct_cnt, sum_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        x = x.float()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)   \n",
        "        loss = criterion(out, target)\n",
        "        \n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth averages\n",
        "        sum_loss += loss.data\n",
        "        \n",
        "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
        "                epoch, batch_idx+1, sum_loss/batch_idx,float(correct_cnt)/total_cnt))\n",
        "    \n",
        "    try:\n",
        "      cm = perf_measure(target.data, pred_label)\n",
        "      cur_fig = create_confusion_matrix_fig(cm)\n",
        "      writer.add_figure('test_confusion_matrix', cur_fig, global_step=None, close=True, walltime=None)\n",
        "    except:\n",
        "      print(\"An exception in test_confusion_matrix\")\n",
        "      continue\n",
        "    writer.add_scalar('test accuracy', float(correct_cnt)/total_cnt, epoch)\n",
        "    writer.add_scalar('test loss', sum_loss/len(test_loader), epoch)\n",
        "\n",
        "#torch.save(model.state_dict(), 'yt_bb_detection_train/mymodel3')\n",
        "writer.close()\n",
        "\n",
        "elapsed = time.time() - tf\n",
        "print(f'Elapsed time: {elapsed}')\n",
        "\n",
        "#orch.save(model, '/content/gdrive/My Drive/myModels/newClassNoPretrainModel.pth')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 44.4MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==>>> epoch: 0, batch index: 100, train loss: 3.034415, acc: 0.2364\n",
            "==>>> epoch: 0, batch index: 200, train loss: 2.908833, acc: 0.2401\n",
            "==>>> epoch: 0, batch index: 300, train loss: 2.857850, acc: 0.23953333333333332\n",
            "==>>> epoch: 0, batch index: 400, train loss: 2.828359, acc: 0.2402\n",
            "==>>> epoch: 0, batch index: 500, train loss: 2.805807, acc: 0.24168\n",
            "==>>> epoch: 0, batch index: 600, train loss: 2.792327, acc: 0.24203333333333332\n",
            "==>>> epoch: 0, batch index: 700, train loss: 2.777004, acc: 0.24345714285714284\n",
            "==>>> epoch: 0, batch index: 800, train loss: 2.759170, acc: 0.244925\n",
            "==>>> epoch: 0, batch index: 900, train loss: 2.748092, acc: 0.24595555555555557\n",
            "==>>> epoch: 0, batch index: 1000, train loss: 2.733339, acc: 0.24804\n",
            "==>>> epoch: 0, batch index: 1100, train loss: 2.722116, acc: 0.24834545454545454\n",
            "==>>> epoch: 0, batch index: 1200, train loss: 2.710063, acc: 0.24956666666666666\n",
            "==>>> epoch: 0, batch index: 1300, train loss: 2.700216, acc: 0.25038461538461537\n",
            "==>>> epoch: 0, batch index: 1400, train loss: 2.691452, acc: 0.25082857142857146\n",
            "==>>> epoch: 0, batch index: 1500, train loss: 2.682753, acc: 0.25136\n",
            "==>>> epoch: 0, batch index: 1597, train loss: 2.673873, acc: 0.2520103208998221\n",
            "==>>> epoch: 0, batch index: 100, test loss: 2.560279, acc: 0.270\n",
            "==>>> epoch: 0, batch index: 200, test loss: 2.535789, acc: 0.269\n",
            "==>>> epoch: 0, batch index: 300, test loss: 2.542282, acc: 0.266\n",
            "==>>> epoch: 0, batch index: 400, test loss: 2.539316, acc: 0.267\n",
            "==>>> epoch: 1, batch index: 100, train loss: 2.532348, acc: 0.2772\n",
            "==>>> epoch: 1, batch index: 200, train loss: 2.519032, acc: 0.2775\n",
            "==>>> epoch: 1, batch index: 300, train loss: 2.509056, acc: 0.27813333333333334\n",
            "==>>> epoch: 1, batch index: 400, train loss: 2.511811, acc: 0.2755\n",
            "==>>> epoch: 1, batch index: 500, train loss: 2.509756, acc: 0.27448\n",
            "==>>> epoch: 1, batch index: 600, train loss: 2.508782, acc: 0.27386666666666665\n",
            "==>>> epoch: 1, batch index: 700, train loss: 2.507060, acc: 0.27314285714285713\n",
            "==>>> epoch: 1, batch index: 800, train loss: 2.506748, acc: 0.272075\n",
            "==>>> epoch: 1, batch index: 900, train loss: 2.501006, acc: 0.27353333333333335\n",
            "==>>> epoch: 1, batch index: 1000, train loss: 2.498260, acc: 0.2729\n",
            "==>>> epoch: 1, batch index: 1100, train loss: 2.497101, acc: 0.2730909090909091\n",
            "==>>> epoch: 1, batch index: 1200, train loss: 2.494967, acc: 0.27353333333333335\n",
            "==>>> epoch: 1, batch index: 1300, train loss: 2.490387, acc: 0.2746\n",
            "==>>> epoch: 1, batch index: 1400, train loss: 2.486776, acc: 0.2754857142857143\n",
            "==>>> epoch: 1, batch index: 1500, train loss: 2.485055, acc: 0.27585333333333334\n",
            "==>>> epoch: 1, batch index: 1597, train loss: 2.481034, acc: 0.2765099326135424\n",
            "==>>> epoch: 1, batch index: 100, test loss: 2.449766, acc: 0.289\n",
            "==>>> epoch: 1, batch index: 200, test loss: 2.417135, acc: 0.293\n",
            "==>>> epoch: 1, batch index: 300, test loss: 2.427474, acc: 0.291\n",
            "==>>> epoch: 1, batch index: 400, test loss: 2.430636, acc: 0.291\n",
            "==>>> epoch: 2, batch index: 100, train loss: 2.457580, acc: 0.2848\n",
            "==>>> epoch: 2, batch index: 200, train loss: 2.448730, acc: 0.2825\n",
            "==>>> epoch: 2, batch index: 300, train loss: 2.436089, acc: 0.2856\n",
            "==>>> epoch: 2, batch index: 400, train loss: 2.423440, acc: 0.28995\n",
            "==>>> epoch: 2, batch index: 500, train loss: 2.421612, acc: 0.29124\n",
            "==>>> epoch: 2, batch index: 600, train loss: 2.408344, acc: 0.2941\n",
            "==>>> epoch: 2, batch index: 700, train loss: 2.409300, acc: 0.2936\n",
            "==>>> epoch: 2, batch index: 800, train loss: 2.408086, acc: 0.2935\n",
            "==>>> epoch: 2, batch index: 900, train loss: 2.406644, acc: 0.29388888888888887\n",
            "==>>> epoch: 2, batch index: 1000, train loss: 2.401690, acc: 0.29524\n",
            "==>>> epoch: 2, batch index: 1100, train loss: 2.397670, acc: 0.2958363636363636\n",
            "==>>> epoch: 2, batch index: 1200, train loss: 2.393983, acc: 0.2973166666666667\n",
            "==>>> epoch: 2, batch index: 1300, train loss: 2.391693, acc: 0.29826153846153847\n",
            "==>>> epoch: 2, batch index: 1400, train loss: 2.390198, acc: 0.2982142857142857\n",
            "==>>> epoch: 2, batch index: 1500, train loss: 2.386951, acc: 0.2982666666666667\n",
            "==>>> epoch: 2, batch index: 1597, train loss: 2.382982, acc: 0.299105689020266\n",
            "==>>> epoch: 2, batch index: 100, test loss: 2.340599, acc: 0.312\n",
            "==>>> epoch: 2, batch index: 200, test loss: 2.321047, acc: 0.312\n",
            "==>>> epoch: 2, batch index: 300, test loss: 2.328413, acc: 0.309\n",
            "==>>> epoch: 2, batch index: 400, test loss: 2.330826, acc: 0.310\n",
            "==>>> epoch: 3, batch index: 100, train loss: 2.353126, acc: 0.3236\n",
            "==>>> epoch: 3, batch index: 200, train loss: 2.336468, acc: 0.3212\n",
            "==>>> epoch: 3, batch index: 300, train loss: 2.332077, acc: 0.3198666666666667\n",
            "==>>> epoch: 3, batch index: 400, train loss: 2.326639, acc: 0.31895\n",
            "==>>> epoch: 3, batch index: 500, train loss: 2.319504, acc: 0.31988\n",
            "==>>> epoch: 3, batch index: 600, train loss: 2.310058, acc: 0.3207333333333333\n",
            "==>>> epoch: 3, batch index: 700, train loss: 2.305563, acc: 0.3206\n",
            "==>>> epoch: 3, batch index: 800, train loss: 2.296913, acc: 0.322975\n",
            "==>>> epoch: 3, batch index: 900, train loss: 2.291759, acc: 0.3247555555555556\n",
            "==>>> epoch: 3, batch index: 1000, train loss: 2.288547, acc: 0.32522\n",
            "==>>> epoch: 3, batch index: 1100, train loss: 2.286378, acc: 0.3256909090909091\n",
            "==>>> epoch: 3, batch index: 1200, train loss: 2.285285, acc: 0.3259166666666667\n",
            "==>>> epoch: 3, batch index: 1300, train loss: 2.282662, acc: 0.3267846153846154\n",
            "==>>> epoch: 3, batch index: 1400, train loss: 2.277679, acc: 0.3281\n",
            "==>>> epoch: 3, batch index: 1500, train loss: 2.273207, acc: 0.32884\n",
            "==>>> epoch: 3, batch index: 1597, train loss: 2.270837, acc: 0.3291164608331872\n",
            "==>>> epoch: 3, batch index: 100, test loss: 2.248289, acc: 0.332\n",
            "==>>> epoch: 3, batch index: 200, test loss: 2.223796, acc: 0.337\n",
            "==>>> epoch: 3, batch index: 300, test loss: 2.233429, acc: 0.334\n",
            "==>>> epoch: 3, batch index: 400, test loss: 2.237245, acc: 0.335\n",
            "==>>> epoch: 4, batch index: 100, train loss: 2.214648, acc: 0.3498\n",
            "==>>> epoch: 4, batch index: 200, train loss: 2.202547, acc: 0.3488\n",
            "==>>> epoch: 4, batch index: 300, train loss: 2.206360, acc: 0.3462\n",
            "==>>> epoch: 4, batch index: 400, train loss: 2.203361, acc: 0.3493\n",
            "==>>> epoch: 4, batch index: 500, train loss: 2.204293, acc: 0.348\n",
            "==>>> epoch: 4, batch index: 600, train loss: 2.195722, acc: 0.3509333333333333\n",
            "==>>> epoch: 4, batch index: 700, train loss: 2.197210, acc: 0.34905714285714284\n",
            "==>>> epoch: 4, batch index: 800, train loss: 2.192855, acc: 0.34885\n",
            "==>>> epoch: 4, batch index: 900, train loss: 2.185659, acc: 0.35\n",
            "==>>> epoch: 4, batch index: 1000, train loss: 2.184959, acc: 0.34952\n",
            "==>>> epoch: 4, batch index: 1100, train loss: 2.180894, acc: 0.3503818181818182\n",
            "==>>> epoch: 4, batch index: 1200, train loss: 2.184564, acc: 0.34958333333333336\n",
            "==>>> epoch: 4, batch index: 1300, train loss: 2.186869, acc: 0.3491692307692308\n",
            "==>>> epoch: 4, batch index: 1400, train loss: 2.186260, acc: 0.3493857142857143\n",
            "==>>> epoch: 4, batch index: 1500, train loss: 2.186329, acc: 0.34986666666666666\n",
            "==>>> epoch: 4, batch index: 1597, train loss: 2.185856, acc: 0.34992109020767054\n",
            "==>>> epoch: 4, batch index: 100, test loss: 2.200481, acc: 0.354\n",
            "==>>> epoch: 4, batch index: 200, test loss: 2.169830, acc: 0.354\n",
            "==>>> epoch: 4, batch index: 300, test loss: 2.181152, acc: 0.350\n",
            "==>>> epoch: 4, batch index: 400, test loss: 2.186594, acc: 0.348\n",
            "==>>> epoch: 5, batch index: 100, train loss: 2.151163, acc: 0.362\n",
            "==>>> epoch: 5, batch index: 200, train loss: 2.133999, acc: 0.3663\n",
            "==>>> epoch: 5, batch index: 300, train loss: 2.126059, acc: 0.36773333333333336\n",
            "==>>> epoch: 5, batch index: 400, train loss: 2.131752, acc: 0.3643\n",
            "==>>> epoch: 5, batch index: 500, train loss: 2.125803, acc: 0.36568\n",
            "==>>> epoch: 5, batch index: 600, train loss: 2.118380, acc: 0.3676333333333333\n",
            "==>>> epoch: 5, batch index: 700, train loss: 2.119089, acc: 0.36788571428571426\n",
            "==>>> epoch: 5, batch index: 800, train loss: 2.114051, acc: 0.369\n",
            "==>>> epoch: 5, batch index: 900, train loss: 2.114460, acc: 0.36924444444444443\n",
            "==>>> epoch: 5, batch index: 1000, train loss: 2.114399, acc: 0.36936\n",
            "==>>> epoch: 5, batch index: 1100, train loss: 2.112834, acc: 0.3695272727272727\n",
            "==>>> epoch: 5, batch index: 1200, train loss: 2.109592, acc: 0.37028333333333335\n",
            "==>>> epoch: 5, batch index: 1300, train loss: 2.105667, acc: 0.37095384615384613\n",
            "==>>> epoch: 5, batch index: 1400, train loss: 2.103422, acc: 0.37092857142857144\n",
            "==>>> epoch: 5, batch index: 1500, train loss: 2.100726, acc: 0.37268\n",
            "==>>> epoch: 5, batch index: 1597, train loss: 2.097267, acc: 0.37338109672085973\n",
            "==>>> epoch: 5, batch index: 100, test loss: 2.115697, acc: 0.369\n",
            "==>>> epoch: 5, batch index: 200, test loss: 2.083009, acc: 0.372\n",
            "==>>> epoch: 5, batch index: 300, test loss: 2.094212, acc: 0.368\n",
            "==>>> epoch: 5, batch index: 400, test loss: 2.096705, acc: 0.368\n",
            "==>>> epoch: 6, batch index: 100, train loss: 2.034137, acc: 0.3986\n",
            "==>>> epoch: 6, batch index: 200, train loss: 2.037042, acc: 0.3958\n",
            "==>>> epoch: 6, batch index: 300, train loss: 2.030927, acc: 0.39413333333333334\n",
            "==>>> epoch: 6, batch index: 400, train loss: 2.018553, acc: 0.39545\n",
            "==>>> epoch: 6, batch index: 500, train loss: 2.022400, acc: 0.39484\n",
            "==>>> epoch: 6, batch index: 600, train loss: 2.017493, acc: 0.3955\n",
            "==>>> epoch: 6, batch index: 700, train loss: 2.018334, acc: 0.39502857142857145\n",
            "==>>> epoch: 6, batch index: 800, train loss: 2.018225, acc: 0.395475\n",
            "==>>> epoch: 6, batch index: 900, train loss: 2.015640, acc: 0.39611111111111114\n",
            "==>>> epoch: 6, batch index: 1000, train loss: 2.015557, acc: 0.39568\n",
            "==>>> epoch: 6, batch index: 1100, train loss: 2.016912, acc: 0.3963090909090909\n",
            "==>>> epoch: 6, batch index: 1200, train loss: 2.015755, acc: 0.39676666666666666\n",
            "==>>> epoch: 6, batch index: 1300, train loss: 2.013269, acc: 0.3975076923076923\n",
            "==>>> epoch: 6, batch index: 1400, train loss: 2.015843, acc: 0.39644285714285715\n",
            "==>>> epoch: 6, batch index: 1500, train loss: 2.017872, acc: 0.3954933333333333\n",
            "==>>> epoch: 6, batch index: 1597, train loss: 2.017944, acc: 0.39566371903103786\n",
            "==>>> epoch: 6, batch index: 100, test loss: 2.080024, acc: 0.388\n",
            "==>>> epoch: 6, batch index: 200, test loss: 2.052239, acc: 0.388\n",
            "==>>> epoch: 6, batch index: 300, test loss: 2.058661, acc: 0.386\n",
            "==>>> epoch: 6, batch index: 400, test loss: 2.060660, acc: 0.387\n",
            "==>>> epoch: 7, batch index: 100, train loss: 1.979499, acc: 0.4102\n",
            "==>>> epoch: 7, batch index: 200, train loss: 1.956489, acc: 0.4139\n",
            "==>>> epoch: 7, batch index: 300, train loss: 1.942110, acc: 0.41806666666666664\n",
            "==>>> epoch: 7, batch index: 400, train loss: 1.943693, acc: 0.41725\n",
            "==>>> epoch: 7, batch index: 500, train loss: 1.939409, acc: 0.41796\n",
            "==>>> epoch: 7, batch index: 600, train loss: 1.939858, acc: 0.4179333333333333\n",
            "==>>> epoch: 7, batch index: 700, train loss: 1.937195, acc: 0.4188\n",
            "==>>> epoch: 7, batch index: 800, train loss: 1.936438, acc: 0.4187\n",
            "==>>> epoch: 7, batch index: 900, train loss: 1.933390, acc: 0.41944444444444445\n",
            "==>>> epoch: 7, batch index: 1000, train loss: 1.935652, acc: 0.41838\n",
            "==>>> epoch: 7, batch index: 1100, train loss: 1.936736, acc: 0.41767272727272725\n",
            "==>>> epoch: 7, batch index: 1200, train loss: 1.937278, acc: 0.4181166666666667\n",
            "==>>> epoch: 7, batch index: 1300, train loss: 1.936065, acc: 0.4182923076923077\n",
            "==>>> epoch: 7, batch index: 1400, train loss: 1.934479, acc: 0.41818571428571427\n",
            "==>>> epoch: 7, batch index: 1500, train loss: 1.931613, acc: 0.4192133333333333\n",
            "==>>> epoch: 7, batch index: 1597, train loss: 1.929267, acc: 0.4203261604749618\n",
            "==>>> epoch: 7, batch index: 100, test loss: 2.000471, acc: 0.425\n",
            "==>>> epoch: 7, batch index: 200, test loss: 1.972301, acc: 0.424\n",
            "==>>> epoch: 7, batch index: 300, test loss: 1.985242, acc: 0.418\n",
            "==>>> epoch: 7, batch index: 400, test loss: 1.986151, acc: 0.417\n",
            "==>>> epoch: 8, batch index: 100, train loss: 1.888497, acc: 0.4318\n",
            "==>>> epoch: 8, batch index: 200, train loss: 1.874555, acc: 0.4399\n",
            "==>>> epoch: 8, batch index: 300, train loss: 1.862299, acc: 0.44126666666666664\n",
            "==>>> epoch: 8, batch index: 400, train loss: 1.860934, acc: 0.44095\n",
            "==>>> epoch: 8, batch index: 500, train loss: 1.855766, acc: 0.44248\n",
            "==>>> epoch: 8, batch index: 600, train loss: 1.854288, acc: 0.4429\n",
            "==>>> epoch: 8, batch index: 700, train loss: 1.854682, acc: 0.4424\n",
            "==>>> epoch: 8, batch index: 800, train loss: 1.849924, acc: 0.443875\n",
            "==>>> epoch: 8, batch index: 900, train loss: 1.850539, acc: 0.4439111111111111\n",
            "==>>> epoch: 8, batch index: 1000, train loss: 1.852372, acc: 0.44276\n",
            "==>>> epoch: 8, batch index: 1100, train loss: 1.853449, acc: 0.44230909090909093\n",
            "==>>> epoch: 8, batch index: 1200, train loss: 1.852191, acc: 0.44255\n",
            "==>>> epoch: 8, batch index: 1300, train loss: 1.849886, acc: 0.4431076923076923\n",
            "==>>> epoch: 8, batch index: 1400, train loss: 1.848836, acc: 0.4440142857142857\n",
            "==>>> epoch: 8, batch index: 1500, train loss: 1.848537, acc: 0.44477333333333335\n",
            "==>>> epoch: 8, batch index: 1597, train loss: 1.866868, acc: 0.44041684410932136\n",
            "==>>> epoch: 8, batch index: 100, test loss: 2.190880, acc: 0.369\n",
            "==>>> epoch: 8, batch index: 200, test loss: 2.156332, acc: 0.369\n",
            "==>>> epoch: 8, batch index: 300, test loss: 2.162877, acc: 0.365\n",
            "==>>> epoch: 8, batch index: 400, test loss: 2.162775, acc: 0.365\n",
            "==>>> epoch: 9, batch index: 100, train loss: 2.041527, acc: 0.3886\n",
            "==>>> epoch: 9, batch index: 200, train loss: 2.017871, acc: 0.3955\n",
            "==>>> epoch: 9, batch index: 300, train loss: 1.991816, acc: 0.4020666666666667\n",
            "==>>> epoch: 9, batch index: 400, train loss: 1.957591, acc: 0.4117\n",
            "==>>> epoch: 9, batch index: 500, train loss: 1.939421, acc: 0.41792\n",
            "==>>> epoch: 9, batch index: 600, train loss: 1.920052, acc: 0.4240333333333333\n",
            "==>>> epoch: 9, batch index: 700, train loss: 1.910455, acc: 0.4270285714285714\n",
            "==>>> epoch: 9, batch index: 800, train loss: 1.899104, acc: 0.4296\n",
            "==>>> epoch: 9, batch index: 900, train loss: 1.891150, acc: 0.4316888888888889\n",
            "==>>> epoch: 9, batch index: 1000, train loss: 1.882379, acc: 0.43468\n",
            "==>>> epoch: 9, batch index: 1100, train loss: 1.873795, acc: 0.4369818181818182\n",
            "==>>> epoch: 9, batch index: 1200, train loss: 1.867242, acc: 0.439\n",
            "==>>> epoch: 9, batch index: 1300, train loss: 1.860120, acc: 0.4402307692307692\n",
            "==>>> epoch: 9, batch index: 1400, train loss: 1.854658, acc: 0.4418142857142857\n",
            "==>>> epoch: 9, batch index: 1500, train loss: 1.849236, acc: 0.44368\n",
            "==>>> epoch: 9, batch index: 1597, train loss: 1.845910, acc: 0.4450762794659185\n",
            "==>>> epoch: 9, batch index: 100, test loss: 1.910864, acc: 0.443\n",
            "==>>> epoch: 9, batch index: 200, test loss: 1.873601, acc: 0.448\n",
            "==>>> epoch: 9, batch index: 300, test loss: 1.885825, acc: 0.443\n",
            "==>>> epoch: 9, batch index: 400, test loss: 1.890211, acc: 0.440\n",
            "==>>> epoch: 10, batch index: 100, train loss: 1.686849, acc: 0.4866\n",
            "==>>> epoch: 10, batch index: 200, train loss: 1.700985, acc: 0.4843\n",
            "==>>> epoch: 10, batch index: 300, train loss: 1.702533, acc: 0.4854\n",
            "==>>> epoch: 10, batch index: 400, train loss: 1.706532, acc: 0.4846\n",
            "==>>> epoch: 10, batch index: 500, train loss: 1.706149, acc: 0.48436\n",
            "==>>> epoch: 10, batch index: 600, train loss: 1.710145, acc: 0.48236666666666667\n",
            "==>>> epoch: 10, batch index: 700, train loss: 1.709261, acc: 0.48214285714285715\n",
            "==>>> epoch: 10, batch index: 800, train loss: 1.708405, acc: 0.48295\n",
            "==>>> epoch: 10, batch index: 900, train loss: 1.706715, acc: 0.4826888888888889\n",
            "==>>> epoch: 10, batch index: 1000, train loss: 1.711345, acc: 0.4821\n",
            "==>>> epoch: 10, batch index: 1100, train loss: 1.711306, acc: 0.48176363636363634\n",
            "==>>> epoch: 10, batch index: 1200, train loss: 1.714295, acc: 0.4809333333333333\n",
            "==>>> epoch: 10, batch index: 1300, train loss: 1.713880, acc: 0.4813230769230769\n",
            "==>>> epoch: 10, batch index: 1400, train loss: 1.713220, acc: 0.4816142857142857\n",
            "==>>> epoch: 10, batch index: 1500, train loss: 1.712171, acc: 0.48258666666666666\n",
            "==>>> epoch: 10, batch index: 1597, train loss: 1.710781, acc: 0.4827400485984118\n",
            "==>>> epoch: 10, batch index: 100, test loss: 1.890046, acc: 0.452\n",
            "==>>> epoch: 10, batch index: 200, test loss: 1.848189, acc: 0.457\n",
            "==>>> epoch: 10, batch index: 300, test loss: 1.855358, acc: 0.454\n",
            "==>>> epoch: 10, batch index: 400, test loss: 1.860125, acc: 0.453\n",
            "==>>> epoch: 11, batch index: 100, train loss: 1.603315, acc: 0.5216\n",
            "==>>> epoch: 11, batch index: 200, train loss: 1.623610, acc: 0.5089\n",
            "==>>> epoch: 11, batch index: 300, train loss: 1.637570, acc: 0.504\n",
            "==>>> epoch: 11, batch index: 400, train loss: 1.633292, acc: 0.50585\n",
            "==>>> epoch: 11, batch index: 500, train loss: 1.636564, acc: 0.50552\n",
            "==>>> epoch: 11, batch index: 600, train loss: 1.636890, acc: 0.5049666666666667\n",
            "==>>> epoch: 11, batch index: 700, train loss: 1.637720, acc: 0.5041142857142857\n",
            "==>>> epoch: 11, batch index: 800, train loss: 1.639328, acc: 0.50445\n",
            "==>>> epoch: 11, batch index: 900, train loss: 1.650365, acc: 0.5008222222222222\n",
            "==>>> epoch: 11, batch index: 1000, train loss: 1.655371, acc: 0.49986\n",
            "==>>> epoch: 11, batch index: 1100, train loss: 1.655871, acc: 0.49978181818181816\n",
            "==>>> epoch: 11, batch index: 1200, train loss: 1.655758, acc: 0.4995833333333333\n",
            "==>>> epoch: 11, batch index: 1300, train loss: 1.654895, acc: 0.4994923076923077\n",
            "==>>> epoch: 11, batch index: 1400, train loss: 1.655198, acc: 0.4997714285714286\n",
            "==>>> epoch: 11, batch index: 1500, train loss: 1.653865, acc: 0.5004266666666667\n",
            "==>>> epoch: 11, batch index: 1597, train loss: 1.651661, acc: 0.5012650617500438\n",
            "==>>> epoch: 11, batch index: 100, test loss: 1.853324, acc: 0.460\n",
            "==>>> epoch: 11, batch index: 200, test loss: 1.818152, acc: 0.464\n",
            "==>>> epoch: 11, batch index: 300, test loss: 1.822649, acc: 0.461\n",
            "==>>> epoch: 11, batch index: 400, test loss: 1.827430, acc: 0.461\n",
            "==>>> epoch: 12, batch index: 100, train loss: 1.564233, acc: 0.529\n",
            "==>>> epoch: 12, batch index: 200, train loss: 1.559368, acc: 0.5289\n",
            "==>>> epoch: 12, batch index: 300, train loss: 1.543508, acc: 0.5323333333333333\n",
            "==>>> epoch: 12, batch index: 400, train loss: 1.534601, acc: 0.53565\n",
            "==>>> epoch: 12, batch index: 500, train loss: 1.541536, acc: 0.53264\n",
            "==>>> epoch: 12, batch index: 600, train loss: 1.545770, acc: 0.5314\n",
            "==>>> epoch: 12, batch index: 700, train loss: 1.550545, acc: 0.5302\n",
            "==>>> epoch: 12, batch index: 800, train loss: 1.552443, acc: 0.529925\n",
            "==>>> epoch: 12, batch index: 900, train loss: 1.551473, acc: 0.5299333333333334\n",
            "==>>> epoch: 12, batch index: 1000, train loss: 1.549952, acc: 0.53092\n",
            "==>>> epoch: 12, batch index: 1100, train loss: 1.553405, acc: 0.5299090909090909\n",
            "==>>> epoch: 12, batch index: 1200, train loss: 1.553172, acc: 0.5306333333333333\n",
            "==>>> epoch: 12, batch index: 1300, train loss: 1.554668, acc: 0.5304307692307693\n",
            "==>>> epoch: 12, batch index: 1400, train loss: 1.553939, acc: 0.5306714285714286\n",
            "==>>> epoch: 12, batch index: 1500, train loss: 1.555976, acc: 0.5304266666666667\n",
            "==>>> epoch: 12, batch index: 1597, train loss: 1.557475, acc: 0.5300733986322302\n",
            "==>>> epoch: 12, batch index: 100, test loss: 1.831687, acc: 0.475\n",
            "==>>> epoch: 12, batch index: 200, test loss: 1.798541, acc: 0.473\n",
            "==>>> epoch: 12, batch index: 300, test loss: 1.805663, acc: 0.468\n",
            "==>>> epoch: 12, batch index: 400, test loss: 1.808685, acc: 0.468\n",
            "==>>> epoch: 13, batch index: 100, train loss: 1.501129, acc: 0.555\n",
            "==>>> epoch: 13, batch index: 200, train loss: 1.482057, acc: 0.5552\n",
            "==>>> epoch: 13, batch index: 300, train loss: 1.495443, acc: 0.5476666666666666\n",
            "==>>> epoch: 13, batch index: 400, train loss: 1.494217, acc: 0.5481\n",
            "==>>> epoch: 13, batch index: 500, train loss: 1.494255, acc: 0.54868\n",
            "==>>> epoch: 13, batch index: 600, train loss: 1.492483, acc: 0.5485666666666666\n",
            "==>>> epoch: 13, batch index: 700, train loss: 1.494249, acc: 0.5476571428571428\n",
            "==>>> epoch: 13, batch index: 800, train loss: 1.495611, acc: 0.547525\n",
            "==>>> epoch: 13, batch index: 900, train loss: 1.497450, acc: 0.5470666666666667\n",
            "==>>> epoch: 13, batch index: 1000, train loss: 1.498388, acc: 0.54634\n",
            "==>>> epoch: 13, batch index: 1100, train loss: 1.501724, acc: 0.5452181818181818\n",
            "==>>> epoch: 13, batch index: 1200, train loss: 1.503045, acc: 0.5449666666666667\n",
            "==>>> epoch: 13, batch index: 1300, train loss: 1.500661, acc: 0.5455538461538462\n",
            "==>>> epoch: 13, batch index: 1400, train loss: 1.499487, acc: 0.5455142857142857\n",
            "==>>> epoch: 13, batch index: 1500, train loss: 1.498882, acc: 0.5456133333333333\n",
            "==>>> epoch: 13, batch index: 1597, train loss: 1.499531, acc: 0.54555474836544\n",
            "==>>> epoch: 13, batch index: 100, test loss: 1.808356, acc: 0.479\n",
            "==>>> epoch: 13, batch index: 200, test loss: 1.763496, acc: 0.486\n",
            "==>>> epoch: 13, batch index: 300, test loss: 1.770237, acc: 0.485\n",
            "==>>> epoch: 13, batch index: 400, test loss: 1.769729, acc: 0.484\n",
            "==>>> epoch: 14, batch index: 100, train loss: 1.391647, acc: 0.5796\n",
            "==>>> epoch: 14, batch index: 200, train loss: 1.387351, acc: 0.5773\n",
            "==>>> epoch: 14, batch index: 300, train loss: 1.394673, acc: 0.574\n",
            "==>>> epoch: 14, batch index: 400, train loss: 1.408339, acc: 0.5711\n",
            "==>>> epoch: 14, batch index: 500, train loss: 1.413742, acc: 0.57\n",
            "==>>> epoch: 14, batch index: 600, train loss: 1.414605, acc: 0.5696\n",
            "==>>> epoch: 14, batch index: 700, train loss: 1.416489, acc: 0.5687142857142857\n",
            "==>>> epoch: 14, batch index: 800, train loss: 1.414150, acc: 0.569875\n",
            "==>>> epoch: 14, batch index: 900, train loss: 1.418262, acc: 0.5679555555555555\n",
            "==>>> epoch: 14, batch index: 1000, train loss: 1.418260, acc: 0.56814\n",
            "==>>> epoch: 14, batch index: 1100, train loss: 1.423114, acc: 0.5672181818181818\n",
            "==>>> epoch: 14, batch index: 1200, train loss: 1.423016, acc: 0.5677833333333333\n",
            "==>>> epoch: 14, batch index: 1300, train loss: 1.424860, acc: 0.5673538461538462\n",
            "==>>> epoch: 14, batch index: 1400, train loss: 1.425056, acc: 0.5673857142857143\n",
            "==>>> epoch: 14, batch index: 1500, train loss: 1.426060, acc: 0.5673466666666667\n",
            "==>>> epoch: 14, batch index: 1597, train loss: 1.427147, acc: 0.5668854430221197\n",
            "==>>> epoch: 14, batch index: 100, test loss: 1.813564, acc: 0.488\n",
            "==>>> epoch: 14, batch index: 200, test loss: 1.769772, acc: 0.493\n",
            "==>>> epoch: 14, batch index: 300, test loss: 1.777309, acc: 0.488\n",
            "==>>> epoch: 14, batch index: 400, test loss: 1.776357, acc: 0.488\n",
            "==>>> epoch: 15, batch index: 100, train loss: 1.317374, acc: 0.6006\n",
            "==>>> epoch: 15, batch index: 200, train loss: 1.323841, acc: 0.5949\n",
            "==>>> epoch: 15, batch index: 300, train loss: 1.324052, acc: 0.5951333333333333\n",
            "==>>> epoch: 15, batch index: 400, train loss: 1.328817, acc: 0.59315\n",
            "==>>> epoch: 15, batch index: 500, train loss: 1.332023, acc: 0.59264\n",
            "==>>> epoch: 15, batch index: 600, train loss: 1.332703, acc: 0.5929333333333333\n",
            "==>>> epoch: 15, batch index: 700, train loss: 1.344279, acc: 0.5894285714285714\n",
            "==>>> epoch: 15, batch index: 800, train loss: 1.353583, acc: 0.58655\n",
            "==>>> epoch: 15, batch index: 900, train loss: 1.355636, acc: 0.5864222222222222\n",
            "==>>> epoch: 15, batch index: 1000, train loss: 1.357136, acc: 0.58568\n",
            "==>>> epoch: 15, batch index: 1100, train loss: 1.359321, acc: 0.5845090909090909\n",
            "==>>> epoch: 15, batch index: 1200, train loss: 1.360218, acc: 0.5842833333333334\n",
            "==>>> epoch: 15, batch index: 1300, train loss: 1.363333, acc: 0.5839384615384615\n",
            "==>>> epoch: 15, batch index: 1400, train loss: 1.364556, acc: 0.5836857142857143\n",
            "==>>> epoch: 15, batch index: 1500, train loss: 1.367126, acc: 0.58268\n",
            "==>>> epoch: 15, batch index: 1597, train loss: 1.369157, acc: 0.5822164883889877\n",
            "==>>> epoch: 15, batch index: 100, test loss: 1.771304, acc: 0.494\n",
            "==>>> epoch: 15, batch index: 200, test loss: 1.743968, acc: 0.496\n",
            "==>>> epoch: 15, batch index: 300, test loss: 1.756764, acc: 0.490\n",
            "==>>> epoch: 15, batch index: 400, test loss: 1.759586, acc: 0.491\n",
            "==>>> epoch: 16, batch index: 100, train loss: 1.270332, acc: 0.6148\n",
            "==>>> epoch: 16, batch index: 200, train loss: 1.249831, acc: 0.6198\n",
            "==>>> epoch: 16, batch index: 300, train loss: 1.252112, acc: 0.6174\n",
            "==>>> epoch: 16, batch index: 400, train loss: 1.259551, acc: 0.6142\n",
            "==>>> epoch: 16, batch index: 500, train loss: 1.267037, acc: 0.6128\n",
            "==>>> epoch: 16, batch index: 600, train loss: 1.266045, acc: 0.6131666666666666\n",
            "==>>> epoch: 16, batch index: 700, train loss: 1.273350, acc: 0.6114571428571428\n",
            "==>>> epoch: 16, batch index: 800, train loss: 1.274781, acc: 0.61085\n",
            "==>>> epoch: 16, batch index: 900, train loss: 1.273129, acc: 0.6105777777777778\n",
            "==>>> epoch: 16, batch index: 1000, train loss: 1.273611, acc: 0.61014\n",
            "==>>> epoch: 16, batch index: 1100, train loss: 1.276875, acc: 0.6091818181818182\n",
            "==>>> epoch: 16, batch index: 1200, train loss: 1.279572, acc: 0.6081166666666666\n",
            "==>>> epoch: 16, batch index: 1300, train loss: 1.282390, acc: 0.6076307692307692\n",
            "==>>> epoch: 16, batch index: 1400, train loss: 1.283238, acc: 0.6077428571428571\n",
            "==>>> epoch: 16, batch index: 1500, train loss: 1.286412, acc: 0.6069733333333334\n",
            "==>>> epoch: 16, batch index: 1597, train loss: 1.288983, acc: 0.6062150855482352\n",
            "==>>> epoch: 16, batch index: 100, test loss: 1.774104, acc: 0.498\n",
            "==>>> epoch: 16, batch index: 200, test loss: 1.739715, acc: 0.505\n",
            "==>>> epoch: 16, batch index: 300, test loss: 1.755063, acc: 0.501\n",
            "==>>> epoch: 16, batch index: 400, test loss: 1.757322, acc: 0.500\n",
            "==>>> epoch: 17, batch index: 100, train loss: 1.195503, acc: 0.642\n",
            "==>>> epoch: 17, batch index: 200, train loss: 1.203345, acc: 0.6356\n",
            "==>>> epoch: 17, batch index: 300, train loss: 1.208788, acc: 0.6336666666666667\n",
            "==>>> epoch: 17, batch index: 400, train loss: 1.207555, acc: 0.63165\n",
            "==>>> epoch: 17, batch index: 500, train loss: 1.200786, acc: 0.63348\n",
            "==>>> epoch: 17, batch index: 600, train loss: 1.201252, acc: 0.6344333333333333\n",
            "==>>> epoch: 17, batch index: 700, train loss: 1.205545, acc: 0.6326285714285714\n",
            "==>>> epoch: 17, batch index: 800, train loss: 1.203831, acc: 0.633175\n",
            "==>>> epoch: 17, batch index: 900, train loss: 1.209567, acc: 0.6309555555555556\n",
            "==>>> epoch: 17, batch index: 1000, train loss: 1.224547, acc: 0.62732\n",
            "==>>> epoch: 17, batch index: 1100, train loss: 1.229740, acc: 0.6253272727272727\n",
            "==>>> epoch: 17, batch index: 1200, train loss: 1.229833, acc: 0.6250166666666667\n",
            "==>>> epoch: 17, batch index: 1300, train loss: 1.232556, acc: 0.6245076923076923\n",
            "==>>> epoch: 17, batch index: 1400, train loss: 1.234129, acc: 0.6237571428571429\n",
            "==>>> epoch: 17, batch index: 1500, train loss: 1.232396, acc: 0.6245333333333334\n",
            "==>>> epoch: 17, batch index: 1597, train loss: 1.233061, acc: 0.6243392870562889\n",
            "==>>> epoch: 17, batch index: 100, test loss: 1.828891, acc: 0.495\n",
            "==>>> epoch: 17, batch index: 200, test loss: 1.792550, acc: 0.498\n",
            "==>>> epoch: 17, batch index: 300, test loss: 1.801679, acc: 0.497\n",
            "==>>> epoch: 17, batch index: 400, test loss: 1.797451, acc: 0.499\n",
            "==>>> epoch: 18, batch index: 100, train loss: 1.100219, acc: 0.6602\n",
            "==>>> epoch: 18, batch index: 200, train loss: 1.113333, acc: 0.6583\n",
            "==>>> epoch: 18, batch index: 300, train loss: 1.111143, acc: 0.6596666666666666\n",
            "==>>> epoch: 18, batch index: 400, train loss: 1.119402, acc: 0.65745\n",
            "==>>> epoch: 18, batch index: 500, train loss: 1.130229, acc: 0.65412\n",
            "==>>> epoch: 18, batch index: 600, train loss: 1.137453, acc: 0.6507\n",
            "==>>> epoch: 18, batch index: 700, train loss: 1.144706, acc: 0.6492\n",
            "==>>> epoch: 18, batch index: 800, train loss: 1.148741, acc: 0.6474\n",
            "==>>> epoch: 18, batch index: 900, train loss: 1.153365, acc: 0.6465777777777778\n",
            "==>>> epoch: 18, batch index: 1000, train loss: 1.154894, acc: 0.64602\n",
            "==>>> epoch: 18, batch index: 1100, train loss: 1.157516, acc: 0.6448545454545455\n",
            "==>>> epoch: 18, batch index: 1200, train loss: 1.161363, acc: 0.6435333333333333\n",
            "==>>> epoch: 18, batch index: 1300, train loss: 1.165708, acc: 0.6423384615384615\n",
            "==>>> epoch: 18, batch index: 1400, train loss: 1.170144, acc: 0.6407571428571428\n",
            "==>>> epoch: 18, batch index: 1500, train loss: 1.171838, acc: 0.6396533333333333\n",
            "==>>> epoch: 18, batch index: 1597, train loss: 1.173977, acc: 0.6392068939602695\n",
            "==>>> epoch: 18, batch index: 100, test loss: 1.777898, acc: 0.506\n",
            "==>>> epoch: 18, batch index: 200, test loss: 1.735870, acc: 0.511\n",
            "==>>> epoch: 18, batch index: 300, test loss: 1.741794, acc: 0.507\n",
            "==>>> epoch: 18, batch index: 400, test loss: 1.741055, acc: 0.506\n",
            "==>>> epoch: 19, batch index: 100, train loss: 1.069817, acc: 0.6736\n",
            "==>>> epoch: 19, batch index: 200, train loss: 1.043287, acc: 0.6806\n",
            "==>>> epoch: 19, batch index: 300, train loss: 1.055187, acc: 0.6749333333333334\n",
            "==>>> epoch: 19, batch index: 400, train loss: 1.059936, acc: 0.6723\n",
            "==>>> epoch: 19, batch index: 500, train loss: 1.067882, acc: 0.66964\n",
            "==>>> epoch: 19, batch index: 600, train loss: 1.071548, acc: 0.6689666666666667\n",
            "==>>> epoch: 19, batch index: 700, train loss: 1.072003, acc: 0.6688571428571428\n",
            "==>>> epoch: 19, batch index: 800, train loss: 1.083976, acc: 0.6649\n",
            "==>>> epoch: 19, batch index: 900, train loss: 1.085694, acc: 0.6643555555555556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1d58575e1523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}